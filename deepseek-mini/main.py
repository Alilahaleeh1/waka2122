#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek Mini - Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
"""

import sys
import os
import argparse
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ Python path
sys.path.insert(0, str(Path(__file__).parent))

from utils.config_loader import load_config
from utils.device_manager import DeviceManager
from gui.app import DeepSeekApp
from model.tiny_llm import TinyLLM
from inference.generator import TextGenerator
import torch
from PyQt5.QtWidgets import QApplication

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚"""
    parser = argparse.ArgumentParser(description="DeepSeek Mini - Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ Ø¹ØµØ¨ÙŠ")
    parser.add_argument("--mode", type=str, choices=["gui", "cli", "train"], 
                       default="gui", help="ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„")
    parser.add_argument("--config", type=str, default="config.yaml",
                       help="Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    parser.add_argument("--model-path", type=str, default=None,
                       help="Ù…Ø³Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    parser.add_argument("--text", type=str, default=None,
                       help="Ù†Øµ Ù„Ù„Ø¥ÙƒÙ…Ø§Ù„ (ÙÙŠ ÙˆØ¶Ø¹ CLI)")
    parser.add_argument("--max-tokens", type=int, default=100,
                       help="Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©")
    
    args = parser.parse_args()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config = load_config(args.config)
    
    # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¬Ù‡Ø§Ø²
    device_manager = DeviceManager()
    device = device_manager.get_device(config["system"]["device"])
    
    print(f"ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ DeepSeek Mini v{config['project']['version']}")
    print(f"ğŸ“± Ø§Ù„Ø¬Ù‡Ø§Ø²: {device}")
    print(f"ğŸ® Ø§Ù„ÙˆØ¶Ø¹: {args.mode}")
    
    if args.mode == "gui":
        # ØªØ´ØºÙŠÙ„ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ©
        app = QApplication(sys.argv)
        window = DeepSeekApp(config)
        window.show()
        sys.exit(app.exec_())
    
    elif args.mode == "cli":
        # ÙˆØ¶Ø¹ Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±
        if args.text:
            run_cli_mode(config, device, args.text, args.max_tokens, args.model_path)
        else:
            run_interactive_cli(config, device, args.model_path)
    
    elif args.mode == "train":
        # ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        run_training(config, device)


def run_cli_mode(config, device, text, max_tokens, model_path):
    """ØªØ´ØºÙŠÙ„ ÙˆØ¶Ø¹ Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±"""
    print("\nğŸ¤– ÙˆØ¶Ø¹ Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø± - DeepSeek Mini")
    print(f"ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„: {text}")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = load_model(config, device, model_path)
    generator = TextGenerator(model, config["inference"])
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
    print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...")
    generated = generator.generate(text, max_tokens=max_tokens)
    
    print(f"\nâœ… Ø§Ù„Ù†Ø§ØªØ¬:\n{generated}\n")


def run_interactive_cli(config, device, model_path):
    """ØªØ´ØºÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©"""
    print("\nğŸ’¬ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©")
    print("Ø£Ø¯Ø®Ù„ 'quit' Ù„Ù„Ø®Ø±ÙˆØ¬ØŒ 'clear' Ù„Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
    
    model = load_model(config, device, model_path)
    generator = TextGenerator(model, config["inference"])
    
    conversation_history = []
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ Ø£Ù†Øª: ").strip()
            
            if user_input.lower() == 'quit':
                print("ğŸ‘‹ ÙˆØ¯Ø§Ø¹Ø§Ù‹!")
                break
            
            if user_input.lower() == 'clear':
                conversation_history = []
                print("ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
                continue
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            conversation_history.append(f"ğŸ‘¤ Ø£Ù†Øª: {user_input}")
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„ØªØ§Ø±ÙŠØ®
            context = "\n".join(conversation_history[-6:])  # Ø¢Ø®Ø± 6 Ø±Ø³Ø§Ø¦Ù„
            full_prompt = f"{context}\nğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:"
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
            print("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: ", end="", flush=True)
            response = generator.generate(full_prompt, max_tokens=200, stream=True)
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ®
            conversation_history.append(f"ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ÙˆØ¯Ø§Ø¹Ø§Ù‹!")
            break
        except Exception as e:
            print(f"\nâŒ Ø®Ø·Ø£: {e}")


def run_training(config, device):
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    print("\nğŸ¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    
    from training.trainer import Trainer
    from data.dataset import TextDataset
    from data.tokenizer import Tokenizer
    
    # ØªØ­Ù…ÙŠÙ„ Tokenizer
    tokenizer = Tokenizer(config["tokenizer"])
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("ğŸ“‚ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    train_dataset = TextDataset(config["data"]["train_path"], tokenizer, config["data"]["max_length"])
    val_dataset = TextDataset(config["data"]["val_path"], tokenizer, config["data"]["max_length"])
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("ğŸ§  Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    model = TinyLLM(
        vocab_size=config["model"]["vocab_size"],
        d_model=config["model"]["d_model"],
        n_heads=config["model"]["n_heads"],
        n_layers=config["model"]["n_layers"],
        max_seq_len=config["model"]["max_seq_len"],
        dropout=config["model"]["dropout"],
        ffn_dim=config["model"]["ffn_dim"],
        use_bias=config["model"]["use_bias"]
    ).to(device)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
    trainer = Trainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        config=config["training"],
        device=device
    )
    
    # Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    trainer.train()


def load_model(config, device, model_path=None):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    print("ğŸ§  Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    
    model = TinyLLM(
        vocab_size=config["model"]["vocab_size"],
        d_model=config["model"]["d_model"],
        n_heads=config["model"]["n_heads"],
        n_layers=config["model"]["n_layers"],
        max_seq_len=config["model"]["max_seq_len"],
        dropout=config["model"]["dropout"],
        ffn_dim=config["model"]["ffn_dim"],
        use_bias=config["model"]["use_bias"]
    ).to(device)
    
    if model_path and os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint["model_state_dict"])
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† {model_path}")
    else:
        print("âš ï¸  Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ Ù†Ù‚Ø·Ø© Ø­ÙØ¸ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆÙ„ÙŠ")
    
    model.eval()
    return model


if __name__ == "__main__":
    main()