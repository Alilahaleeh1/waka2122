# -*- coding: utf-8 -*-
"""
Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ù„ØªØ­Ù…ÙŠØ¯ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Øµ
"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import List, Tuple, Dict, Optional, Union
import os
import json
from pathlib import Path


class TextDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù†ØµÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    def __init__(self, 
                 data_path: str,
                 tokenizer,
                 max_length: int = 2048,
                 stride: int = 512,
                 lazy_loading: bool = False):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            data_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            tokenizer: Tokenizer
            max_length: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            stride: Ø§Ù„Ø®Ø·ÙˆØ© Ø¹Ù†Ø¯ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
            lazy_loading: Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ³ÙˆÙ„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        """
        self.data_path = data_path
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.stride = stride
        self.lazy_loading = lazy_loading
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.data = self._load_data()
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¥Ù„Ù‰ ÙƒØªÙ„
        self.sequences = self._create_sequences()
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª: {len(self.sequences)} ØªØ³Ù„Ø³Ù„")
    
    def _load_data(self) -> List[str]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        print(f"ğŸ“‚ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {self.data_path}...")
        
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {self.data_path}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        file_ext = os.path.splitext(self.data_path)[1].lower()
        
        if file_ext == '.pt':
            # Ù…Ù„Ù PyTorch
            data = torch.load(self.data_path)
            
            if isinstance(data, list):
                return data
            elif isinstance(data, dict) and 'texts' in data:
                return data['texts']
            else:
                raise ValueError(f"ØªÙ†Ø³ÙŠÙ‚ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {self.data_path}")
        
        elif file_ext == '.json':
            # Ù…Ù„Ù JSON
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ÙØªØ§Ø­ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ
                for key in ['texts', 'data', 'content']:
                    if key in data and isinstance(data[key], list):
                        return data[key]
            
            raise ValueError(f"ØªÙ†Ø³ÙŠÙ‚ JSON ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {self.data_path}")
        
        elif file_ext == '.txt':
            # Ù…Ù„Ù Ù†ØµÙŠ
            with open(self.data_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ø·Ø±
            texts = [line.strip() for line in lines if line.strip()]
            return texts
        
        else:
            raise ValueError(f"Ø§Ù…ØªØ¯Ø§Ø¯ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {file_ext}")
    
    def _create_sequences(self) -> List[Dict[str, torch.Tensor]]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØ³Ù„Ø³Ù„Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        sequences = []
        
        print("ğŸ”¡ Ø¬Ø§Ø±ÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„Ø§Øª...")
        
        for text in self.data:
            # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ
            token_ids = self.tokenizer.encode(text, add_special_tokens=False)
            
            if len(token_ids) <= self.max_length:
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ±Ø§Ù‹ØŒ Ø£Ø¶ÙÙ‡ ÙƒÙ…Ø§ Ù‡Ùˆ
                sequence = self._prepare_sequence(token_ids)
                sequences.append(sequence)
            else:
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø·ÙˆÙŠÙ„ Ù…Ø¹ Ø§Ù„ØªØ¯Ø§Ø®Ù„
                for i in range(0, len(token_ids) - self.max_length + 1, self.stride):
                    chunk = token_ids[i:i + self.max_length]
                    sequence = self._prepare_sequence(chunk)
                    sequences.append(sequence)
        
        return sequences
    
    def _prepare_sequence(self, token_ids: List[int]) -> Dict[str, torch.Tensor]:
        """ØªØ­Ø¶ÙŠØ± ØªØ³Ù„Ø³Ù„ Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
        # Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù‡ÙŠ ÙƒÙ„ Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ø§Ø¹Ø¯Ø§ Ø§Ù„Ø£Ø®ÙŠØ±
        input_ids = torch.tensor(token_ids[:-1], dtype=torch.long)
        
        # Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù‡ÙŠ ÙƒÙ„ Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ø§Ø¹Ø¯Ø§ Ø§Ù„Ø£ÙˆÙ„ (shifted right)
        labels = torch.tensor(token_ids[1:], dtype=torch.long)
        
        # Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ (ÙƒÙ„Ù‡Ø§ 1 Ù„Ø£Ù† Ù„Ø§ Ø­Ø´Ùˆ Ù‡Ù†Ø§)
        attention_mask = torch.ones_like(input_ids)
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    def __len__(self) -> int:
        """Ø·ÙˆÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        return len(self.sequences)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø©"""
        return self.sequences[idx]
    
    def collate_fn(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        Ø¯Ø§Ù„Ø© ØªØ¬Ù…ÙŠØ¹ Ù„Ù„Ù€ DataLoader
        
        Args:
            batch: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
        
        Returns:
            Ø¯ÙØ¹Ø© Ù…Ø¬Ù…Ø¹Ø©
        """
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„ ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø©
        max_len = max(item["input_ids"].size(0) for item in batch)
        
        # Ø­Ø´Ùˆ Ø¬Ù…ÙŠØ¹ Tensors Ù„Ù†ÙØ³ Ø§Ù„Ø·ÙˆÙ„
        input_ids = []
        attention_masks = []
        labels = []
        
        for item in batch:
            seq_len = item["input_ids"].size(0)
            
            if seq_len < max_len:
                # Ø§Ù„Ø­Ø´Ùˆ
                pad_len = max_len - seq_len
                pad_tensor = torch.full((pad_len,), self.tokenizer.vocab[self.tokenizer.pad_token])
                
                input_ids.append(torch.cat([item["input_ids"], pad_tensor]))
                attention_masks.append(torch.cat([item["attention_mask"], torch.zeros(pad_len)]))
                labels.append(torch.cat([item["labels"], pad_tensor]))
            else:
                input_ids.append(item["input_ids"])
                attention_masks.append(item["attention_mask"])
                labels.append(item["labels"])
        
        return {
            "input_ids": torch.stack(input_ids),
            "attention_mask": torch.stack(attention_masks),
            "labels": torch.stack(labels)
        }
    
    def get_dataloader(self, 
                      batch_size: int = 32,
                      shuffle: bool = True,
                      num_workers: int = 0) -> DataLoader:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ DataLoader
        
        Args:
            batch_size: Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
            shuffle: Ø®Ù„Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            num_workers: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø§Ù…Ù„ÙŠÙ†
        
        Returns:
            DataLoader
        """
        return DataLoader(
            self,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self.collate_fn,
            pin_memory=True
        )
    
    def split(self, train_ratio: float = 0.8, val_ratio: float = 0.1) -> Tuple['TextDataset', 'TextDataset', 'TextDataset']:
        """
        ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            train_ratio: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            val_ratio: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ­Ù‚Ù‚
        
        Returns:
            Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ØŒ ØªØ­Ù‚Ù‚ØŒ Ø§Ø®ØªØ¨Ø§Ø±
        """
        from copy import deepcopy
        
        total_size = len(self)
        train_size = int(total_size * train_ratio)
        val_size = int(total_size * val_ratio)
        test_size = total_size - train_size - val_size
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙØ±Ø¹ÙŠØ©
        train_dataset = deepcopy(self)
        train_dataset.sequences = self.sequences[:train_size]
        
        val_dataset = deepcopy(self)
        val_dataset.sequences = self.sequences[train_size:train_size + val_size]
        
        test_dataset = deepcopy(self)
        test_dataset.sequences = self.sequences[train_size + val_size:]
        
        print(f"ğŸ“Š ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        print(f"   Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(train_dataset)} ØªØ³Ù„Ø³Ù„")
        print(f"   Ø§Ù„ØªØ­Ù‚Ù‚: {len(val_dataset)} ØªØ³Ù„Ø³Ù„")
        print(f"   Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {len(test_dataset)} ØªØ³Ù„Ø³Ù„")
        
        return train_dataset, val_dataset, test_dataset
    
    def save(self, path: str) -> None:
        """
        Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            path: Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸
        """
        save_data = {
            'data': self.data,
            'config': {
                'max_length': self.max_length,
                'stride': self.stride
            }
        }
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save(save_data, path)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ {path}")
    
    @classmethod
    def load(cls, path: str, tokenizer, **kwargs) -> 'TextDataset':
        """
        ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            path: Ù…Ø³Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
            tokenizer: Tokenizer
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        
        Returns:
            Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {path}")
        
        data = torch.load(path)
        
        dataset = cls(
            data_path=path,
            tokenizer=tokenizer,
            max_length=data.get('config', {}).get('max_length', 2048),
            stride=data.get('config', {}).get('stride', 512),
            **kwargs
        )
        
        dataset.data = data['data']
        dataset.sequences = dataset._create_sequences()
        
        return dataset


class StreamingTextDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙÙ‚ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹"""
    
    def __init__(self, 
                 data_paths: List[str],
                 tokenizer,
                 max_length: int = 2048):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯ÙÙ‚
        
        Args:
            data_paths: Ù‚Ø§Ø¦Ù…Ø© Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
            tokenizer: Tokenizer
            max_length: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„
        """
        self.data_paths = data_paths
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
        self.file_index = self._build_file_index()
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙÙ‚: {len(self.file_index)} Ø¹ÙŠÙ†Ø©")
    
    def _build_file_index(self) -> List[Tuple[str, int, int]]:
        """Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù„ÙØ§Øª"""
        file_index = []
        
        for file_path in self.data_paths:
            if not os.path.exists(file_path):
                print(f"âš ï¸  Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
                continue
            
            # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø± ÙÙŠ Ø§Ù„Ù…Ù„Ù
            with open(file_path, 'r', encoding='utf-8') as f:
                num_lines = sum(1 for _ in f)
            
            # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„ Ø³Ø·Ø± Ø¥Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³
            for line_idx in range(num_lines):
                file_index.append((file_path, line_idx))
        
        return file_index
    
    def __len__(self) -> int:
        """Ø·ÙˆÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        return len(self.file_index)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø©"""
        file_path, line_idx = self.file_index[idx]
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i == line_idx:
                    text = line.strip()
                    break
        
        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ
        token_ids = self.tokenizer.encode(text, add_special_tokens=False)
        
        # Ø§Ù‚ØªØµØ§Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹
        if len(token_ids) > self.max_length:
            token_ids = token_ids[:self.max_length]
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØ³Ù„Ø³Ù„
        input_ids = torch.tensor(token_ids[:-1], dtype=torch.long)
        labels = torch.tensor(token_ids[1:], dtype=torch.long)
        attention_mask = torch.ones_like(input_ids)
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }


def create_sample_dataset(output_path: str = "./data/processed/sample.pt", 
                         num_samples: int = 1000) -> None:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    
    Args:
        output_path: Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸
        num_samples: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
    """
    import random
    import string
    
    # Ù†ØµÙˆØµ Ø¹ÙŠÙ†Ø©
    arabic_texts = [
        "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ù…Ø­Ø§ÙƒØ§Ø© Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø´Ø±ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø¢Ù„Ø§Øª.",
        "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù‡Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©.",
        "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù„ØºØ© ØºÙ†ÙŠØ© Ø¨Ù…ÙØ±Ø¯Ø§ØªÙ‡Ø§ ÙˆØªØ±Ø§ÙƒÙŠØ¨Ù‡Ø§ Ø§Ù„Ù†Ø­ÙˆÙŠØ©.",
        "Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© ØªØ³ØªØ®Ø¯Ù… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ.",
        "Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØºØ© Ù‡ÙŠ Ù…Ø¬Ø§Ù„ ÙŠÙ‡ØªÙ… Ø¨ØªÙØ§Ø¹Ù„ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø´Ø±ÙŠØ©."
    ]
    
    english_texts = [
        "Artificial intelligence is the simulation of human intelligence processes by machines.",
        "Deep learning is a subset of machine learning that uses neural networks.",
        "Natural Language Processing enables computers to understand human language.",
        "Transformers have revolutionized the field of language modeling.",
        "Attention mechanisms allow models to focus on relevant parts of the input."
    ]
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
    samples = []
    
    for _ in range(num_samples):
        # Ø§Ø®ØªÙŠØ§Ø± Ù„ØºØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        if random.random() < 0.5:
            text = random.choice(arabic_texts)
        else:
            text = random.choice(english_texts)
        
        # Ø¥Ø¶Ø§ÙØ© Ø¨Ø¹Ø¶ Ø§Ù„ØªÙ†ÙˆØ¹
        words = text.split()
        if len(words) > 3:
            # ØªØºÙŠÙŠØ± ØªØ±ØªÙŠØ¨ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            idx1, idx2 = random.sample(range(len(words)), 2)
            words[idx1], words[idx2] = words[idx2], words[idx1]
            text = " ".join(words)
        
        samples.append(text)
    
    # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    torch.save({"texts": samples}, output_path)
    
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø©: {len(samples)} Ø¹ÙŠÙ†Ø© ÙÙŠ {output_path}")


if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    from tokenizer import Tokenizer
    
    # Ø¥Ù†Ø´Ø§Ø¡ Tokenizer
    tokenizer_config = {
        "vocab_size": 5000,
        "special_tokens": {
            "bos": "<bos>",
            "eos": "<eos>",
            "pad": "<pad>",
            "unk": "<unk>"
        }
    }
    
    tokenizer = Tokenizer(tokenizer_config)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø©
    create_sample_dataset()
    
    # ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    dataset = TextDataset(
        data_path="./data/processed/sample.pt",
        tokenizer=tokenizer,
        max_length=128
    )
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ DataLoader
    dataloader = dataset.get_dataloader(batch_size=4, shuffle=True)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
    for batch in dataloader:
        print(f"\nØ¯ÙØ¹Ø©:")
        print(f"  input_ids shape: {batch['input_ids'].shape}")
        print(f"  attention_mask shape: {batch['attention_mask'].shape}")
        print(f"  labels shape: {batch['labels'].shape}")
        
        # ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙˆÙ„
        first_text = tokenizer.decode(batch['input_ids'][0].tolist())
        print(f"  Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙˆÙ„: {first_text[:100]}...")
        
        break
    
    print(f"\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")