# -*- coding: utf-8 -*-
"""
Tokenizer Ù…Ø®ØµØµ - Ù…Ø¹ Ø¯Ø¹Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª ÙˆØ§Ù„Ø¹Ø±Ø¨ÙŠØ©
"""

import json
import os
import re
from collections import Counter
from typing import List, Dict, Tuple, Optional, Union
import torch


class Tokenizer:
    """Tokenizer Ù…Ø®ØµØµ Ù…Ø¹ Ø¯Ø¹Ù… BPE"""
    
    def __init__(self, config: Dict):
        """
        ØªÙ‡ÙŠØ¦Ø© Tokenizer
        
        Args:
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Tokenizer
        """
        self.config = config
        self.vocab_size = config.get("vocab_size", 50000)
        self.special_tokens = config.get("special_tokens", {})
        
        # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
        self.vocab = {}
        self.inverse_vocab = {}
        
        # Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
        self.bos_token = self.special_tokens.get("bos", "<bos>")
        self.eos_token = self.special_tokens.get("eos", "<eos>")
        self.pad_token = self.special_tokens.get("pad", "<pad>")
        self.unk_token = self.special_tokens.get("unk", "<unk>")
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
        self._init_special_tokens()
        
        # Ø£Ù†Ù…Ø§Ø· Tokenization
        self.pattern = self._build_pattern()
        
        # BPE merges
        self.merges = {}
        self.bpe_ranks = {}
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            "total_tokens": 0,
            "unique_tokens": 0,
            "vocab_loaded": False
        }
    
    def _init_special_tokens(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©"""
        special_tokens_list = [
            self.pad_token,
            self.bos_token,
            self.eos_token,
            self.unk_token
        ]
        
        for i, token in enumerate(special_tokens_list):
            self.vocab[token] = i
            self.inverse_vocab[i] = token
        
        self.special_token_ids = set(self.vocab.values())
    
    def _build_pattern(self) -> re.Pattern:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø· Tokenization"""
        # Ù†Ù…Ø· Ø¨Ø³ÙŠØ· Ù„Ù„ØºØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© (ÙŠØ´Ù…Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
        pattern_parts = [
            r"\w+",                     # ÙƒÙ„Ù…Ø§Øª
            r"[\u0600-\u06FF]+",        # Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÙŠØ©
            r"\d+",                     # Ø£Ø±Ù‚Ø§Ù…
            r"[^\w\s\u0600-\u06FF]",    # Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ…
            r"\s+",                     # Ù…Ø³Ø§ÙØ§Øª
        ]
        
        return re.compile("|".join(pattern_parts))
    
    def train(self, texts: List[str], save_path: Optional[str] = None) -> None:
        """
        ØªØ¯Ø±ÙŠØ¨ Tokenizer Ø¹Ù„Ù‰ Ù†Øµ
        
        Args:
            texts: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„ØªØ¯Ø±ÙŠØ¨
            save_path: Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        """
        print("ğŸ”¤ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Tokenizer...")
        
        # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ
        all_text = " ".join(texts)
        
        # Tokenization Ø£ÙˆÙ„ÙŠ
        tokens = self.pattern.findall(all_text)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        word_counts = Counter(tokens)
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù† Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹
        most_common = word_counts.most_common(self.vocab_size - len(self.special_token_ids))
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
        start_idx = len(self.special_token_ids)
        for i, (word, count) in enumerate(most_common):
            idx = start_idx + i
            self.vocab[word] = idx
            self.inverse_vocab[idx] = word
        
        # ØªØ¯Ø±ÙŠØ¨ BPE (Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø³Ø·)
        self._train_bpe(word_counts)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats["total_tokens"] = sum(word_counts.values())
        self.stats["unique_tokens"] = len(self.vocab)
        self.stats["vocab_loaded"] = True
        
        print(f"âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Tokenizer Ø¹Ù„Ù‰ {len(texts)} Ù†Øµ")
        print(f"   Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {len(self.vocab)}")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±Ù…ÙˆØ²: {self.stats['total_tokens']}")
        
        # Ø­ÙØ¸ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
        if save_path:
            self.save(save_path)
    
    def _train_bpe(self, word_counts: Counter, num_merges: int = 10000) -> None:
        """ØªØ¯Ø±ÙŠØ¨ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© BPE Ù…Ø¨Ø³Ø·Ø©"""
        print("   ØªØ¯Ø±ÙŠØ¨ BPE...")
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙƒØ­Ø±ÙˆÙ ÙØ±Ø¯ÙŠØ©
        vocab = set()
        for word in word_counts.keys():
            vocab.update(word)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ù…ÙˆØ² Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
        for char in vocab:
            if char not in self.vocab:
                idx = len(self.vocab)
                self.vocab[char] = idx
                self.inverse_vocab[idx] = char
        
        # Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¯Ù…Ø¬ (Ù…Ø¨Ø³Ø·Ø©)
        merges = []
        for i in range(min(num_merges, len(vocab) * 10)):
            # Ù‡Ù†Ø§ ÙŠØ¬Ø¨ ØªÙ†ÙÙŠØ° Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© BPE ÙƒØ§Ù…Ù„Ø©
            # Ù„ÙƒÙ†Ù†Ø§ Ø³Ù†Ø¨Ù‚ÙŠÙ‡Ø§ Ù…Ø¨Ø³Ø·Ø© Ù„Ø£Ø¬Ù„ Ø§Ù„Ù…Ø«Ø§Ù„
            break
        
        self.merges = {i: merge for i, merge in enumerate(merges)}
    
    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """
        ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ±Ù…ÙŠØ²Ù‡
            add_special_tokens: Ø¥Ø¶Ø§ÙØ© Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±Ù…ÙˆØ²
        """
        # Tokenization Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø·
        tokens = self.pattern.findall(text)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²
        token_ids = []
        
        if add_special_tokens:
            token_ids.append(self.vocab[self.bos_token])
        
        for token in tokens:
            if token in self.vocab:
                token_ids.append(self.vocab[token])
            else:
                # ØªØ·Ø¨ÙŠÙ‚ BPE Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
                bpe_tokens = self._apply_bpe(token)
                for bpe_token in bpe_tokens:
                    if bpe_token in self.vocab:
                        token_ids.append(self.vocab[bpe_token])
                    else:
                        token_ids.append(self.vocab[self.unk_token])
        
        if add_special_tokens:
            token_ids.append(self.vocab[self.eos_token])
        
        return token_ids
    
    def _apply_bpe(self, token: str) -> List[str]:
        """ØªØ·Ø¨ÙŠÙ‚ BPE Ø¹Ù„Ù‰ Ø±Ù…Ø²"""
        # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¨Ø³Ø· Ù„Ù€ BPE
        if not self.merges:
            return [token]
        
        # Ù‡Ù†Ø§ ÙŠØ¬Ø¨ ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¯Ù…Ø¬ Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„
        # Ù„ÙƒÙ†Ù†Ø§ Ø³Ù†Ø±Ø¬Ø¹ Ø§Ù„Ø±Ù…Ø² ÙƒÙ…Ø§ Ù‡Ùˆ Ù„Ø£Ø¬Ù„ Ø§Ù„Ù…Ø«Ø§Ù„
        return [token]
    
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """
        ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ù…ÙˆØ² Ø¥Ù„Ù‰ Ù†Øµ
        
        Args:
            token_ids: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ²
            skip_special_tokens: ØªØ®Ø·ÙŠ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
        
        Returns:
            Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙÙƒÙˆÙƒ
        """
        tokens = []
        
        for token_id in token_ids:
            if token_id in self.inverse_vocab:
                token = self.inverse_vocab[token_id]
                
                if skip_special_tokens and token in [self.bos_token, self.eos_token, self.pad_token]:
                    continue
                
                tokens.append(token)
            else:
                tokens.append(self.unk_token)
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Øµ
        text = "".join(tokens)
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def save(self, path: str) -> None:
        """
        Ø­ÙØ¸ Tokenizer
        
        Args:
            path: Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸
        """
        save_data = {
            "vocab": self.vocab,
            "inverse_vocab": self.inverse_vocab,
            "config": self.config,
            "merges": self.merges,
            "stats": self.stats
        }
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Tokenizer ÙÙŠ {path}")
    
    def load(self, path: str) -> None:
        """
        ØªØ­Ù…ÙŠÙ„ Tokenizer
        
        Args:
            path: Ù…Ø³Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Ù…Ù„Ù Tokenizer ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            save_data = json.load(f)
        
        self.vocab = save_data["vocab"]
        self.inverse_vocab = save_data["inverse_vocab"]
        self.config = save_data["config"]
        self.merges = save_data["merges"]
        self.stats = save_data["stats"]
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
        self.bos_token = self.config["special_tokens"]["bos"]
        self.eos_token = self.config["special_tokens"]["eos"]
        self.pad_token = self.config["special_tokens"]["pad"]
        self.unk_token = self.config["special_tokens"]["unk"]
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Tokenizer Ù…Ù† {path}")
        print(f"   Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {len(self.vocab)}")
    
    def tokenize(self, text: str) -> List[str]:
        """
        ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² Ù†ØµÙŠØ©
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ‚Ø³ÙŠÙ…Ù‡
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù†ØµÙŠØ©
        """
        return self.pattern.findall(text)
    
    def get_vocab_size(self) -> int:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª"""
        return len(self.vocab)
    
    def pad_sequence(self, sequences: List[List[int]], 
                    max_len: Optional[int] = None,
                    padding_side: str = "right") -> torch.Tensor:
        """
        Ø­Ø´Ùˆ ØªØ³Ù„Ø³Ù„ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²
        
        Args:
            sequences: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª
            max_len: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„ (Ø¥Ø°Ø§ NoneØŒ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØªØ³Ù„Ø³Ù„Ø§Øª)
            padding_side: Ø¬Ù‡Ø© Ø§Ù„Ø­Ø´Ùˆ ("right" Ø£Ùˆ "left")
        
        Returns:
            Tensor Ù…Ø­Ø´Ùˆ
        """
        pad_token_id = self.vocab[self.pad_token]
        
        if max_len is None:
            max_len = max(len(seq) for seq in sequences)
        
        padded_sequences = []
        
        for seq in sequences:
            if len(seq) > max_len:
                # Ø§Ù‚ØªØµØ§Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£Ø·ÙˆÙ„
                padded_seq = seq[:max_len]
            else:
                padded_seq = seq.copy()
                
                # Ø§Ù„Ø­Ø´Ùˆ
                padding_length = max_len - len(seq)
                padding = [pad_token_id] * padding_length
                
                if padding_side == "right":
                    padded_seq = padded_seq + padding
                else:
                    padded_seq = padding + padded_seq
            
            padded_sequences.append(padded_seq)
        
        return torch.tensor(padded_sequences)
    
    def batch_encode(self, texts: List[str], 
                    max_length: Optional[int] = None,
                    truncation: bool = True) -> Dict[str, torch.Tensor]:
        """
        ØªØ±Ù…ÙŠØ² Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ
        
        Args:
            texts: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ
            max_length: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„
            truncation: Ø§Ù‚ØªØµØ§Øµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
        
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ù€ Tensors
        """
        all_token_ids = []
        all_attention_masks = []
        
        for text in texts:
            token_ids = self.encode(text, add_special_tokens=True)
            
            # Ø§Ù‚ØªØµØ§Øµ Ø¥Ø°Ø§ Ø·Ù„Ø¨
            if max_length and truncation and len(token_ids) > max_length:
                token_ids = token_ids[:max_length]
            
            all_token_ids.append(token_ids)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„
        if max_length is None:
            max_length = max(len(ids) for ids in all_token_ids)
        
        # Ø§Ù„Ø­Ø´Ùˆ
        input_ids = self.pad_sequence(all_token_ids, max_len=max_length)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‚Ù†Ø¹Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attention_mask = torch.ones_like(input_ids)
        attention_mask[input_ids == self.vocab[self.pad_token]] = 0
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask
        }
    
    def print_stats(self) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Tokenizer"""
        print("=" * 60)
        print("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Tokenizer:")
        print("=" * 60)
        print(f"Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {self.get_vocab_size()}")
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©: {self.stats.get('total_tokens', 0)}")
        print(f"Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ÙØ±ÙŠØ¯Ø©: {self.stats.get('unique_tokens', 0)}")
        print(f"Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©: {list(self.special_tokens.values())}")
        print("=" * 60)


class ArabicTokenizer(Tokenizer):
    """Tokenizer Ù…Ø®ØµØµ Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    
    def __init__(self, config: Dict):
        """ØªÙ‡ÙŠØ¦Ø© Tokenizer Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        super().__init__(config)
        
        # Ù†Ù…Ø· Ù…Ø­Ø³Ù† Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.pattern = self._build_arabic_pattern()
    
    def _build_arabic_pattern(self) -> re.Pattern:
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø· Ø®Ø§Øµ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        # Ù†Ù…Ø· Ø´Ø§Ù…Ù„ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ØªØ´ÙƒÙŠÙ„
        arabic_pattern = r"[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]+"
        
        # Ù†Ù…Ø· Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© (Ù„Ù„Ù…ØµØ·Ù„Ø­Ø§Øª)
        english_pattern = r"\b[A-Za-z]+\b"
        
        # Ù†Ù…Ø· Ù„Ù„Ø£Ø±Ù‚Ø§Ù…
        digit_pattern = r"\d+"
        
        # Ù†Ù…Ø· Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        punctuation_pattern = r"[^\w\s\u0600-\u06FF]"
        
        # Ù†Ù…Ø· Ù„Ù„Ù…Ø³Ø§ÙØ§Øª
        space_pattern = r"\s+"
        
        return re.compile("|".join([
            arabic_pattern,
            english_pattern,
            digit_pattern,
            punctuation_pattern,
            space_pattern
        ]))
    
    def normalize_arabic(self, text: str) -> str:
        """
        ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        
        Returns:
            Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ø¨ÙŠØ¹
        """
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        text = re.sub(r'[\u064B-\u065F\u0670]', '', text)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù„Ù Ø§Ù„Ù…Ù‚ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø£Ù„Ù
        text = text.replace('Ù‰', 'Ø§')
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø© Ø¥Ù„Ù‰ Ù‡Ø§Ø¡
        text = text.replace('Ø©', 'Ù‡')
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
        text = re.sub(r'(.)\1+', r'\1', text)
        
        return text.strip()


def load_tokenizer(tokenizer_path: str) -> Tokenizer:
    """
    Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­Ù…ÙŠÙ„ Tokenizer
    
    Args:
        tokenizer_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù Tokenizer
    
    Returns:
        Tokenizer Ù…Ø­Ù…Ù„
    """
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    with open(tokenizer_path, 'r', encoding='utf-8') as f:
        config = json.load(f)["config"]
    
    # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Tokenizer
    tokenizer_type = config.get("type", "standard")
    
    if tokenizer_type == "arabic":
        tokenizer = ArabicTokenizer(config)
    else:
        tokenizer = Tokenizer(config)
    
    tokenizer.load(tokenizer_path)
    return tokenizer


if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Tokenizer
    config = {
        "vocab_size": 10000,
        "special_tokens": {
            "bos": "<bos>",
            "eos": "<eos>",
            "pad": "<pad>",
            "unk": "<unk>"
        }
    }
    
    tokenizer = Tokenizer(config)
    
    # Ù†Øµ Ø§Ø®ØªØ¨Ø§Ø±
    test_texts = [
        "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ DeepSeek Mini!",
        "Ù‡Ø°Ø§ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.",
        "Hello world! ÙƒÙŠÙ Ø§Ù„Ø­Ø§Ù„ØŸ"
    ]
    
    # ØªØ¯Ø±ÙŠØ¨ Tokenizer
    tokenizer.train(test_texts)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ±Ù…ÙŠØ² ÙˆØ§Ù„ØªÙÙƒÙŠÙƒ
    for text in test_texts:
        token_ids = tokenizer.encode(text)
        decoded = tokenizer.decode(token_ids)
        
        print(f"\nØ§Ù„Ù†Øµ: {text}")
        print(f"Ø§Ù„Ø±Ù…ÙˆØ²: {token_ids}")
        print(f"Ø§Ù„Ù…ÙÙƒÙˆÙƒ: {decoded}")
    
    tokenizer.print_stats()