# -*- coding: utf-8 -*-
"""
Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª - Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„Ù„ØªÙˆÙ„ÙŠØ¯
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Optional, List, Dict, Any, Tuple
import math


class Sampler:
    """ÙØ¦Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª"""
    
    def __init__(self, temperature: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
        
        Args:
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        """
        self.temperature = temperature
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """
        Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† logits
        
        Args:
            logits: logits Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ [batch_size, vocab_size]
        
        Returns:
            Ø±Ù…ÙˆØ² Ù…Ø®ØªØ§Ø±Ø© [batch_size, 1]
        """
        raise NotImplementedError
    
    def apply_temperature(self, logits: torch.Tensor) -> torch.Tensor:
        """
        ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø¹Ù„Ù‰ logits
        
        Args:
            logits: logits Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Returns:
            logits Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        """
        if self.temperature != 1.0:
            logits = logits / self.temperature
        return logits


class GreedySampler(Sampler):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¬Ø´Ø¹ (Ø§Ø®ØªÙŠØ§Ø± Ø£Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„)"""
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¬Ø´Ø¹"""
        logits = self.apply_temperature(logits)
        return torch.argmax(logits, dim=-1, keepdim=True)


class RandomSampler(Sampler):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©"""
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©"""
        logits = self.apply_temperature(logits)
        probs = F.softmax(logits, dim=-1)
        return torch.multinomial(probs, num_samples=1)


class TopKSampler(Sampler):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª top-k"""
    
    def __init__(self, k: int = 50, temperature: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© top-k
        
        Args:
            k: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¹Ù„ÙˆÙŠØ©
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        """
        super().__init__(temperature)
        self.k = k
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª top-k"""
        logits = self.apply_temperature(logits)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ k Ù‚ÙŠÙ…Ø©
        top_k_logits, top_k_indices = torch.topk(logits, self.k, dim=-1)
        
        # ØªØ·Ø¨ÙŠÙ‚ softmax Ø¹Ù„Ù‰ top-k ÙÙ‚Ø·
        top_k_probs = F.softmax(top_k_logits, dim=-1)
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† top-k
        sampled_indices = torch.multinomial(top_k_probs, num_samples=1)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ÙØ¹Ù„ÙŠØ©
        sampled_tokens = torch.gather(top_k_indices, -1, sampled_indices)
        
        return sampled_tokens


class TopPSampler(Sampler):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª nucleus (top-p)"""
    
    def __init__(self, p: float = 0.9, temperature: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© top-p
        
        Args:
            p: Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© nucleus
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        """
        super().__init__(temperature)
        self.p = p
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª top-p"""
        logits = self.apply_temperature(logits)
        
        # ØªØ±ØªÙŠØ¨ logits ØªÙ†Ø§Ø²Ù„ÙŠØ§Ù‹
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠØ©
        sorted_probs = F.softmax(sorted_logits, dim=-1)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø¹Ø¯ nucleus
        sorted_indices_to_remove = cumulative_probs > self.p
        
        # ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø®ØªÙŠØ§Ø± Ø±Ù…Ø² ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ù†Ø§Ø¹
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        logits[..., indices_to_remove] = float('-inf')
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
        probs = F.softmax(logits, dim=-1)
        sampled_tokens = torch.multinomial(probs, num_samples=1)
        
        return sampled_tokens


class TypicalSampler(Sampler):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠ"""
    
    def __init__(self, mass: float = 0.9, temperature: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ
        
        Args:
            mass: ÙƒØªÙ„Ø© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        """
        super().__init__(temperature)
        self.mass = mass
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠ"""
        logits = self.apply_temperature(logits)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù„ÙˆØºØ§Ø±ÙŠØªÙ…ÙŠØ©
        log_probs = F.log_softmax(logits, dim=-1)
        probs = torch.exp(log_probs)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ±ÙˆØ¨ÙŠØ§
        entropy = -torch.sum(probs * log_probs, dim=-1, keepdim=True)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø·Ù„Ù‚
        abs_dev = torch.abs(log_probs + entropy)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
        sorted_abs_dev, sorted_indices = torch.sort(abs_dev, dim=-1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠØ©
        sorted_probs = torch.gather(probs, -1, sorted_indices)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ
        mask = cumulative_probs <= self.mass
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠØ©
        typical_indices = sorted_indices[mask]
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø±Ù…ÙˆØ² Ù†Ù…ÙˆØ°Ø¬ÙŠØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„ Ø§Ù„Ø±Ù…ÙˆØ²
        if typical_indices.numel() == 0:
            typical_indices = sorted_indices
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ logits
        typical_mask = torch.zeros_like(logits, dtype=torch.bool)
        typical_mask.scatter_(-1, typical_indices.unsqueeze(-1), True)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ù†Ø§Ø¹
        masked_logits = torch.where(
            typical_mask,
            logits,
            torch.full_like(logits, float('-inf'))
        )
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠØ©
        probs = F.softmax(masked_logits, dim=-1)
        sampled_tokens = torch.multinomial(probs, num_samples=1)
        
        return sampled_tokens


class MirostatSampler(Sampler):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Mirostat (Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø¬Ø¯Ø©)"""
    
    def __init__(self, tau: float = 3.0, learning_rate: float = 0.1, 
                 temperature: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Mirostat
        
        Args:
            tau: Ù‡Ø¯Ù Ø§Ù„Ø§Ù†ØªØ±ÙˆØ¨ÙŠØ§
            learning_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        """
        super().__init__(temperature)
        self.tau = tau
        self.learning_rate = learning_rate
        self.error = 0.0
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Mirostat"""
        logits = self.apply_temperature(logits)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ±ÙˆØ¨ÙŠØ§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        probs = F.softmax(logits, dim=-1)
        log_probs = torch.log(probs + 1e-10)
        entropy = -torch.sum(probs * log_probs, dim=-1).item()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£
        error = self.tau - entropy
        self.error += error
        
        # ØªØ­Ø¯ÙŠØ« Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        self.temperature += self.learning_rate * self.error
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        logits = logits / self.temperature
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        probs = F.softmax(logits, dim=-1)
        sampled_tokens = torch.multinomial(probs, num_samples=1)
        
        return sampled_tokens
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø­Ø§Ù„Ø© Mirostat"""
        self.error = 0.0


class RepetitionPenaltySampler(Sampler):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ø¹ Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    
    def __init__(self, penalty: float = 1.1, temperature: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
        
        Args:
            penalty: Ù‚ÙˆØ© Ø§Ù„Ø¹Ù‚Ø§Ø¨ (>1 Ù„Ù„Ø¹Ù‚Ø§Ø¨ØŒ <1 Ù„Ù„ØªØ´Ø¬ÙŠØ¹)
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        """
        super().__init__(temperature)
        self.penalty = penalty
        self.generated_tokens = []
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ø¹ Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        logits = self.apply_temperature(logits)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù‚Ø§Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
        for token in self.generated_tokens:
            logits[0, token] /= self.penalty
        
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª
        probs = F.softmax(logits, dim=-1)
        sampled_token = torch.multinomial(probs, num_samples=1)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
        self.generated_tokens.append(sampled_token.item())
        
        return sampled_token
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©"""
        self.generated_tokens = []


class BeamSampler:
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¨Ø§Ù„Ø­Ø²Ù…Ø©"""
    
    def __init__(self, num_beams: int = 5, length_penalty: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¨Ø§Ù„Ø­Ø²Ù…Ø©
        
        Args:
            num_beams: Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø²Ù…
            length_penalty: Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ø·ÙˆÙ„
        """
        self.num_beams = num_beams
        self.length_penalty = length_penalty
    
    def sample(self, logits: torch.Tensor, 
               beam_scores: torch.Tensor,
               beam_sequences: List[torch.Tensor]) -> Tuple[List[torch.Tensor], torch.Tensor]:
        """
        Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¨Ø§Ù„Ø­Ø²Ù…Ø©
        
        Args:
            logits: logits Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ [batch_size * num_beams, vocab_size]
            beam_scores: Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ© [batch_size * num_beams]
            beam_sequences: ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        
        Returns:
            ØªØ³Ù„Ø³Ù„Ø§Øª ÙˆØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        """
        batch_size = beam_scores.size(0) // self.num_beams
        
        # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù„ÙˆØºØ§Ø±ÙŠØªÙ…
        log_probs = F.log_softmax(logits, dim=-1)  # [batch_size * num_beams, vocab_size]
        
        # ØªÙˆØ³ÙŠØ¹ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø²Ù…
        beam_scores_expanded = beam_scores.unsqueeze(-1)  # [batch_size * num_beams, 1]
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
        candidate_scores = beam_scores_expanded + log_probs  # [batch_size * num_beams, vocab_size]
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        candidate_scores = candidate_scores.view(
            batch_size, self.num_beams * logits.size(-1)
        )
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù…Ø±Ø´Ø­ÙŠÙ†
        topk_scores, topk_indices = torch.topk(
            candidate_scores, self.num_beams, dim=-1
        )
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø¥Ù„Ù‰ ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ø­Ø²Ù…Ø© ÙˆØ§Ù„Ø±Ù…Ø²
        beam_indices = topk_indices // logits.size(-1)
        token_indices = topk_indices % logits.size(-1)
        
        # ØªØ­Ø¯ÙŠØ« ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø­Ø²Ù…
        new_beam_sequences = []
        for batch_idx in range(batch_size):
            batch_sequences = []
            for beam_idx in range(self.num_beams):
                # ÙÙ‡Ø±Ø³ Ø§Ù„Ø­Ø²Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                original_beam_idx = beam_indices[batch_idx, beam_idx]
                
                # Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ
                original_sequence = beam_sequences[batch_idx * self.num_beams + original_beam_idx]
                
                # Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø¬Ø¯ÙŠØ¯
                new_token = token_indices[batch_idx, beam_idx].unsqueeze(0)
                
                # Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
                new_sequence = torch.cat([original_sequence, new_token])
                batch_sequences.append(new_sequence)
            
            new_beam_sequences.extend(batch_sequences)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ø·ÙˆÙ„
        for i, sequence in enumerate(new_beam_sequences):
            length = sequence.size(0)
            topk_scores.view(-1)[i] = topk_scores.view(-1)[i] / (length ** self.length_penalty)
        
        return new_beam_sequences, topk_scores.view(-1)


class SamplerFactory:
    """Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª"""
    
    @staticmethod
    def create_sampler(sampler_type: str, **kwargs) -> Sampler:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª
        
        Args:
            sampler_type: Ù†ÙˆØ¹ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
        
        Returns:
            Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª
        """
        if sampler_type == 'greedy':
            return GreedySampler(**kwargs)
        elif sampler_type == 'random':
            return RandomSampler(**kwargs)
        elif sampler_type == 'top_k':
            return TopKSampler(**kwargs)
        elif sampler_type == 'top_p':
            return TopPSampler(**kwargs)
        elif sampler_type == 'typical':
            return TypicalSampler(**kwargs)
        elif sampler_type == 'mirostat':
            return MirostatSampler(**kwargs)
        elif sampler_type == 'repetition_penalty':
            return RepetitionPenaltySampler(**kwargs)
        else:
            raise ValueError(f"Ù†ÙˆØ¹ Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {sampler_type}")


class DynamicSampler:
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ ÙŠØºÙŠØ± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
    
    def __init__(self, initial_sampler: Sampler, 
                 change_steps: List[int] = None,
                 sampler_types: List[str] = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        
        Args:
            initial_sampler: Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠ
            change_steps: Ø§Ù„Ø®Ø·ÙˆØ§Øª Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
            sampler_types: Ø£Ù†ÙˆØ§Ø¹ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„ÙƒÙ„ Ù…Ø±Ø­Ù„Ø©
        """
        self.current_sampler = initial_sampler
        self.change_steps = change_steps or [10, 20, 30]
        self.sampler_types = sampler_types or ['top_k', 'top_p', 'greedy']
        self.step = 0
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù„ÙƒÙ„ Ù…Ø±Ø­Ù„Ø©
        self.samplers = []
        for sampler_type in self.sampler_types:
            sampler = SamplerFactory.create_sampler(sampler_type)
            self.samplers.append(sampler)
    
    def sample(self, logits: torch.Tensor) -> torch.Tensor:
        """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ø¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©"""
        # ØªØ­Ø¯ÙŠØ« Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        for i, change_step in enumerate(self.change_steps):
            if self.step == change_step and i < len(self.samplers):
                self.current_sampler = self.samplers[i]
                break
        
        # Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
        result = self.current_sampler.sample(logits)
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯
        self.step += 1
        
        return result
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø­Ø§Ù„Ø© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª"""
        self.step = 0
        self.current_sampler = self.samplers[0] if self.samplers else None


def test_samplers():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ logits Ø§Ø®ØªØ¨Ø§Ø±
    vocab_size = 100
    logits = torch.randn(1, vocab_size)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø´Ø¹
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø´Ø¹:")
    greedy_sampler = GreedySampler()
    greedy_token = greedy_sampler.sample(logits)
    print(f"   Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ø®ØªØ§Ø±: {greedy_token.item()}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± top-k
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª top-k:")
    topk_sampler = TopKSampler(k=10)
    topk_token = topk_sampler.sample(logits)
    print(f"   Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ø®ØªØ§Ø±: {topk_token.item()}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± top-p
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª top-p:")
    top_p_sampler = TopPSampler(p=0.9)
    top_p_token = top_p_sampler.sample(logits)
    print(f"   Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ø®ØªØ§Ø±: {top_p_token.item()}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    print("\n4. Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±:")
    penalty_sampler = RepetitionPenaltySampler(penalty=1.5)
    
    # ØªÙˆÙ„ÙŠØ¯ Ø¹Ø¯Ø© Ø±Ù…ÙˆØ²
    tokens = []
    for _ in range(5):
        token = penalty_sampler.sample(logits)
        tokens.append(token.item())
    
    print(f"   Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©: {tokens}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± SamplerFactory
    print("\n5. Ø§Ø®ØªØ¨Ø§Ø± SamplerFactory:")
    factory = SamplerFactory()
    
    sampler_types = ['greedy', 'top_k', 'top_p']
    for sampler_type in sampler_types:
        sampler = factory.create_sampler(
            sampler_type,
            k=10,
            p=0.9,
            temperature=0.8
        )
        token = sampler.sample(logits)
        print(f"   {sampler_type}: {token.item()}")
    
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
    print("\n6. Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ:")
    dynamic_sampler = DynamicSampler(
        initial_sampler=TopKSampler(k=10),
        change_steps=[2, 4],
        sampler_types=['top_k', 'top_p', 'greedy']
    )
    
    tokens = []
    for i in range(6):
        token = dynamic_sampler.sample(logits)
        tokens.append(token.item())
        print(f"   Ø§Ù„Ø®Ø·ÙˆØ© {i}: {token.item()}")
    
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_samplers()