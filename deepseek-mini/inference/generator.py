# -*- coding: utf-8 -*-
"""
Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ - Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
"""

import torch
import torch.nn.functional as F
from typing import Optional, List, Dict, Any, Tuple, Union
import time
from tqdm import tqdm


class TextGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ"""
    
    def __init__(self, model, config: Dict[str, Any]):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ
        
        Args:
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯
        """
        self.model = model
        self.config = config
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯
        self.max_new_tokens = config.get('max_new_tokens', 512)
        self.temperature = config.get('temperature', 1.0)
        self.top_p = config.get('top_p', 1.0)
        self.top_k = config.get('top_k', 50)
        self.repetition_penalty = config.get('repetition_penalty', 1.1)
        self.do_sample = config.get('do_sample', True)
        self.use_cache = config.get('use_cache', True)
        
        # Ø§Ù„Ø¬Ù‡Ø§Ø²
        self.device = next(model.parameters()).device
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.generation_stats = {
            'total_tokens': 0,
            'total_time': 0,
            'tokens_per_second': 0
        }
    
    def generate(self, 
                 prompt: Union[str, torch.Tensor],
                 max_new_tokens: Optional[int] = None,
                 temperature: Optional[float] = None,
                 top_p: Optional[float] = None,
                 top_k: Optional[int] = None,
                 repetition_penalty: Optional[float] = None,
                 do_sample: Optional[bool] = None,
                 stream: bool = False,
                 stop_tokens: Optional[List[int]] = None) -> str:
        """
        ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…Ù† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©
        
        Args:
            prompt: Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© (Ù†Øµ Ø£Ùˆ tensor)
            max_new_tokens: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
            top_p: Ø¹ÙŠÙ†Ø© nucleus
            top_k: Ø¹ÙŠÙ†Ø© top-k
            repetition_penalty: Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            do_sample: Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª
            stream: Ø¥Ø®Ø±Ø§Ø¬ Ù…ØªØ¯ÙÙ‚
            stop_tokens: Ø±Ù…ÙˆØ² Ø§Ù„ØªÙˆÙ‚Ù
        
        Returns:
            Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯
        """
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø£Ùˆ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        max_new_tokens = max_new_tokens or self.max_new_tokens
        temperature = temperature or self.temperature
        top_p = top_p or self.top_p
        top_k = top_k or self.top_k
        repetition_penalty = repetition_penalty or self.repetition_penalty
        do_sample = do_sample if do_sample is not None else self.do_sample
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        input_ids = self._prepare_inputs(prompt)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if hasattr(self.model, 'transformer'):
            self.model.transformer.reset_cache()
        
        # Ø§Ù„ØªÙˆÙ„ÙŠØ¯
        start_time = time.time()
        
        if stream:
            generated_text = self._generate_streaming(
                input_ids, max_new_tokens, temperature, top_p, 
                top_k, repetition_penalty, do_sample, stop_tokens
            )
        else:
            generated_ids = self._generate_ids(
                input_ids, max_new_tokens, temperature, top_p,
                top_k, repetition_penalty, do_sample, stop_tokens
            )
            generated_text = self._decode_output(generated_ids)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self._update_stats(start_time, generated_ids if not stream else None)
        
        return generated_text
    
    def _prepare_inputs(self, prompt: Union[str, torch.Tensor]) -> torch.Tensor:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ù„Ù…Ø·Ø§Ù„Ø¨Ø©"""
        if isinstance(prompt, str):
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²
            # Ù‡Ù†Ø§ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ tokenizerØŒ Ø³Ù†ÙØªØ±Ø¶ ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            if hasattr(self.model, 'tokenizer'):
                input_ids = self.model.tokenizer.encode(prompt, add_special_tokens=True)
                input_ids = torch.tensor([input_ids], device=self.device)
            else:
                # Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±ØŒ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
                input_ids = torch.randint(100, 1000, (1, 10), device=self.device)
        elif isinstance(prompt, torch.Tensor):
            input_ids = prompt.to(self.device)
        else:
            raise ValueError(f"Ù†ÙˆØ¹ Ù…Ø·Ø§Ù„Ø¨Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {type(prompt)}")
        
        return input_ids
    
    def _generate_ids(self, 
                     input_ids: torch.Tensor,
                     max_new_tokens: int,
                     temperature: float,
                     top_p: float,
                     top_k: int,
                     repetition_penalty: float,
                     do_sample: bool,
                     stop_tokens: Optional[List[int]] = None) -> torch.Tensor:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø±Ù…ÙˆØ²"""
        self.model.eval()
        
        with torch.no_grad():
            generated = input_ids
            
            for i in range(max_new_tokens):
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ logits
                outputs = self.model(
                    input_ids=generated,
                    use_cache=self.use_cache,
                    start_pos=generated.size(1) - 1 if i > 0 else 0
                )
                
                logits = outputs['logits'][:, -1, :] / temperature
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
                if repetition_penalty != 1.0:
                    logits = self._apply_repetition_penalty(logits, generated, repetition_penalty)
                
                # ØªØ·Ø¨ÙŠÙ‚ top-k
                if top_k > 0:
                    logits = self._apply_top_k(logits, top_k)
                
                # ØªØ·Ø¨ÙŠÙ‚ top-p
                if top_p < 1.0:
                    logits = self._apply_top_p(logits, top_p)
                
                # Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø£Ùˆ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ø´Ø¹
                if do_sample:
                    probs = F.softmax(logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(logits, dim=-1, keepdim=True)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ù…ÙˆØ² Ø§Ù„ØªÙˆÙ‚Ù
                if stop_tokens and next_token.item() in stop_tokens:
                    break
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø¬Ø¯ÙŠØ¯
                generated = torch.cat([generated, next_token], dim=-1)
        
        return generated
    
    def _generate_streaming(self, 
                           input_ids: torch.Tensor,
                           max_new_tokens: int,
                           temperature: float,
                           top_p: float,
                           top_k: int,
                           repetition_penalty: float,
                           do_sample: bool,
                           stop_tokens: Optional[List[int]] = None) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…ØªØ¯ÙÙ‚"""
        self.model.eval()
        
        generated = input_ids
        generated_text = ""
        
        with torch.no_grad():
            for i in range(max_new_tokens):
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ logits
                outputs = self.model(
                    input_ids=generated[:, -1:] if i > 0 else generated,
                    use_cache=self.use_cache,
                    start_pos=generated.size(1) - 1 if i > 0 else 0
                )
                
                logits = outputs['logits'][:, -1, :] / temperature
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
                if repetition_penalty != 1.0:
                    logits = self._apply_repetition_penalty(logits, generated, repetition_penalty)
                
                # ØªØ·Ø¨ÙŠÙ‚ top-k
                if top_k > 0:
                    logits = self._apply_top_k(logits, top_k)
                
                # ØªØ·Ø¨ÙŠÙ‚ top-p
                if top_p < 1.0:
                    logits = self._apply_top_p(logits, top_p)
                
                # Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø£Ùˆ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ø´Ø¹
                if do_sample:
                    probs = F.softmax(logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(logits, dim=-1, keepdim=True)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ù…ÙˆØ² Ø§Ù„ØªÙˆÙ‚Ù
                if stop_tokens and next_token.item() in stop_tokens:
                    break
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø¬Ø¯ÙŠØ¯
                generated = torch.cat([generated, next_token], dim=-1)
                
                # ÙÙƒ ØªØ±Ù…ÙŠØ² ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Øµ
                token_text = self._decode_token(next_token.item())
                generated_text += token_text
                
                # Ø¥Ø®Ø±Ø§Ø¬ Ù…ØªØ¯ÙÙ‚
                print(token_text, end='', flush=True)
        
        print()  # Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
        return generated_text
    
    def _apply_repetition_penalty(self, logits: torch.Tensor, 
                                 generated: torch.Tensor, 
                                 penalty: float) -> torch.Tensor:
        """ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        for token in generated[0].unique():
            if token.item() != 0:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ø´Ùˆ
                logits[0, token] /= penalty
        
        return logits
    
    def _apply_top_k(self, logits: torch.Tensor, top_k: int) -> torch.Tensor:
        """ØªØ·Ø¨ÙŠÙ‚ top-k"""
        values, indices = torch.topk(logits, top_k, dim=-1)
        min_values = values[:, -1].unsqueeze(-1)
        
        logits = torch.where(
            logits < min_values,
            torch.full_like(logits, float('-inf')),
            logits
        )
        
        return logits
    
    def _apply_top_p(self, logits: torch.Tensor, top_p: float) -> torch.Tensor:
        """ØªØ·Ø¨ÙŠÙ‚ top-p (nucleus sampling)"""
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø¹Ø¯ top-p
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        logits[0, indices_to_remove] = float('-inf')
        
        return logits
    
    def _decode_output(self, generated_ids: torch.Tensor) -> str:
        """ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª Ø¥Ù„Ù‰ Ù†Øµ"""
        # Ù‡Ù†Ø§ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ tokenizer
        if hasattr(self.model, 'tokenizer'):
            text = self.model.tokenizer.decode(generated_ids[0].tolist())
        else:
            # Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±ØŒ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø³ÙŠØ·
            text = f"[Token IDs: {generated_ids[0].tolist()}]"
        
        return text
    
    def _decode_token(self, token_id: int) -> str:
        """ÙÙƒ ØªØ±Ù…ÙŠØ² Ø±Ù…Ø² ÙˆØ§Ø­Ø¯"""
        if hasattr(self.model, 'tokenizer'):
            token = self.model.tokenizer.decode([token_id])
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
            special_tokens = ['<pad>', '<bos>', '<eos>', '<unk>']
            if any(st in token for st in special_tokens):
                token = ''
        else:
            token = f" {token_id}"
        
        return token
    
    def _update_stats(self, start_time: float, 
                     generated_ids: Optional[torch.Tensor] = None) -> None:
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        if generated_ids is not None:
            num_tokens = generated_ids.size(1)
            tokens_per_second = num_tokens / elapsed_time if elapsed_time > 0 else 0
            
            self.generation_stats['total_tokens'] += num_tokens
            self.generation_stats['total_time'] += elapsed_time
            self.generation_stats['tokens_per_second'] = tokens_per_second
    
    def beam_search(self, 
                    prompt: Union[str, torch.Tensor],
                    num_beams: int = 5,
                    max_new_tokens: int = 100,
                    length_penalty: float = 1.0,
                    early_stopping: bool = True) -> List[Tuple[str, float]]:
        """
        Ø¨Ø­Ø« Ø¨Ø§Ù„Ø­Ø²Ù…Ø© (Beam Search)
        
        Args:
            prompt: Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©
            num_beams: Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø²Ù…
            max_new_tokens: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
            length_penalty: Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ø·ÙˆÙ„
            early_stopping: Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¨ÙƒØ±
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ø¯Ø±Ø¬Ø§Øª
        """
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        input_ids = self._prepare_inputs(prompt)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if hasattr(self.model, 'transformer'):
            self.model.transformer.reset_cache()
        
        # Ø¨Ø­Ø« Ø¨Ø§Ù„Ø­Ø²Ù…Ø©
        beams = self._beam_search_implementation(
            input_ids, num_beams, max_new_tokens, 
            length_penalty, early_stopping
        )
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†ØµÙˆØµ
        results = []
        for beam_ids, score in beams:
            text = self._decode_output(beam_ids.unsqueeze(0))
            results.append((text, score))
        
        return results
    
    def _beam_search_implementation(self, 
                                   input_ids: torch.Tensor,
                                   num_beams: int,
                                   max_new_tokens: int,
                                   length_penalty: float,
                                   early_stopping: bool) -> List[Tuple[torch.Tensor, float]]:
        """ØªÙ†ÙÙŠØ° Ø¨Ø­Ø« Ø¨Ø§Ù„Ø­Ø²Ù…Ø©"""
        self.model.eval()
        
        with torch.no_grad():
            # Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            beams = [(input_ids.clone(), 0.0)]  # (token_ids, log_prob)
            
            for step in range(max_new_tokens):
                new_beams = []
                
                for beam_ids, beam_score in beams:
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ logits Ù„Ù„Ø´Ø¹Ø§Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
                    outputs = self.model(
                        input_ids=beam_ids,
                        use_cache=self.use_cache,
                        start_pos=beam_ids.size(1) - 1
                    )
                    
                    logits = outputs['logits'][:, -1, :]
                    log_probs = F.log_softmax(logits, dim=-1)
                    
                    # Ø£ÙØ¶Ù„ k ØªÙƒÙ…Ù„Ø§Øª
                    topk_log_probs, topk_indices = torch.topk(
                        log_probs, num_beams, dim=-1
                    )
                    
                    for i in range(num_beams):
                        new_token = topk_indices[0, i].unsqueeze(0).unsqueeze(0)
                        new_beam_ids = torch.cat([beam_ids, new_token], dim=-1)
                        
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¯Ø±Ø¬Ø© Ù…Ø¹ Ø¹Ù‚Ø§Ø¨ Ø§Ù„Ø·ÙˆÙ„
                        new_score = beam_score + topk_log_probs[0, i].item()
                        length = new_beam_ids.size(1)
                        penalized_score = new_score / (length ** length_penalty)
                        
                        new_beams.append((new_beam_ids, penalized_score))
                
                # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ num_beams
                new_beams.sort(key=lambda x: x[1], reverse=True)
                beams = new_beams[:num_beams]
                
                # Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¨ÙƒØ± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ø²Ù… Ø§Ù†ØªÙ‡Øª
                if early_stopping:
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ù…ÙˆØ² Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
                    # Ù‡Ù†Ø§ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø±ÙØ© Ø±Ù…Ø² <eos>
                    eos_token = 2  # Ù…Ø«Ø§Ù„
                    all_finished = all(
                        beam_ids[0, -1].item() == eos_token 
                        for beam_ids, _ in beams
                    )
                    
                    if all_finished:
                        break
            
            return beams
    
    def get_generation_stats(self) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        return self.generation_stats.copy()
    
    def reset_stats(self) -> None:
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        self.generation_stats = {
            'total_tokens': 0,
            'total_time': 0,
            'tokens_per_second': 0
        }
    
    def print_stats(self) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        stats = self.get_generation_stats()
        
        print("=" * 60)
        print("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯:")
        print("=" * 60)
        
        if stats['total_time'] > 0:
            print(f"Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ÙƒÙ„ÙŠØ©: {stats['total_tokens']}")
            print(f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙƒÙ„ÙŠ: {stats['total_time']:.2f} Ø«Ø§Ù†ÙŠØ©")
            print(f"Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©: {stats['tokens_per_second']:.2f}")
            print(f"Ø§Ù„ÙˆÙ‚Øª Ù„ÙƒÙ„ Ø±Ù…Ø²: {(stats['total_time'] / stats['total_tokens'] * 1000):.2f} Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©")
        else:
            print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ø¨Ø¹Ø¯")
        
        print("=" * 60)


class StreamingGenerator(TextGenerator):
    """Ù…ÙˆÙ„Ø¯ Ù…ØªØ¯ÙÙ‚ Ù„Ù„Ù†Øµ"""
    
    def __init__(self, model, config: Dict[str, Any]):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù…Ø¯ÙÙ‚"""
        super().__init__(model, config)
        self.callbacks = []
    
    def register_callback(self, callback):
        """ØªØ³Ø¬ÙŠÙ„ Ø±Ø¯ Ø§ØªØµØ§Ù„ Ù„Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª"""
        self.callbacks.append(callback)
    
    def generate_stream(self, 
                       prompt: Union[str, torch.Tensor],
                       max_new_tokens: Optional[int] = None,
                       temperature: Optional[float] = None,
                       top_p: Optional[float] = None,
                       top_k: Optional[int] = None,
                       stop_tokens: Optional[List[int]] = None) -> None:
        """
        ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…ØªØ¯ÙÙ‚ Ù…Ø¹ ØªØ­Ø¯ÙŠØ«Ø§Øª
        
        Args:
            prompt: Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©
            max_new_tokens: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ²
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
            top_p: Ø¹ÙŠÙ†Ø© nucleus
            top_k: Ø¹ÙŠÙ†Ø© top-k
            stop_tokens: Ø±Ù…ÙˆØ² Ø§Ù„ØªÙˆÙ‚Ù
        """
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø£Ùˆ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        max_new_tokens = max_new_tokens or self.max_new_tokens
        temperature = temperature or self.temperature
        top_p = top_p or self.top_p
        top_k = top_k or self.top_k
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        input_ids = self._prepare_inputs(prompt)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if hasattr(self.model, 'transformer'):
            self.model.transformer.reset_cache()
        
        # Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¯ÙÙ‚
        generated = input_ids
        full_text = ""
        
        self._notify_callbacks('start', full_text)
        
        with torch.no_grad():
            for i in range(max_new_tokens):
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ logits
                outputs = self.model(
                    input_ids=generated[:, -1:] if i > 0 else generated,
                    use_cache=self.use_cache,
                    start_pos=generated.size(1) - 1 if i > 0 else 0
                )
                
                logits = outputs['logits'][:, -1, :] / temperature
                
                # ØªØ·Ø¨ÙŠÙ‚ top-k Ùˆ top-p
                if top_k > 0:
                    logits = self._apply_top_k(logits, top_k)
                if top_p < 1.0:
                    logits = self._apply_top_p(logits, top_p)
                
                # Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ù…ÙˆØ² Ø§Ù„ØªÙˆÙ‚Ù
                if stop_tokens and next_token.item() in stop_tokens:
                    self._notify_callbacks('stop', full_text, next_token.item())
                    break
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø¬Ø¯ÙŠØ¯
                generated = torch.cat([generated, next_token], dim=-1)
                
                # ÙÙƒ ØªØ±Ù…ÙŠØ² ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Øµ
                token_text = self._decode_token(next_token.item())
                full_text += token_text
                
                # Ø¥Ø¹Ù„Ø§Ù… Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø§ØªØµØ§Ù„
                self._notify_callbacks('token', token_text, next_token.item())
                self._notify_callbacks('update', full_text, i + 1)
        
        self._notify_callbacks('complete', full_text)
    
    def _notify_callbacks(self, event_type: str, 
                         text: str, 
                         data: Any = None) -> None:
        """Ø¥Ø¹Ù„Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø§ØªØµØ§Ù„"""
        for callback in self.callbacks:
            try:
                callback(event_type, text, data)
            except Exception as e:
                print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ Ø±Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„: {e}")


def test_generator():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙˆÙ„Ø¯"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ø®ØªØ¨Ø§Ø± ØµØºÙŠØ±
    from ..model.tiny_llm import TinyLLM
    
    vocab_size = 1000
    model = TinyLLM(
        vocab_size=vocab_size,
        d_model=128,
        n_heads=4,
        n_layers=2,
        max_seq_len=256
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ„Ø¯
    config = {
        'max_new_tokens': 20,
        'temperature': 0.8,
        'top_p': 0.9,
        'top_k': 50,
        'repetition_penalty': 1.1,
        'do_sample': True,
        'use_cache': True
    }
    
    generator = TextGenerator(model, config)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø§Ø¯ÙŠ:")
    prompt = torch.randint(100, 200, (1, 5))
    
    generated = generator.generate(prompt, max_new_tokens=10)
    print(f"   Ø§Ù„Ù†Ø§ØªØ¬: {generated}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¯ÙÙ‚
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¯ÙÙ‚:")
    print("   Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...")
    stream_output = generator.generate(prompt, max_new_tokens=10, stream=True)
    print(f"   Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„ÙƒØ§Ù…Ù„: {stream_output}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø­Ø« Ø§Ù„Ø­Ø²Ù…Ø©
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø­Ø« Ø§Ù„Ø­Ø²Ù…Ø©:")
    try:
        results = generator.beam_search(
            prompt, 
            num_beams=3, 
            max_new_tokens=10
        )
        
        for i, (text, score) in enumerate(results):
            print(f"   Ø§Ù„Ø­Ø²Ù…Ø© {i+1}: {text[:50]}... (Ø§Ù„Ø¯Ø±Ø¬Ø©: {score:.2f})")
        
        print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        print(f"   âœ— Ø®Ø·Ø£: {e}")
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    print("\n4. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯:")
    generator.print_stats()
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_generator()