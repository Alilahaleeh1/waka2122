# -*- coding: utf-8 -*-
"""
Ø§Ù„ØªØ¶Ù…ÙŠÙ† - ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple


class TokenEmbedding(nn.Module):
    """ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ²"""
    
    def __init__(self, vocab_size: int, d_model: int, padding_idx: int = 0):
        """
        ØªÙ‡ÙŠØ¦Ø© ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ²
        
        Args:
            vocab_size: Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
            padding_idx: ÙÙ‡Ø±Ø³ Ø§Ù„Ø­Ø´Ùˆ
        """
        super().__init__()
        self.embedding = nn.Embedding(
            vocab_size, 
            d_model, 
            padding_idx=padding_idx
        )
        self.d_model = d_model
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_weights()
    
    def _init_weights(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)
        
        # ØªØ¹ÙŠÙŠÙ† ÙˆØ²Ù† Ø§Ù„Ø­Ø´Ùˆ Ø¥Ù„Ù‰ ØµÙØ±
        if self.embedding.padding_idx is not None:
            nn.init.constant_(self.embedding.weight[self.embedding.padding_idx], 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            x: Tensor Ù„Ù„Ø±Ù…ÙˆØ² [batch_size, seq_len]
        
        Returns:
            Tensor Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª [batch_size, seq_len, d_model]
        """
        # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØªØ·Ø¨ÙŠØ¹ Ø­Ø³Ø¨ sqrt(d_model)
        embeddings = self.embedding(x) * math.sqrt(self.d_model)
        return embeddings
    
    def get_embedding_weight(self) -> torch.Tensor:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØ¶Ù…ÙŠÙ†"""
        return self.embedding.weight
    
    def set_embedding_weight(self, weight: torch.Tensor) -> None:
        """ØªØ¹ÙŠÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØ¶Ù…ÙŠÙ†"""
        self.embedding.weight.data.copy_(weight)


class PositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¬ÙŠØ¨ÙŠØ©"""
    
    def __init__(self, d_model: int, max_seq_len: int = 2048, dropout: float = 0.1):
        """
        ØªÙ‡ÙŠØ¦Ø© ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        
        Args:
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            max_seq_len: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
        """
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.d_model = d_model
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        pe = torch.zeros(max_seq_len, d_model)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * 
            (-math.log(10000.0) / d_model)
        )
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø«Ù„Ø«ÙŠØ©
        pe[:, 0::2] = torch.sin(position * div_term)  # Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø²ÙˆØ¬ÙŠØ©
        pe[:, 1::2] = torch.cos(position * div_term)  # Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ÙØ±Ø¯ÙŠØ©
        
        # Ø¥Ø¶Ø§ÙØ© Ø¨Ø¹Ø¯ Ø§Ù„Ø¯ÙØ¹Ø© ÙˆØªØ³Ø¬ÙŠÙ„ ÙƒÙ…Ø¹Ø§Ù…Ù„ Ø«Ø§Ø¨Øª
        pe = pe.unsqueeze(0)  # [1, max_seq_len, d_model]
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            x: Tensor Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª [batch_size, seq_len, d_model]
        
        Returns:
            Tensor Ù…Ø¹ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ [batch_size, seq_len, d_model]
        """
        # Ø¥Ø¶Ø§ÙØ© ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
        x = x + self.pe[:, :x.size(1), :]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ³Ø±Ø¨
        return self.dropout(x)
    
    def get_positional_encoding(self, seq_len: int) -> torch.Tensor:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ù„ØªØ³Ù„Ø³Ù„ Ù…Ø¹ÙŠÙ†
        
        Args:
            seq_len: Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
        
        Returns:
            Tensor Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ [1, seq_len, d_model]
        """
        return self.pe[:, :seq_len, :]


class RotaryPositionalEmbedding(nn.Module):
    """ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø© (RoPE)"""
    
    def __init__(self, d_model: int, max_seq_len: int = 2048):
        """
        ØªÙ‡ÙŠØ¦Ø© RoPE
        
        Args:
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø²ÙˆØ¬ÙŠØ§Ù‹)
            max_seq_len: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
        """
        super().__init__()
        
        if d_model % 2 != 0:
            raise ValueError(f"d_model ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø²ÙˆØ¬ÙŠØ§Ù‹ Ù„Ù€ RoPEØŒ Ù„ÙƒÙ†Ù‡ {d_model}")
        
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø«ÙŠØªØ§ Ù„Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¯ÙˆØ§Ø±
        theta = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))
        self.register_buffer('theta', theta)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª
        self._cache = {}
    
    def _compute_rotary_matrix(self, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØµÙÙˆÙØ© Ø§Ù„Ø¯ÙˆØ§Ø±Ø©"""
        if seq_len in self._cache:
            return self._cache[seq_len]
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        positions = torch.arange(seq_len).float()
        
        # Ø­Ø³Ø§Ø¨ Ø²ÙˆØ§ÙŠØ§ Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
        angles = positions.unsqueeze(1) * self.theta.unsqueeze(0)  # [seq_len, d_model/2]
        
        # Ø­Ø³Ø§Ø¨ Ø¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù… ÙˆØ§Ù„Ø¬ÙŠØ¨
        cos = torch.cos(angles)  # [seq_len, d_model/2]
        sin = torch.sin(angles)  # [seq_len, d_model/2]
        
        # ØªÙˆØ³ÙŠØ¹ Ù„Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø²ÙˆØ¬ÙŠØ© ÙˆØ§Ù„ÙØ±Ø¯ÙŠØ©
        cos = self._repeat_interleave(cos)  # [seq_len, d_model]
        sin = self._repeat_interleave(sin)  # [seq_len, d_model]
        
        # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        self._cache[seq_len] = (cos, sin)
        
        return cos, sin
    
    def _repeat_interleave(self, x: torch.Tensor) -> torch.Tensor:
        """ØªÙƒØ±Ø§Ø± ÙˆØªØ¯Ø§Ø®Ù„ Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø²ÙˆØ¬ÙŠØ© ÙˆØ§Ù„ÙØ±Ø¯ÙŠØ©"""
        # x: [seq_len, d_model/2]
        return x.repeat_interleave(2, dim=1)  # [seq_len, d_model]
    
    def forward(self, x: torch.Tensor, start_pos: int = 0) -> torch.Tensor:
        """
        ØªØ·Ø¨ÙŠÙ‚ RoPE Ø¹Ù„Ù‰ Tensor
        
        Args:
            x: Tensor Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„ [batch_size, seq_len, d_model]
            start_pos: Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ²Ø§ÙŠØ¯)
        
        Returns:
            Tensor Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ RoPE
        """
        batch_size, seq_len, d_model = x.shape
        
        if d_model != self.d_model:
            raise ValueError(f"Ø§Ù„Ø¨Ø¹Ø¯ ØºÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚: {d_model} != {self.d_model}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
        cos, sin = self._compute_rotary_matrix(start_pos + seq_len)
        
        # Ø§Ù‚ØªØµØ§Øµ Ù„Ø­Ø¬Ù… Ø§Ù„ØªØ³Ù„Ø³Ù„
        cos = cos[start_pos:start_pos + seq_len, :].unsqueeze(0)  # [1, seq_len, d_model]
        sin = sin[start_pos:start_pos + seq_len, :].unsqueeze(0)  # [1, seq_len, d_model]
        
        # ÙØµÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø²ÙˆØ¬ÙŠØ© ÙˆØ§Ù„ÙØ±Ø¯ÙŠØ©
        x_even = x[..., 0::2]  # [batch_size, seq_len, d_model/2]
        x_odd = x[..., 1::2]   # [batch_size, seq_len, d_model/2]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
        x_even_rot = x_even * cos[..., 0::2] - x_odd * sin[..., 0::2]
        x_odd_rot = x_odd * cos[..., 1::2] + x_even * sin[..., 1::2]
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        x_rotated = torch.zeros_like(x)
        x_rotated[..., 0::2] = x_even_rot
        x_rotated[..., 1::2] = x_odd_rot
        
        return x_rotated
    
    def apply_rotary_pos_emb(self, q: torch.Tensor, k: torch.Tensor, 
                            start_pos: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ØªØ·Ø¨ÙŠÙ‚ RoPE Ø¹Ù„Ù‰ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆÙ…ÙØ§ØªÙŠØ­ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        
        Args:
            q: Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            k: Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            start_pos: Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        
        Returns:
            Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆÙ…ÙØ§ØªÙŠØ­ Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ RoPE
        """
        q_rotated = self.forward(q, start_pos)
        k_rotated = self.forward(k, start_pos)
        
        return q_rotated, k_rotated


class EmbeddingLayer(nn.Module):
    """Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„ÙƒØ§Ù…Ù„Ø© (Ø±Ù…ÙˆØ² + Ù…ÙˆØ§Ø¶Ø¹)"""
    
    def __init__(self, 
                 vocab_size: int, 
                 d_model: int, 
                 max_seq_len: int = 2048,
                 dropout: float = 0.1,
                 positional_encoding: str = "sinusoidal",
                 padding_idx: int = 0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        
        Args:
            vocab_size: Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            max_seq_len: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
            positional_encoding: Ù†ÙˆØ¹ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
            padding_idx: ÙÙ‡Ø±Ø³ Ø§Ù„Ø­Ø´Ùˆ
        """
        super().__init__()
        
        # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ²
        self.token_embedding = TokenEmbedding(vocab_size, d_model, padding_idx)
        
        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        self.positional_encoding_type = positional_encoding
        
        if positional_encoding == "sinusoidal":
            self.positional_encoding = PositionalEncoding(
                d_model, max_seq_len, dropout
            )
        elif positional_encoding == "rotary":
            self.positional_encoding = RotaryPositionalEmbedding(
                d_model, max_seq_len
            )
            # Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ³Ø±Ø¨ Ù…Ù†ÙØµÙ„ Ù„Ù€ RoPE
            self.dropout = nn.Dropout(dropout)
        else:
            raise ValueError(f"Ù†ÙˆØ¹ ØªØ±Ù…ÙŠØ² Ù…ÙˆØ§Ø¶Ø¹ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {positional_encoding}")
        
        # ØªØ³Ø±Ø¨ Ø¥Ø¶Ø§ÙÙŠ
        self.dropout_layer = nn.Dropout(dropout)
        
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        self.norm = nn.LayerNorm(d_model)
        self.use_norm = True
        
        # Ø§Ù„ØªØ®Ø²ÙŠÙ†
        self.d_model = d_model
        self.vocab_size = vocab_size
    
    def forward(self, x: torch.Tensor, start_pos: int = 0) -> torch.Tensor:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            x: Tensor Ù„Ù„Ø±Ù…ÙˆØ² [batch_size, seq_len]
            start_pos: Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù„Ù€ RoPE)
        
        Returns:
            Tensor Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª [batch_size, seq_len, d_model]
        """
        # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ²
        token_embeddings = self.token_embedding(x)
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        if self.positional_encoding_type == "sinusoidal":
            embeddings = self.positional_encoding(token_embeddings)
        elif self.positional_encoding_type == "rotary":
            # RoPE ÙŠØ·Ø¨Ù‚ Ù„Ø§Ø­Ù‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            embeddings = token_embeddings
            embeddings = self.dropout(embeddings)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ³Ø±Ø¨
        embeddings = self.dropout_layer(embeddings)
        
        # Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        if self.use_norm:
            embeddings = self.norm(embeddings)
        
        return embeddings
    
    def get_input_embeddings(self) -> nn.Module:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø·Ø¨Ù‚Ø© ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª"""
        return self.token_embedding
    
    def set_input_embeddings(self, embedding: nn.Module) -> None:
        """ØªØ¹ÙŠÙŠÙ† Ø·Ø¨Ù‚Ø© ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª"""
        self.token_embedding = embedding
    
    def tie_weights(self, output_embedding: nn.Module) -> None:
        """Ø±Ø¨Ø· Ø£ÙˆØ²Ø§Ù† ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª"""
        if isinstance(output_embedding, nn.Linear):
            # Ø±Ø¨Ø· Ù…Ø¹ Ø·Ø¨Ù‚Ø© Ø®Ø·ÙŠØ©
            output_embedding.weight = self.token_embedding.embedding.weight
        elif isinstance(output_embedding, nn.Embedding):
            # Ø±Ø¨Ø· Ù…Ø¹ ØªØ¶Ù…ÙŠÙ† Ø¢Ø®Ø±
            output_embedding.weight = self.token_embedding.embedding.weight
    
    def compute_positional_encoding(self, seq_len: int) -> torch.Tensor:
        """
        Ø­Ø³Ø§Ø¨ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        
        Args:
            seq_len: Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
        
        Returns:
            Tensor Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        """
        if self.positional_encoding_type == "sinusoidal":
            return self.positional_encoding.get_positional_encoding(seq_len)
        else:
            # Ù„Ù€ RoPEØŒ Ù†Ø­Ø³Ø¨ Ø§Ù„Ù…ØµÙÙˆÙØ© Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
            cos, sin = self.positional_encoding._compute_rotary_matrix(seq_len)
            return cos, sin


class AdaptiveEmbedding(nn.Module):
    """ØªØ¶Ù…ÙŠÙ† ØªÙƒÙŠÙÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"""
    
    def __init__(self, vocab_size: int, d_model: int, 
                 cutoffs: list = [20000, 40000], 
                 div_val: float = 4.0,
                 padding_idx: int = 0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„ØªÙƒÙŠÙÙŠ
        
        Args:
            vocab_size: Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            cutoffs: Ù†Ù‚Ø§Ø· Ù‚Ø·Ø¹ Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            div_val: Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
            padding_idx: ÙÙ‡Ø±Ø³ Ø§Ù„Ø­Ø´Ùˆ
        """
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.cutoffs = cutoffs + [vocab_size]
        self.div_val = div_val
        self.padding_idx = padding_idx
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        self.embeddings = nn.ModuleList()
        self.embedding_dims = []
        
        prev_cutoff = 0
        for i, cutoff in enumerate(self.cutoffs):
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨Ø¹Ø¯ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            if i == 0:
                # Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¯ Ø§Ù„ÙƒØ§Ù…Ù„
                dim = d_model
            else:
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨Ø¹Ø¯ Ø­Ø³Ø¨ div_val
                dim = d_model // (div_val ** i)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ† Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            embedding = nn.Embedding(
                cutoff - prev_cutoff,
                dim,
                padding_idx=padding_idx if i == 0 else None
            )
            
            self.embeddings.append(embedding)
            self.embedding_dims.append(dim)
            
            prev_cutoff = cutoff
        
        # Ø·Ø¨Ù‚Ø© Ø¥Ø³Ù‚Ø§Ø· Ù„Ù…ÙˆØ§Ø¡Ù…Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        self.projection = nn.ModuleList()
        for i, dim in enumerate(self.embedding_dims):
            if dim != d_model:
                proj = nn.Linear(dim, d_model, bias=False)
                self.projection.append(proj)
            else:
                self.projection.append(nn.Identity())
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            x: Tensor Ù„Ù„Ø±Ù…ÙˆØ² [batch_size, seq_len]
        
        Returns:
            Tensor Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª [batch_size, seq_len, d_model]
        """
        batch_size, seq_len = x.shape
        
        # Ø¥Ù†Ø´Ø§Ø¡ Tensor Ù„Ù„Ø¥Ø®Ø±Ø§Ø¬
        output = torch.zeros(batch_size, seq_len, self.d_model, 
                           device=x.device, dtype=torch.float32)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©
        for i, embedding in enumerate(self.embeddings):
            # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ù„Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            if i == 0:
                mask = (x < self.cutoffs[i])
            else:
                mask = (x >= self.cutoffs[i-1]) & (x < self.cutoffs[i])
            
            if mask.any():
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
                indices = x[mask]
                if i > 0:
                    indices = indices - self.cutoffs[i-1]
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
                emb = embedding(indices)
                
                # Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                emb = self.projection[i](emb)
                
                # ÙˆØ¶Ø¹ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„ØµØ­ÙŠØ­Ø©
                output[mask] = emb
        
        return output


def test_embeddings():
    """Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªØ¶Ù…ÙŠÙ†"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
    
    # Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    batch_size = 2
    seq_len = 10
    vocab_size = 1000
    d_model = 512
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    x = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # Ø§Ø®ØªØ¨Ø§Ø± TokenEmbedding
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± TokenEmbedding:")
    token_embedding = TokenEmbedding(vocab_size, d_model)
    token_embeds = token_embedding(x)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {token_embeds.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± PositionalEncoding
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± PositionalEncoding:")
    pos_encoding = PositionalEncoding(d_model, max_seq_len=2048)
    pos_embeds = pos_encoding(token_embeds)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {pos_embeds.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± RotaryPositionalEmbedding
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± RotaryPositionalEmbedding:")
    try:
        rope = RotaryPositionalEmbedding(d_model)
        rope_embeds = rope(token_embeds)
        print(f"   Ø§Ù„Ø´ÙƒÙ„: {rope_embeds.shape}")
        print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        print(f"   âœ— Ø®Ø·Ø£: {e}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± EmbeddingLayer Ø§Ù„ÙƒØ§Ù…Ù„Ø©
    print("\n4. Ø§Ø®ØªØ¨Ø§Ø± EmbeddingLayer:")
    embedding_layer = EmbeddingLayer(
        vocab_size=vocab_size,
        d_model=d_model,
        max_seq_len=2048,
        positional_encoding="sinusoidal"
    )
    full_embeds = embedding_layer(x)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {full_embeds.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_embeddings()