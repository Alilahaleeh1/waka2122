# -*- coding: utf-8 -*-
"""
Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„ØµØºÙŠØ± - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any, List

from .embedding import EmbeddingLayer
from .transformer_block import TransformerStack
from .attention import apply_rotary_pos_emb


class TinyLLM(nn.Module):
    """Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø¹ØµØ¨ÙŠ Ø§Ù„ØµØºÙŠØ±"""
    
    def __init__(self, 
                 vocab_size: int,
                 d_model: int = 768,
                 n_heads: int = 12,
                 n_layers: int = 12,
                 max_seq_len: int = 2048,
                 dropout: float = 0.1,
                 ffn_dim: int = 3072,
                 use_bias: bool = True,
                 positional_encoding: str = "sinusoidal",
                 use_rmsnorm: bool = False,
                 rotary_emb: bool = False,
                 tie_weights: bool = True):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
        
        Args:
            vocab_size: Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            n_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            n_layers: Ø¹Ø¯Ø¯ Ø·Ø¨Ù‚Ø§Øª Transformer
            max_seq_len: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
            ffn_dim: Ø¨Ø¹Ø¯ Ø´Ø¨ÙƒØ© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
            use_bias: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­ÙŠØ²
            positional_encoding: Ù†ÙˆØ¹ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
            use_rmsnorm: Ø§Ø³ØªØ®Ø¯Ø§Ù… RMSNorm
            rotary_emb: Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
            tie_weights: Ø±Ø¨Ø· Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        """
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_layers = n_layers
        self.max_seq_len = max_seq_len
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        self.embedding = EmbeddingLayer(
            vocab_size=vocab_size,
            d_model=d_model,
            max_seq_len=max_seq_len,
            dropout=dropout,
            positional_encoding=positional_encoding,
            padding_idx=0
        )
        
        # ÙƒÙˆÙ…Ø© Transformer
        self.transformer = TransformerStack(
            n_layers=n_layers,
            block_config={
                "block_type": "standard",
                "d_model": d_model,
                "n_heads": n_heads,
                "ffn_dim": ffn_dim,
                "dropout": dropout,
                "activation": "gelu",
                "bias": use_bias,
                "use_rmsnorm": use_rmsnorm,
                "rotary_emb": rotary_emb,
                "max_seq_len": max_seq_len
            }
        )
        
        # ØªØ·Ø¨ÙŠØ¹ Ù†Ù‡Ø§Ø¦ÙŠ
        if use_rmsnorm:
            from .transformer_block import RMSNorm
            self.norm = RMSNorm(d_model)
        else:
            self.norm = nn.LayerNorm(d_model)
        
        # Ø±Ø£Ø³ Ø§Ù„Ù„ØºØ© (Language Model Head)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
        # Ø±Ø¨Ø· Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¥Ø°Ø§ Ø·Ù„Ø¨
        if tie_weights:
            self.embedding.tie_weights(self.lm_head)
        
        # Ø§Ù„ØªØ³Ø±Ø¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self.dropout = nn.Dropout(dropout)
        
        # Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„ØªÙˆÙ„ÙŠØ¯
        self._cache = None
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_weights()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.total_params = self._count_parameters()
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ TinyLLM:")
        print(f"   Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª: {self.total_params:,}")
        print(f"   Ø§Ù„Ø·Ø¨Ù‚Ø§Øª: {n_layers}")
        print(f"   Ø§Ù„Ø¨Ø¹Ø¯: {d_model}")
        print(f"   Ø§Ù„Ø±Ø¤ÙˆØ³: {n_heads}")
    
    def _init_weights(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        # ØªÙ‡ÙŠØ¦Ø© Ø±Ø£Ø³ Ø§Ù„Ù„ØºØ©
        nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.02)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        if isinstance(self.norm, nn.LayerNorm):
            nn.init.ones_(self.norm.weight)
            nn.init.zeros_(self.norm.bias)
    
    def _count_parameters(self) -> int:
        """Ø¹Ø¯ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def forward(self, 
                input_ids: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                use_cache: bool = False,
                start_pos: int = 0) -> Dict[str, torch.Tensor]:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            input_ids: Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch_size, seq_len]
            attention_mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ [batch_size, seq_len]
            labels: ØªØ³Ù…ÙŠØ§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ [batch_size, seq_len]
            use_cache: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            start_pos: Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ²Ø§ÙŠØ¯)
        
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ logits ÙˆØ®Ø³Ø§Ø¦Ø±
        """
        batch_size, seq_len = input_ids.shape
        
        # 1. Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        x = self.embedding(input_ids, start_pos=start_pos)
        
        # 2. Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø³Ø¨Ø¨ÙŠ
        if attention_mask is None:
            attention_mask = torch.ones(batch_size, seq_len, 
                                       device=input_ids.device).bool()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø³Ø¨Ø¨ÙŠ
        causal_mask = self._create_causal_mask(seq_len, input_ids.device)
        
        # Ø¯Ù…Ø¬ Ù…Ø¹ Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        if attention_mask is not None:
            # ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]
            combined_mask = causal_mask & attention_mask
        else:
            combined_mask = causal_mask
        
        # 3. Ø·Ø¨Ù‚Ø§Øª Transformer
        x, all_attn_weights = self.transformer(
            x, 
            mask=combined_mask,
            use_cache=use_cache,
            start_pos=start_pos
        )
        
        # 4. Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        x = self.norm(x)
        
        # 5. Ø±Ø£Ø³ Ø§Ù„Ù„ØºØ©
        logits = self.lm_head(x)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
        loss = None
        if labels is not None:
            # Ø§Ù‚ØªØµØ§Øµ logits Ù„Ù„ØªØ³Ù…ÙŠØ§Øª
            logits_for_loss = logits[..., :-1, :].contiguous()
            labels_for_loss = labels[..., 1:].contiguous()
            
            # Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠ
            loss = F.cross_entropy(
                logits_for_loss.view(-1, logits_for_loss.size(-1)),
                labels_for_loss.view(-1),
                ignore_index=0  # ØªØ¬Ø§Ù‡Ù„ ÙÙ‡Ø±Ø³ Ø§Ù„Ø­Ø´Ùˆ
            )
        
        return {
            "logits": logits,
            "loss": loss,
            "attention_weights": all_attn_weights,
            "hidden_states": x
        }
    
    def _create_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø³Ø¨Ø¨ÙŠ"""
        return torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()
    
    def generate(self, 
                input_ids: torch.Tensor,
                max_new_tokens: int = 100,
                temperature: float = 1.0,
                top_p: float = 1.0,
                top_k: int = 0,
                repetition_penalty: float = 1.0,
                do_sample: bool = True,
                use_cache: bool = True) -> torch.Tensor:
        """
        ØªÙˆÙ„ÙŠØ¯ Ù†Øµ
        
        Args:
            input_ids: Ø±Ù…ÙˆØ² Ø§Ù„Ø¨Ø¯Ø¡ [batch_size, seq_len]
            max_new_tokens: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ù„Ù„Ø¹ÙŠÙ†Ø©
            top_p: Ø¹ÙŠÙ†Ø© nucleus
            top_k: Ø¹ÙŠÙ†Ø© top-k
            repetition_penalty: Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            do_sample: Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø£Ùˆ Ø§Ø®ØªÙŠØ§Ø± Ø¬Ø´Ø¹
            use_cache: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        
        Returns:
            Ø±Ù…ÙˆØ² Ù…ÙˆÙ„Ø¯Ø©
        """
        self.eval()
        batch_size = input_ids.shape[0]
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if use_cache:
            self.transformer.reset_cache()
        
        with torch.no_grad():
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ù…ÙˆØ²
            generated = input_ids
            
            for i in range(max_new_tokens):
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ logits Ù„Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø­Ø§Ù„ÙŠØ©
                outputs = self.forward(
                    input_ids=generated if i == 0 else generated[:, -1:],
                    use_cache=use_cache,
                    start_pos=generated.shape[1] - 1 if i > 0 else 0
                )
                
                logits = outputs["logits"]
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ logits Ù„Ù„Ø±Ù…Ø² Ø§Ù„ØªØ§Ù„ÙŠ
                next_token_logits = logits[:, -1, :] / temperature
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
                if repetition_penalty != 1.0:
                    self._apply_repetition_penalty(next_token_logits, generated, repetition_penalty)
                
                # ØªØ·Ø¨ÙŠÙ‚ top-k
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)
                    min_logits = top_k_logits[:, -1].unsqueeze(-1)
                    next_token_logits = torch.where(
                        next_token_logits < min_logits,
                        torch.full_like(next_token_logits, float('-inf')),
                        next_token_logits
                    )
                
                # ØªØ·Ø¨ÙŠÙ‚ top-p (nucleus sampling)
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    # Ø¥Ø²Ø§ÙØ© Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø¹Ø¯ top-p
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    
                    for idx in range(batch_size):
                        indices_to_remove = sorted_indices[idx][sorted_indices_to_remove[idx]]
                        next_token_logits[idx][indices_to_remove] = float('-inf')
                
                # Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø£Ùˆ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ø´Ø¹
                if do_sample:
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø¬Ø¯ÙŠØ¯
                generated = torch.cat([generated, next_token], dim=-1)
        
        return generated
    
    def _apply_repetition_penalty(self, logits: torch.Tensor, 
                                 generated: torch.Tensor, 
                                 penalty: float) -> None:
        """ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù‚Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        for batch_idx in range(generated.shape[0]):
            for token in generated[batch_idx].unique():
                if token.item() != 0:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ø´Ùˆ
                    logits[batch_idx, token] /= penalty
    
    def save_pretrained(self, save_path: str) -> None:
        """
        Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            save_path: Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸
        """
        import os
        os.makedirs(save_path, exist_ok=True)
        
        # Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model_path = os.path.join(save_path, "model.pt")
        torch.save({
            'model_state_dict': self.state_dict(),
            'config': self.get_config()
        }, model_path)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        config_path = os.path.join(save_path, "config.json")
        import json
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.get_config(), f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ {save_path}")
    
    @classmethod
    def from_pretrained(cls, load_path: str) -> 'TinyLLM':
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            load_path: Ù…Ø³Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
        
        Returns:
            Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù…Ù„
        """
        import json
        import os
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        config_path = os.path.join(load_path, "config.json")
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model = cls(**config)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        model_path = os.path.join(load_path, "model.pt")
        checkpoint = torch.load(model_path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† {load_path}")
        return model
    
    def get_config(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        return {
            "vocab_size": self.vocab_size,
            "d_model": self.d_model,
            "n_heads": self.n_heads,
            "n_layers": self.n_layers,
            "max_seq_len": self.max_seq_len,
            "dropout": self.dropout.p if hasattr(self.dropout, 'p') else 0.1,
            "ffn_dim": getattr(self.transformer.blocks[0].ffn, 'fc1', None).out_features 
                      if hasattr(self.transformer.blocks[0], 'ffn') else 3072,
            "use_bias": True,  # ÙŠÙ…ÙƒÙ† Ø¬Ù„Ø¨Ù‡Ø§ Ù…Ù† Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
            "positional_encoding": self.embedding.positional_encoding_type,
            "use_rmsnorm": isinstance(self.norm, type(self.transformer.blocks[0].norm1)),
            "rotary_emb": hasattr(self.transformer.blocks[0], 'rotary_emb') 
                         and self.transformer.blocks[0].rotary_emb is not None,
            "tie_weights": self.lm_head.weight is self.embedding.token_embedding.embedding.weight
        }
    
    def print_model_info(self) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        print("=" * 60)
        print("ğŸ§  Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
        print("=" * 60)
        
        config = self.get_config()
        for key, value in config.items():
            print(f"{key}: {value}")
        
        print(f"\nØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {self.total_params:,}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        print("\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª:")
        for name, param in self.named_parameters():
            if param.requires_grad:
                print(f"  {name}: {param.numel():,}")
        
        print("=" * 60)


class TinyLLMForSequenceClassification(TinyLLM):
    """Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ³Ù„Ø³Ù„"""
    
    def __init__(self, 
                 vocab_size: int,
                 num_labels: int = 2,
                 **kwargs):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØµÙ†ÙŠÙ
        
        Args:
            vocab_size: Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            num_labels: Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù€ TinyLLM
        """
        super().__init__(vocab_size, **kwargs)
        
        # Ø±Ø£Ø³ Ø§Ù„ØªØµÙ†ÙŠÙ
        self.classifier = nn.Linear(self.d_model, num_labels)
        
        # Ø±Ø£Ø³ Ø§Ù„ÙƒØ´Ù
        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø±Ø£Ø³ Ø§Ù„ØªØµÙ†ÙŠÙ
        nn.init.normal_(self.classifier.weight, mean=0.0, std=0.02)
        if self.classifier.bias is not None:
            nn.init.zeros_(self.classifier.bias)
    
    def forward(self, 
                input_ids: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù„Ù„ØªØµÙ†ÙŠÙ
        
        Args:
            input_ids: Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            attention_mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            labels: ØªØ³Ù…ÙŠØ§Øª Ø§Ù„ÙØ¦Ø§Øª
        
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ logits ÙˆØ®Ø³Ø§Ø¦Ø±
        """
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        outputs = super().forward(input_ids, attention_mask)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø£ÙˆÙ„ Ø±Ù…Ø² ([CLS] Ø£Ùˆ Ø£ÙˆÙ„ Ø±Ù…Ø²)
        pooled_output = outputs["hidden_states"][:, 0, :]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ³Ø±Ø¨ ÙˆØ§Ù„ØªØµÙ†ÙŠÙ
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
        loss = None
        if labels is not None:
            loss = F.cross_entropy(logits, labels)
        
        return {
            "logits": logits,
            "loss": loss,
            "hidden_states": outputs["hidden_states"],
            "attention_weights": outputs["attention_weights"]
        }


class TinyLLMForQuestionAnswering(TinyLLM):
    """Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"""
    
    def __init__(self, 
                 vocab_size: int,
                 **kwargs):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        
        Args:
            vocab_size: Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù€ TinyLLM
        """
        super().__init__(vocab_size, **kwargs)
        
        # Ø±Ø£Ø³ Ø¨Ø¯Ø§ÙŠØ© ÙˆÙ†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        self.qa_outputs = nn.Linear(self.d_model, 2)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø©
        nn.init.normal_(self.qa_outputs.weight, mean=0.0, std=0.02)
        if self.qa_outputs.bias is not None:
            nn.init.zeros_(self.qa_outputs.bias)
    
    def forward(self, 
                input_ids: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                start_positions: Optional[torch.Tensor] = None,
                end_positions: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        
        Args:
            input_ids: Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            attention_mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            start_positions: Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            end_positions: Ù…ÙˆØ§Ù‚Ø¹ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ logits ÙˆØ®Ø³Ø§Ø¦Ø±
        """
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        outputs = super().forward(input_ids, attention_mask)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ logits Ù„Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©
        seq_output = outputs["hidden_states"]
        logits = self.qa_outputs(seq_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1)
        end_logits = end_logits.squeeze(-1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ù…ÙˆØ¬ÙˆØ¯Ø©
        loss = None
        if start_positions is not None and end_positions is not None:
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ø´Ùˆ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
            loss_fn = nn.CrossEntropyLoss(ignore_index=-1)
            start_loss = loss_fn(start_logits, start_positions)
            end_loss = loss_fn(end_logits, end_positions)
            loss = (start_loss + end_loss) / 2
        
        return {
            "start_logits": start_logits,
            "end_logits": end_logits,
            "loss": loss,
            "hidden_states": outputs["hidden_states"],
            "attention_weights": outputs["attention_weights"]
        }


def create_model_from_config(config: Dict[str, Any]) -> TinyLLM:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    
    Args:
        config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    
    Returns:
        Ù†Ù…ÙˆØ°Ø¬ TinyLLM
    """
    model_config = config.get("model", {})
    
    return TinyLLM(
        vocab_size=model_config.get("vocab_size", 50000),
        d_model=model_config.get("d_model", 768),
        n_heads=model_config.get("n_heads", 12),
        n_layers=model_config.get("n_layers", 12),
        max_seq_len=model_config.get("max_seq_len", 2048),
        dropout=model_config.get("dropout", 0.1),
        ffn_dim=model_config.get("ffn_dim", 3072),
        use_bias=model_config.get("use_bias", True),
        positional_encoding=model_config.get("positional_encoding", "sinusoidal"),
        use_rmsnorm=model_config.get("use_rmsnorm", False),
        rotary_emb=model_config.get("rotary_emb", False),
        tie_weights=model_config.get("tie_weights", True)
    )


def test_tiny_llm():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± TinyLLM...")
    
    # Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    batch_size = 2
    seq_len = 20
    vocab_size = 5000
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len).bool()
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± TinyLLM Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:")
    model = TinyLLM(
        vocab_size=vocab_size,
        d_model=256,
        n_heads=4,
        n_layers=2,
        max_seq_len=512,
        dropout=0.0  # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø±Ø¨ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    )
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ
    outputs = model(input_ids, attention_mask, labels)
    print(f"   logits shape: {outputs['logits'].shape}")
    print(f"   loss: {outputs['loss']}")
    print(f"   Ø¹Ø¯Ø¯ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…: {len(outputs['attention_weights'])}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ:")
    generated = model.generate(
        input_ids=input_ids[:, :5],  # 5 Ø±Ù…ÙˆØ² Ø¨Ø¯Ø§ÙŠØ©
        max_new_tokens=10,
        temperature=0.8,
        do_sample=True
    )
    print(f"   Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù…ÙˆÙ„Ø¯: {generated.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø­ÙØ¸ ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± Ø­ÙØ¸ ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
    import tempfile
    import os
    
    with tempfile.TemporaryDirectory() as tmpdir:
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model.save_pretrained(tmpdir)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        loaded_model = TinyLLM.from_pretrained(tmpdir)
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        outputs1 = model(input_ids, attention_mask)
        outputs2 = loaded_model(input_ids, attention_mask)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        logits_diff = torch.abs(outputs1['logits'] - outputs2['logits']).max().item()
        print(f"   Ø§Ù„ÙØ±Ù‚ Ø§Ù„Ø£Ù‚ØµÙ‰ ÙÙŠ logits: {logits_diff:.6f}")
        
        if logits_diff < 1e-5:
            print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
        else:
            print(f"   âœ— Ø®Ø·Ø£: Ø§Ù„ÙØ±Ù‚ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹")
    
    # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("\n4. Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
    model.print_model_info()
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± TinyLLM Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_tiny_llm()