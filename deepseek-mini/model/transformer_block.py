# -*- coding: utf-8 -*-
"""
ÙƒØªÙ„Ø© Transformer - Ø§Ù„Ù„Ø¨Ù†Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØ©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any

from .attention import CausalSelfAttention
from .embedding import RotaryPositionalEmbedding


class FeedForward(nn.Module):
    """Ø´Ø¨ÙƒØ© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©"""
    
    def __init__(self, 
                 d_model: int, 
                 ffn_dim: int, 
                 dropout: float = 0.1,
                 activation: str = "gelu",
                 bias: bool = True):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø´Ø¨ÙƒØ© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
        
        Args:
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            ffn_dim: Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
            activation: Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ·
            bias: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­ÙŠØ²
        """
        super().__init__()
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
        self.fc1 = nn.Linear(d_model, ffn_dim, bias=bias)
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
        self.fc2 = nn.Linear(ffn_dim, d_model, bias=bias)
        
        # Ø§Ù„ØªØ³Ø±Ø¨
        self.dropout = nn.Dropout(dropout)
        
        # Ø§Ù„ØªÙ†Ø´ÙŠØ·
        self.activation_fn = self._get_activation_fn(activation)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_weights()
    
    def _get_activation_fn(self, activation: str):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ·"""
        if activation == "gelu":
            return F.gelu
        elif activation == "relu":
            return F.relu
        elif activation == "silu" or activation == "swish":
            return F.silu
        elif activation == "tanh":
            return torch.tanh
        else:
            raise ValueError(f"Ø¯Ø§Ù„Ø© ØªÙ†Ø´ÙŠØ· ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©: {activation}")
    
    def _init_weights(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        # ØªÙ‡ÙŠØ¦Ø© He Ù„Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
        nn.init.xavier_uniform_(self.fc2.weight)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ­ÙŠØ²Ø§Øª
        if self.fc1.bias is not None:
            nn.init.constant_(self.fc1.bias, 0)
        if self.fc2.bias is not None:
            nn.init.constant_(self.fc2.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            x: Tensor Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch_size, seq_len, d_model]
        
        Returns:
            Tensor Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª [batch_size, seq_len, d_model]
        """
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ø¹ Ø§Ù„ØªÙ†Ø´ÙŠØ·
        hidden = self.fc1(x)
        hidden = self.activation_fn(hidden)
        hidden = self.dropout(hidden)
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
        output = self.fc2(hidden)
        output = self.dropout(output)
        
        return output


class RMSNorm(nn.Module):
    """ØªØ·Ø¨ÙŠØ¹ RMS (Ø¨Ø¯ÙŠÙ„ Ù„Ù€ LayerNorm)"""
    
    def __init__(self, dim: int, eps: float = 1e-6):
        """
        ØªÙ‡ÙŠØ¦Ø© RMSNorm
        
        Args:
            dim: Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
            eps: Ù‚ÙŠÙ…Ø© epsilon Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø¯Ø¯ÙŠ
        """
        super().__init__()
        self.dim = dim
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            x: Tensor Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        
        Returns:
            Tensor Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        """
        # Ø­Ø³Ø§Ø¨ RMS (Ø¬Ø°Ø± Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª)
        rms = torch.sqrt(torch.mean(x.pow(2), dim=-1, keepdim=True) + self.eps)
        
        # Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        x_norm = x / rms
        
        # Ø§Ù„ØªÙˆØ³ÙŠØ¹
        return x_norm * self.weight


class TransformerBlock(nn.Module):
    """ÙƒØªÙ„Ø© Transformer ÙƒØ§Ù…Ù„Ø©"""
    
    def __init__(self, 
                 d_model: int,
                 n_heads: int,
                 ffn_dim: int,
                 dropout: float = 0.1,
                 activation: str = "gelu",
                 bias: bool = True,
                 use_rmsnorm: bool = False,
                 rotary_emb: bool = False,
                 max_seq_len: int = 2048):
        """
        ØªÙ‡ÙŠØ¦Ø© ÙƒØªÙ„Ø© Transformer
        
        Args:
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            n_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            ffn_dim: Ø¨Ø¹Ø¯ Ø´Ø¨ÙƒØ© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
            activation: Ø¯Ø§Ù„Ø© ØªÙ†Ø´ÙŠØ· Ø§Ù„Ù€ FFN
            bias: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­ÙŠØ²
            use_rmsnorm: Ø§Ø³ØªØ®Ø¯Ø§Ù… RMSNorm Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† LayerNorm
            rotary_emb: Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
            max_seq_len: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„
        """
        super().__init__()
        
        # Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø³Ø¨Ø¨ÙŠ
        self.attention = CausalSelfAttention(
            d_model=d_model,
            n_heads=n_heads,
            dropout=dropout,
            bias=bias
        )
        
        # Ø´Ø¨ÙƒØ© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
        self.ffn = FeedForward(
            d_model=d_model,
            ffn_dim=ffn_dim,
            dropout=dropout,
            activation=activation,
            bias=bias
        )
        
        # ØªØ·Ø¨ÙŠØ¹ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        if use_rmsnorm:
            self.norm1 = RMSNorm(d_model)
            self.norm2 = RMSNorm(d_model)
        else:
            self.norm1 = nn.LayerNorm(d_model)
            self.norm2 = nn.LayerNorm(d_model)
        
        # Ø§Ù„ØªØ³Ø±Ø¨ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ
        self.dropout = nn.Dropout(dropout)
        
        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
        self.rotary_emb = None
        if rotary_emb:
            self.rotary_emb = RotaryPositionalEmbedding(
                d_model=d_model,
                max_seq_len=max_seq_len
            )
    
    def forward(self, 
                x: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                start_pos: int = 0) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            x: Tensor Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch_size, seq_len, d_model]
            mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            cache: Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù‚ÙŠÙ…
            start_pos: Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ²Ø§ÙŠØ¯)
        
        Returns:
            Tensor Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙˆØ°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        """
        # ØªØ­Ø¶ÙŠØ± ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
        rotary_pos_emb = None
        if self.rotary_emb is not None:
            # Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
            cos, sin = self.rotary_emb._compute_rotary_matrix(start_pos + x.size(1))
            rotary_pos_emb = (cos, sin)
        
        # ØªØ·Ø¨ÙŠØ¹ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        norm_x = self.norm1(x)
        
        # Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø³Ø¨Ø¨ÙŠ
        attn_output, attn_weights, new_cache = self.attention(
            norm_x, 
            mask=mask,
            rotary_pos_emb=rotary_pos_emb,
            cache=cache
        )
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ Ù…Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        x = x + self.dropout(attn_output)
        
        # ØªØ·Ø¨ÙŠØ¹ Ù…Ø§ Ù‚Ø¨Ù„ FFN
        norm_x = self.norm2(x)
        
        # Ø´Ø¨ÙƒØ© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
        ffn_output = self.ffn(norm_x)
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ Ù…Ø¹ FFN
        x = x + self.dropout(ffn_output)
        
        return x, attn_weights, new_cache


class ParallelTransformerBlock(TransformerBlock):
    """ÙƒØªÙ„Ø© Transformer Ù…ØªÙˆØ§Ø²ÙŠØ© (Ù…Ø«Ù„ ÙÙŠ PaLM)"""
    
    def __init__(self, 
                 d_model: int,
                 n_heads: int,
                 ffn_dim: int,
                 dropout: float = 0.1,
                 activation: str = "gelu",
                 bias: bool = True,
                 use_rmsnorm: bool = False,
                 rotary_emb: bool = False,
                 max_seq_len: int = 2048):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©"""
        super().__init__(d_model, n_heads, ffn_dim, dropout, activation, 
                        bias, use_rmsnorm, rotary_emb, max_seq_len)
    
    def forward(self, 
                x: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                start_pos: int = 0) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù…ØªÙˆØ§Ø²ÙŠ"""
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        norm_x = self.norm1(x)
        
        # ØªØ­Ø¶ÙŠØ± ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
        rotary_pos_emb = None
        if self.rotary_emb is not None:
            cos, sin = self.rotary_emb._compute_rotary_matrix(start_pos + x.size(1))
            rotary_pos_emb = (cos, sin)
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙˆFFN Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        attn_output, attn_weights, new_cache = self.attention(
            norm_x, 
            mask=mask,
            rotary_pos_emb=rotary_pos_emb,
            cache=cache
        )
        
        ffn_output = self.ffn(norm_x)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        combined_output = attn_output + ffn_output
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
        x = x + self.dropout(combined_output)
        
        # ØªØ·Ø¨ÙŠØ¹ Ù†Ù‡Ø§Ø¦ÙŠ
        x = self.norm2(x)
        
        return x, attn_weights, new_cache


class GLUFeedForward(FeedForward):
    """Ø´Ø¨ÙƒØ© ØªØºØ°ÙŠØ© Ø£Ù…Ø§Ù…ÙŠØ© Ù…Ø¹ Ø¨ÙˆØ§Ø¨Ø© Ø®Ø·ÙŠØ© (GLU)"""
    
    def __init__(self, 
                 d_model: int, 
                 ffn_dim: int, 
                 dropout: float = 0.1,
                 activation: str = "silu",
                 bias: bool = True):
        """ØªÙ‡ÙŠØ¦Ø© GLU FFN"""
        super().__init__(d_model, ffn_dim, dropout, activation, bias)
        
        # Ø·Ø¨Ù‚Ø© Ø¨ÙˆØ§Ø¨Ø© Ø¥Ø¶Ø§ÙÙŠØ©
        self.gate = nn.Linear(d_model, ffn_dim, bias=bias)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_glu_weights()
    
    def _init_glu_weights(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø£ÙˆØ²Ø§Ù† GLU"""
        nn.init.kaiming_normal_(self.gate.weight, nonlinearity='relu')
        if self.gate.bias is not None:
            nn.init.constant_(self.gate.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù…Ø¹ GLU"""
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø¨ÙˆØ§Ø¨Ø©
        hidden = self.fc1(x)
        gate = self.gate(x)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø´ÙŠØ· ÙˆØ§Ù„Ø¨ÙˆØ§Ø¨Ø©
        hidden = self.activation_fn(hidden) * gate.sigmoid()
        hidden = self.dropout(hidden)
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
        output = self.fc2(hidden)
        output = self.dropout(output)
        
        return output


class MoEBlock(TransformerBlock):
    """ÙƒØªÙ„Ø© Transformer Ù…Ø¹ Ø®Ø¨Ø±Ø§Ø¡ Ù…ØªØ¹Ø¯Ø¯ÙŠÙ† (MoE)"""
    
    def __init__(self, 
                 d_model: int,
                 n_heads: int,
                 ffn_dim: int,
                 num_experts: int = 8,
                 top_k: int = 2,
                 dropout: float = 0.1,
                 activation: str = "gelu",
                 bias: bool = True,
                 use_rmsnorm: bool = False,
                 rotary_emb: bool = False,
                 max_seq_len: int = 2048):
        """
        ØªÙ‡ÙŠØ¦Ø© ÙƒØªÙ„Ø© MoE
        
        Args:
            num_experts: Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡
            top_k: Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ø®ØªÙŠØ§Ø±Ù‡Ù…
        """
        super().__init__(d_model, n_heads, ffn_dim, dropout, activation, 
                        bias, use_rmsnorm, rotary_emb, max_seq_len)
        
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ FFN Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© Ø¨Ù€ MoE
        self.num_experts = num_experts
        self.top_k = top_k
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡
        self.experts = nn.ModuleList([
            FeedForward(d_model, ffn_dim, dropout, activation, bias)
            for _ in range(num_experts)
        ])
        
        # Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡
        self.gate = nn.Linear(d_model, num_experts, bias=False)
        
        # Ø§Ù„ØªØ³Ø±Ø¨ Ù„Ù„Ø¨ÙˆØ§Ø¨Ø©
        self.gate_dropout = nn.Dropout(dropout)
    
    def forward(self, 
                x: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                start_pos: int = 0) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù…Ø¹ MoE"""
        # Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… (ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©)
        norm_x = self.norm1(x)
        
        rotary_pos_emb = None
        if self.rotary_emb is not None:
            cos, sin = self.rotary_emb._compute_rotary_matrix(start_pos + x.size(1))
            rotary_pos_emb = (cos, sin)
        
        attn_output, attn_weights, new_cache = self.attention(
            norm_x, 
            mask=mask,
            rotary_pos_emb=rotary_pos_emb,
            cache=cache
        )
        
        x = x + self.dropout(attn_output)
        
        # MoE Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† FFN
        norm_x = self.norm2(x)
        
        # Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©
        gate_logits = self.gate(norm_x)  # [batch_size, seq_len, num_experts]
        gate_logits = self.gate_dropout(gate_logits)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ k Ø®Ø¨Ø±Ø§Ø¡
        top_k_gate_logits, top_k_indices = torch.topk(
            gate_logits, 
            k=self.top_k, 
            dim=-1
        )
        
        # ØªØ·Ø¨ÙŠÙ‚ softmax Ø¹Ù„Ù‰ top-k
        gate_weights = F.softmax(top_k_gate_logits, dim=-1)
        
        # ØªØ¬Ù…ÙŠØ¹ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡
        moe_output = torch.zeros_like(norm_x)
        
        for i in range(self.top_k):
            expert_indices = top_k_indices[..., i]  # [batch_size, seq_len]
            weights = gate_weights[..., i].unsqueeze(-1)  # [batch_size, seq_len, 1]
            
            # ØªØ·Ø¨ÙŠÙ‚ ÙƒÙ„ Ø®Ø¨ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
            for expert_idx in range(self.num_experts):
                # Ù‚Ù†Ø§Ø¹ Ù„Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨ÙŠØ±
                mask = (expert_indices == expert_idx)
                
                if mask.any():
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø®Ø¨ÙŠØ±
                    expert_output = self.experts[expert_idx](
                        norm_x * mask.unsqueeze(-1).float()
                    )
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù…Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                    moe_output = moe_output + expert_output * weights * mask.unsqueeze(-1).float()
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
        x = x + self.dropout(moe_output)
        
        return x, attn_weights, new_cache


class TransformerBlockFactory:
    """Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† ÙƒØªÙ„ Transformer"""
    
    @staticmethod
    def create_block(block_type: str, **kwargs):
        """
        Ø¥Ù†Ø´Ø§Ø¡ ÙƒØªÙ„Ø© Transformer
        
        Args:
            block_type: Ù†ÙˆØ¹ Ø§Ù„ÙƒØªÙ„Ø©
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙƒØªÙ„Ø©
        
        Returns:
            ÙƒØªÙ„Ø© Transformer
        """
        if block_type == "standard":
            return TransformerBlock(**kwargs)
        elif block_type == "parallel":
            return ParallelTransformerBlock(**kwargs)
        elif block_type == "moe":
            return MoEBlock(**kwargs)
        elif block_type == "glu":
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ FFN Ø¨Ù€ GLU FFN
            kwargs_copy = kwargs.copy()
            ffn_dim = kwargs_copy.pop('ffn_dim')
            d_model = kwargs_copy.pop('d_model')
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØªÙ„Ø© Ø¹Ø§Ø¯ÙŠØ© Ø«Ù… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ FFN
            block = TransformerBlock(**kwargs_copy)
            block.ffn = GLUFeedForward(d_model, ffn_dim, kwargs_copy.get('dropout', 0.1))
            return block
        else:
            raise ValueError(f"Ù†ÙˆØ¹ ÙƒØªÙ„Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {block_type}")


class TransformerStack(nn.Module):
    """ÙƒÙˆÙ…Ø© Ù…Ù† ÙƒØªÙ„ Transformer"""
    
    def __init__(self, 
                 n_layers: int,
                 block_config: Dict[str, Any]):
        """
        ØªÙ‡ÙŠØ¦Ø© ÙƒÙˆÙ…Ø© Transformer
        
        Args:
            n_layers: Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
            block_config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙƒØªÙ„
        """
        super().__init__()
        
        self.n_layers = n_layers
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙƒØªÙ„
        self.blocks = nn.ModuleList([
            TransformerBlockFactory.create_block(**block_config)
            for _ in range(n_layers)
        ])
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„ØªÙˆÙ„ÙŠØ¯
        self.cache = [None] * n_layers
    
    def forward(self, 
                x: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                use_cache: bool = False,
                start_pos: int = 0) -> torch.Tensor:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØªÙ„
        
        Args:
            x: Tensor Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            use_cache: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            start_pos: Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        
        Returns:
            Tensor Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        """
        all_attn_weights = []
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø¥Ø°Ø§ Ù„Ù… Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§
        if not use_cache:
            self.cache = [None] * self.n_layers
        
        for i, block in enumerate(self.blocks):
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø¨Ù‚Ø©
            layer_cache = self.cache[i] if use_cache else None
            
            # ØªÙ…Ø±ÙŠØ± Ø¹Ø¨Ø± Ø§Ù„ÙƒØªÙ„Ø©
            x, attn_weights, new_cache = block(
                x, 
                mask=mask,
                cache=layer_cache,
                start_pos=start_pos
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            if use_cache and new_cache is not None:
                self.cache[i] = new_cache
            
            # ØªØ®Ø²ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            if attn_weights is not None:
                all_attn_weights.append(attn_weights)
        
        return x, all_attn_weights
    
    def reset_cache(self) -> None:
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        self.cache = [None] * self.n_layers
    
    def get_cache(self) -> list:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        return self.cache
    
    def set_cache(self, cache: list) -> None:
        """ØªØ¹ÙŠÙŠÙ† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        self.cache = cache


def test_transformer_blocks():
    """Ø§Ø®ØªØ¨Ø§Ø± ÙƒØªÙ„ Transformer"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± ÙƒØªÙ„ Transformer...")
    
    # Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    batch_size = 2
    seq_len = 10
    d_model = 512
    n_heads = 8
    ffn_dim = 2048
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    x = torch.randn(batch_size, seq_len, d_model)
    mask = torch.ones(batch_size, seq_len).bool()
    
    # Ø§Ø®ØªØ¨Ø§Ø± TransformerBlock Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± TransformerBlock:")
    block = TransformerBlock(d_model, n_heads, ffn_dim)
    output, attn_weights, _ = block(x, mask)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± ParallelTransformerBlock
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± ParallelTransformerBlock:")
    parallel_block = ParallelTransformerBlock(d_model, n_heads, ffn_dim)
    output, attn_weights, _ = parallel_block(x, mask)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± ÙƒØªÙ„Ø© Ù…Ø¹ GLU
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± ÙƒØªÙ„Ø© Ù…Ø¹ GLU:")
    glu_block = TransformerBlockFactory.create_block(
        block_type="glu",
        d_model=d_model,
        n_heads=n_heads,
        ffn_dim=ffn_dim
    )
    output, attn_weights, _ = glu_block(x, mask)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± TransformerStack
    print("\n4. Ø§Ø®ØªØ¨Ø§Ø± TransformerStack:")
    stack = TransformerStack(
        n_layers=4,
        block_config={
            "block_type": "standard",
            "d_model": d_model,
            "n_heads": n_heads,
            "ffn_dim": ffn_dim
        }
    )
    output, all_weights = stack(x, mask)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
    print(f"   Ø¹Ø¯Ø¯ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…: {len(all_weights)}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
    print("\n5. Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª:")
    stack.reset_cache()
    
    # ØªÙ…Ø±ÙŠØ± Ø£ÙˆÙ„ (ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ³Ù„Ø³Ù„)
    output1, _ = stack(x[:, :5, :], use_cache=True)
    
    # ØªÙ…Ø±ÙŠØ± Ø«Ø§Ù†Ù (Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„ØªØ³Ù„Ø³Ù„)
    output2, _ = stack(x[:, 5:, :], use_cache=True, start_pos=5)
    
    print(f"   Ø§Ù„Ø´ÙƒÙ„ 1: {output1.shape}")
    print(f"   Ø§Ù„Ø´ÙƒÙ„ 2: {output2.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ ÙƒØªÙ„ Transformer Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_transformer_blocks()