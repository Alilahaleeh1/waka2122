# -*- coding: utf-8 -*-
"""
Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… (Attention) - Ù‚Ù„Ø¨ Ù†Ù…ÙˆØ°Ø¬ Transformer
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, List


class MultiHeadAttention(nn.Module):
    """Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³"""
    
    def __init__(self, 
                 d_model: int, 
                 n_heads: int, 
                 dropout: float = 0.1,
                 bias: bool = True,
                 flash_attention: bool = False):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        
        Args:
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            n_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
            bias: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­ÙŠØ²
            flash_attention: Ø§Ø³ØªØ®Ø¯Ø§Ù… Flash Attention
        """
        super().__init__()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† d_model ÙŠÙ‚Ø¨Ù„ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ n_heads
        assert d_model % n_heads == 0, "d_model ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¨Ù„Ø§Ù‹ Ù„Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ n_heads"
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.dropout = dropout
        self.use_flash = flash_attention
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·
        self.W_q = nn.Linear(d_model, d_model, bias=bias)
        self.W_k = nn.Linear(d_model, d_model, bias=bias)
        self.W_v = nn.Linear(d_model, d_model, bias=bias)
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.W_o = nn.Linear(d_model, d_model, bias=bias)
        
        # Ø§Ù„ØªØ³Ø±Ø¨
        self.dropout_layer = nn.Dropout(dropout)
        
        # Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ù‚ÙŠØ§Ø³
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_weights()
    
    def _init_weights(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        # ØªÙ‡ÙŠØ¦Ø© He Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ù„Ù…ÙØ§ØªÙŠØ­
        nn.init.xavier_uniform_(self.W_q.weight, gain=1.0 / math.sqrt(2))
        nn.init.xavier_uniform_(self.W_k.weight, gain=1.0 / math.sqrt(2))
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù‚ÙŠÙ… ÙˆØ§Ù„Ø¥Ø®Ø±Ø§Ø¬
        nn.init.xavier_uniform_(self.W_v.weight)
        nn.init.xavier_uniform_(self.W_o.weight)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ­ÙŠØ²Ø§Øª
        if self.W_q.bias is not None:
            nn.init.constant_(self.W_q.bias, 0)
            nn.init.constant_(self.W_k.bias, 0)
            nn.init.constant_(self.W_v.bias, 0)
            nn.init.constant_(self.W_o.bias, 0)
    
    def forward(self, 
                query: torch.Tensor,
                key: torch.Tensor,
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                rotary_pos_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù…
        
        Args:
            query: Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª [batch_size, seq_len_q, d_model]
            key: Ù…ÙØ§ØªÙŠØ­ [batch_size, seq_len_k, d_model]
            value: Ù‚ÙŠÙ… [batch_size, seq_len_v, d_model]
            mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… [batch_size, n_heads, seq_len_q, seq_len_k]
            rotary_pos_emb: ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø© (cos, sin)
            cache: Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù‚ÙŠÙ… (Ù„Ù„ØªÙˆÙ„ÙŠØ¯)
        
        Returns:
            Tensor Ø§Ù„Ù†Ø§ØªØ¬ ÙˆØ§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        """
        batch_size = query.size(0)
        
        # Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø®Ø·ÙŠ ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø±Ø¤ÙˆØ³
        Q = self.W_q(query)  # [batch_size, seq_len_q, d_model]
        K = self.W_k(key)    # [batch_size, seq_len_k, d_model]
        V = self.W_v(value)  # [batch_size, seq_len_v, d_model]
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø¤ÙˆØ³
        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
        if rotary_pos_emb is not None:
            cos, sin = rotary_pos_emb
            Q = apply_rotary_pos_emb(Q, cos, sin)
            K = apply_rotary_pos_emb(K, cos, sin)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø© (Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ²Ø§ÙŠØ¯)
        if cache is not None:
            K_cache, V_cache = cache
            K = torch.cat([K_cache, K], dim=2)
            V = torch.cat([V_cache, V], dim=2)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        if self.use_flash and self._can_use_flash_attention(Q, K, mask):
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Flash Attention
            output, attention_weights = self._flash_attention(Q, K, V, mask)
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ø§Ø¯ÙŠ
            output, attention_weights = self._scaled_dot_product_attention(Q, K, V, mask)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„Ø¨Ø¹Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠ
        output = output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        output = self.W_o(output)
        output = self.dropout_layer(output)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        new_cache = (K, V) if cache is not None else None
        
        return output, attention_weights, new_cache
    
    def _scaled_dot_product_attention(self, 
                                     Q: torch.Tensor,
                                     K: torch.Tensor,
                                     V: torch.Tensor,
                                     mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„Ø¶Ø±Ø¨ Ø§Ù„Ù†Ù‚Ø·ÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ
        
        Args:
            Q: Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª [batch_size, n_heads, seq_len_q, head_dim]
            K: Ù…ÙØ§ØªÙŠØ­ [batch_size, n_heads, seq_len_k, head_dim]
            V: Ù‚ÙŠÙ… [batch_size, n_heads, seq_len_v, head_dim]
            mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… [batch_size, n_heads, seq_len_q, seq_len_k]
        
        Returns:
            Tensor Ø§Ù„Ù†Ø§ØªØ¬ ÙˆØ£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        """
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # ØªØ·Ø¨ÙŠÙ‚ softmax Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        attention_weights = F.softmax(scores, dim=-1)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ³Ø±Ø¨ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        attention_weights = self.dropout_layer(attention_weights)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ…
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def _flash_attention(self, 
                        Q: torch.Tensor,
                        K: torch.Tensor,
                        V: torch.Tensor,
                        mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ØªÙ†ÙÙŠØ° Ù…Ø¨Ø³Ø· Ù„Ù€ Flash Attention
        
        Note: Ù‡Ø°Ø§ ØªÙ†ÙÙŠØ° Ù…Ø¨Ø³Ø·ØŒ Flash Attention Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹
        """
        # ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ù†Ø³ØªØ®Ø¯Ù… torch.nn.functional.scaled_dot_product_attention
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹ (PyTorch 2.0+)
        if hasattr(F, 'scaled_dot_product_attention'):
            output = F.scaled_dot_product_attention(
                Q, K, V, 
                attn_mask=mask,
                dropout_p=self.dropout if self.training else 0.0,
                scale=self.scale
            )
            # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
            _, attention_weights = self._scaled_dot_product_attention(Q, K, V, mask)
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¹Ø§Ø¯ÙŠ
            output, attention_weights = self._scaled_dot_product_attention(Q, K, V, mask)
        
        return output, attention_weights
    
    def _can_use_flash_attention(self, Q: torch.Tensor, K: torch.Tensor, 
                                mask: Optional[torch.Tensor]) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Flash Attention"""
        if not self.use_flash:
            return False
        
        # Flash Attention ÙŠØ¹Ù…Ù„ ÙÙ‚Ø· Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ float16 Ùˆ bfloat16 Ø¹Ù„Ù‰ GPU
        if Q.dtype not in [torch.float16, torch.bfloat16]:
            return False
        
        if not Q.is_cuda:
            return False
        
        # Flash Attention Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø¨Ø¹Ø¶ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù‚Ù†Ø¹Ø©
        if mask is not None and mask.dtype != torch.bool:
            return False
        
        return True
    
    def get_attention_weights(self, 
                             query: torch.Tensor,
                             key: torch.Tensor,
                             value: torch.Tensor,
                             mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙ‚Ø·"""
        _, attention_weights, _ = self.forward(query, key, value, mask)
        return attention_weights


class CausalSelfAttention(MultiHeadAttention):
    """Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø³Ø¨Ø¨ÙŠ (Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØ©)"""
    
    def __init__(self, 
                 d_model: int, 
                 n_heads: int, 
                 dropout: float = 0.1,
                 bias: bool = True,
                 flash_attention: bool = False):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø³Ø¨Ø¨ÙŠ"""
        super().__init__(d_model, n_heads, dropout, bias, flash_attention)
    
    def forward(self, 
                x: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                rotary_pos_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù…Ø¹ Ù‚Ù†Ø§Ø¹ Ø³Ø¨Ø¨ÙŠ
        
        Args:
            x: Tensor Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª [batch_size, seq_len, d_model]
            mask: Ù‚Ù†Ø§Ø¹ Ø¥Ø¶Ø§ÙÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            rotary_pos_emb: ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
            cache: Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚ØªØ©
        
        Returns:
            Tensor Ø§Ù„Ù†Ø§ØªØ¬ ÙˆØ§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        """
        batch_size, seq_len, _ = x.shape
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø³Ø¨Ø¨ÙŠ
        causal_mask = self._create_causal_mask(seq_len, x.device)
        
        # Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if mask is not None:
            # ØªÙˆØ³ÙŠØ¹ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø³Ø¨Ø¨ÙŠ
            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]
            mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]
            combined_mask = causal_mask & mask
        else:
            combined_mask = causal_mask
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        return super().forward(x, x, x, combined_mask, rotary_pos_emb, cache)
    
    def _create_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø³Ø¨Ø¨ÙŠ"""
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ù…Ø«Ù„Ø«ÙŠØ© Ø³ÙÙ„ÙŠØ©
        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
        return mask.bool()


class GroupedQueryAttention(MultiHeadAttention):
    """Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª (GQA)"""
    
    def __init__(self, 
                 d_model: int, 
                 n_heads: int, 
                 n_kv_heads: int,
                 dropout: float = 0.1,
                 bias: bool = True):
        """
        ØªÙ‡ÙŠØ¦Ø© GQA
        
        Args:
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            n_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            n_kv_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ù…ÙØ§ØªÙŠØ­/Ø§Ù„Ù‚ÙŠÙ…
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
            bias: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­ÙŠØ²
        """
        super().__init__(d_model, n_heads, dropout, bias, flash_attention=False)
        
        self.n_kv_heads = n_kv_heads
        self.n_rep = n_heads // n_kv_heads  # Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª ØªÙƒØ±Ø§Ø± ÙƒÙ„ Ø±Ø£Ø³ KV
        
        # Ø·Ø¨Ù‚Ø§Øª Ù…Ù†ÙØµÙ„Ø© Ù„Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù‚ÙŠÙ…
        self.W_k = nn.Linear(d_model, d_model // (n_heads // n_kv_heads), bias=bias)
        self.W_v = nn.Linear(d_model, d_model // (n_heads // n_kv_heads), bias=bias)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_gqa_weights()
    
    def _init_gqa_weights(self) -> None:
        """ØªÙ‡ÙŠØ¦Ø© Ø£ÙˆØ²Ø§Ù† GQA"""
        # ØªÙ‡ÙŠØ¦Ø© Ø·Ø¨Ù‚Ø§Øª KV
        nn.init.xavier_uniform_(self.W_k.weight, gain=1.0 / math.sqrt(2))
        nn.init.xavier_uniform_(self.W_v.weight)
        
        if self.W_k.bias is not None:
            nn.init.constant_(self.W_k.bias, 0)
            nn.init.constant_(self.W_v.bias, 0)
    
    def forward(self, 
                query: torch.Tensor,
                key: torch.Tensor,
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                rotary_pos_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù„Ù€ GQA"""
        batch_size = query.size(0)
        
        # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª (Q)
        Q = self.W_q(query)
        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        
        # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù‚ÙŠÙ… (K, V) Ù…Ø¹ Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø£Ù‚Ù„
        K = self.W_k(key)
        V = self.W_v(value)
        
        kv_head_dim = self.d_model // self.n_kv_heads
        K = K.view(batch_size, -1, self.n_kv_heads, kv_head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_kv_heads, kv_head_dim).transpose(1, 2)
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø©
        if rotary_pos_emb is not None:
            cos, sin = rotary_pos_emb
            Q = apply_rotary_pos_emb(Q, cos, sin)
            K = apply_rotary_pos_emb(K, cos, sin)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        if cache is not None:
            K_cache, V_cache = cache
            K = torch.cat([K_cache, K], dim=2)
            V = torch.cat([V_cache, V], dim=2)
        
        # ØªÙƒØ±Ø§Ø± Ø±Ø¤ÙˆØ³ K ÙˆV Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Q
        K = K.repeat_interleave(self.n_rep, dim=1)
        V = V.repeat_interleave(self.n_rep, dim=1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        output, attention_weights = self._scaled_dot_product_attention(Q, K, V, mask)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        output = output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        output = self.W_o(output)
        output = self.dropout_layer(output)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        new_cache = (K[:, :self.n_kv_heads, :, :], V[:, :self.n_kv_heads, :, :]) if cache is not None else None
        
        return output, attention_weights, new_cache


class SlidingWindowAttention(MultiHeadAttention):
    """Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù†Ø§ÙØ°Ø© Ù…Ù†Ø²Ù„Ù‚Ø©"""
    
    def __init__(self, 
                 d_model: int, 
                 n_heads: int, 
                 window_size: int,
                 dropout: float = 0.1,
                 bias: bool = True):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù†Ø§ÙØ°Ø© Ù…Ù†Ø²Ù„Ù‚Ø©
        
        Args:
            d_model: Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            n_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            window_size: Ø­Ø¬Ù… Ø§Ù„Ù†Ø§ÙØ°Ø©
            dropout: Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ³Ø±Ø¨
            bias: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­ÙŠØ²
        """
        super().__init__(d_model, n_heads, dropout, bias, flash_attention=False)
        self.window_size = window_size
    
    def forward(self, 
                query: torch.Tensor,
                key: torch.Tensor,
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None,
                rotary_pos_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """ØªÙ…Ø±ÙŠØ± Ù„Ù„Ø£Ù…Ø§Ù… Ù…Ø¹ Ù†Ø§ÙØ°Ø© Ù…Ù†Ø²Ù„Ù‚Ø©"""
        batch_size, seq_len, _ = query.shape
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø²Ù„Ù‚Ø©
        window_mask = self._create_sliding_window_mask(seq_len, query.device)
        
        # Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø³Ø¨Ø¨ÙŠ
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=query.device)).bool()
        combined_mask = causal_mask & window_mask
        
        if mask is not None:
            combined_mask = combined_mask.unsqueeze(0) & mask.unsqueeze(1)
        
        return super().forward(query, key, value, combined_mask, rotary_pos_emb, cache)
    
    def _create_sliding_window_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø²Ù„Ù‚Ø©"""
        mask = torch.zeros(seq_len, seq_len, device=device, dtype=torch.bool)
        
        for i in range(seq_len):
            start = max(0, i - self.window_size + 1)
            mask[i, start:i+1] = True
        
        return mask


def apply_rotary_pos_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:
    """
    ØªØ·Ø¨ÙŠÙ‚ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø¯ÙˆØ§Ø±Ø© Ø¹Ù„Ù‰ Tensor
    
    Args:
        x: Tensor Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„ [batch_size, n_heads, seq_len, head_dim]
        cos: Ø¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù… Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        sin: Ø§Ù„Ø¬ÙŠØ¨ Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
    
    Returns:
        Tensor Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ RoPE
    """
    # ÙØµÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø²ÙˆØ¬ÙŠØ© ÙˆØ§Ù„ÙØ±Ø¯ÙŠØ©
    x_even = x[..., 0::2]  # [batch_size, n_heads, seq_len, head_dim/2]
    x_odd = x[..., 1::2]   # [batch_size, n_heads, seq_len, head_dim/2]
    
    # Ø§Ù‚ØªØµØ§Øµ cos Ùˆ sin Ù„Ø£Ø¨Ø¹Ø§Ø¯ x
    cos = cos[:, :x.size(2), :]  # [1, seq_len, head_dim]
    sin = sin[:, :x.size(2), :]  # [1, seq_len, head_dim]
    
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© x
    cos = cos.unsqueeze(1)  # [1, 1, seq_len, head_dim]
    sin = sin.unsqueeze(1)  # [1, 1, seq_len, head_dim]
    
    # Ø§Ù‚ØªØµØ§Øµ Ù„Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø²ÙˆØ¬ÙŠØ© ÙˆØ§Ù„ÙØ±Ø¯ÙŠØ©
    cos_even = cos[..., 0::2]
    cos_odd = cos[..., 1::2]
    sin_even = sin[..., 0::2]
    sin_odd = sin[..., 1::2]
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
    x_even_rot = x_even * cos_even - x_odd * sin_odd
    x_odd_rot = x_odd * cos_odd + x_even * sin_even
    
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¬Ù…ÙŠØ¹
    x_rotated = torch.zeros_like(x)
    x_rotated[..., 0::2] = x_even_rot
    x_rotated[..., 1::2] = x_odd_rot
    
    return x_rotated


class AttentionFactory:
    """Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…"""
    
    @staticmethod
    def create_attention(attention_type: str, **kwargs):
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø·Ø¨Ù‚Ø© Ø§Ù‡ØªÙ…Ø§Ù…
        
        Args:
            attention_type: Ù†ÙˆØ¹ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…
        
        Returns:
            Ø·Ø¨Ù‚Ø© Ø§Ù‡ØªÙ…Ø§Ù…
        """
        if attention_type == "multihead":
            return MultiHeadAttention(**kwargs)
        elif attention_type == "causal":
            return CausalSelfAttention(**kwargs)
        elif attention_type == "gqa":
            return GroupedQueryAttention(**kwargs)
        elif attention_type == "sliding":
            return SlidingWindowAttention(**kwargs)
        else:
            raise ValueError(f"Ù†ÙˆØ¹ Ø§Ù‡ØªÙ…Ø§Ù… ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {attention_type}")


def test_attention():
    """Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…...")
    
    # Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    batch_size = 2
    seq_len = 10
    d_model = 512
    n_heads = 8
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    x = torch.randn(batch_size, seq_len, d_model)
    
    # Ø§Ø®ØªØ¨Ø§Ø± MultiHeadAttention
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± MultiHeadAttention:")
    mha = MultiHeadAttention(d_model, n_heads)
    output, weights, _ = mha.forward(x, x, x)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
    print(f"   Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…: {weights.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± CausalSelfAttention
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± CausalSelfAttention:")
    causal = CausalSelfAttention(d_model, n_heads)
    output, weights, _ = causal.forward(x)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± GroupedQueryAttention
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± GroupedQueryAttention:")
    try:
        gqa = GroupedQueryAttention(d_model, n_heads, n_kv_heads=4)
        output, weights, _ = gqa.forward(x, x, x)
        print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
        print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        print(f"   âœ— Ø®Ø·Ø£: {e}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± SlidingWindowAttention
    print("\n4. Ø§Ø®ØªØ¨Ø§Ø± SlidingWindowAttention:")
    sliding = SlidingWindowAttention(d_model, n_heads, window_size=5)
    output, weights, _ = sliding.forward(x, x, x)
    print(f"   Ø§Ù„Ø´ÙƒÙ„: {output.shape}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± AttentionFactory
    print("\n5. Ø§Ø®ØªØ¨Ø§Ø± AttentionFactory:")
    factory = AttentionFactory()
    attention = factory.create_attention(
        "causal",
        d_model=d_model,
        n_heads=n_heads
    )
    print(f"   Ø§Ù„Ù†ÙˆØ¹: {type(attention).__name__}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_attention()