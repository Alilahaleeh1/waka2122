# -*- coding: utf-8 -*-
"""
Ù…Ø¯ÙŠØ± Ø§Ù„Ø¬Ù‡Ø§Ø² - Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© (GPU/CPU) ÙˆØªØ®ØµÙŠØµ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
"""

import torch
import gc
import psutil
import platform
from typing import Optional, Tuple, Dict, Any
import warnings


class DeviceManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©"""
    
    def __init__(self, auto_select: bool = True):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„Ø¬Ù‡Ø§Ø²
        
        Args:
            auto_select: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø² ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
        """
        self.device_info = self._get_device_info()
        self.selected_device = None
        
        if auto_select:
            self.selected_device = self.select_best_device()
    
    def _get_device_info(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Ø²"""
        info = {
            "cpu": {
                "cores": psutil.cpu_count(logical=False),
                "threads": psutil.cpu_count(logical=True),
                "freq": psutil.cpu_freq().current if psutil.cpu_freq() else "Unknown",
                "memory": psutil.virtual_memory().total / (1024**3),  #GB
            },
            "cuda": {
                "available": torch.cuda.is_available(),
                "devices": [],
                "driver": None
            },
            "system": {
                "os": platform.system(),
                "version": platform.version(),
                "machine": platform.machine()
            }
        }
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª CUDA Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
        if info["cuda"]["available"]:
            info["cuda"]["driver"] = torch.version.cuda
            info["cuda"]["devices"] = []
            
            for i in range(torch.cuda.device_count()):
                device_props = {
                    "name": torch.cuda.get_device_name(i),
                    "memory": torch.cuda.get_device_properties(i).total_memory / (1024**3),  #GB
                    "capability": torch.cuda.get_device_capability(i),
                    "current_memory": torch.cuda.memory_allocated(i) / (1024**3)  #GB
                }
                info["cuda"]["devices"].append(device_props)
        
        return info
    
    def select_best_device(self, preference: str = "cuda") -> torch.device:
        """
        Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø¬Ù‡Ø§Ø² Ù…ØªØ§Ø­
        
        Args:
            preference: Ø§Ù„ØªÙØ¶ÙŠÙ„ ("cuda", "mps", "cpu")
        
        Returns:
            Ø¬Ù‡Ø§Ø² PyTorch
        """
        # Ù…Ø­Ø§ÙˆÙ„Ø© CUDA Ø£ÙˆÙ„Ø§Ù‹
        if preference == "cuda" and torch.cuda.is_available():
            device = torch.device("cuda:0")
            print(f"âœ… ØªÙ… Ø§Ø®ØªÙŠØ§Ø± GPU: {torch.cuda.get_device_name(0)}")
            print(f"   Ø°Ø§ÙƒØ±Ø© GPU: {self.get_gpu_memory()[0]:.2f} GB")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© MPS (Ù„Ù€ Apple Silicon)
        elif preference == "mps" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device("mps")
            print("âœ… ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Apple Silicon (MPS)")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU ÙƒØ®ÙŠØ§Ø± Ø£Ø®ÙŠØ±
        else:
            device = torch.device("cpu")
            print(f"âš ï¸  Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU ({self.device_info['cpu']['cores']} Ù†ÙˆØ§Ø©)")
        
        self.selected_device = device
        return device
    
    def get_device(self, device_str: str = "auto") -> torch.device:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ù„Ø³Ù„Ø©
        
        Args:
            device_str: "cuda", "cpu", "mps", Ø£Ùˆ "auto"
        
        Returns:
            Ø¬Ù‡Ø§Ø² PyTorch
        """
        if device_str == "auto":
            return self.select_best_device()
        
        elif device_str == "cuda":
            if torch.cuda.is_available():
                return torch.device("cuda:0")
            else:
                warnings.warn("CUDA ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ")
                return torch.device("cpu")
        
        elif device_str == "mps":
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return torch.device("mps")
            else:
                warnings.warn("MPS ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ")
                return torch.device("cpu")
        
        elif device_str == "cpu":
            return torch.device("cpu")
        
        else:
            raise ValueError(f"Ø¬Ù‡Ø§Ø² ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {device_str}")
    
    def get_gpu_memory(self, device_id: int = 0) -> Tuple[float, float]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© GPU
        
        Args:
            device_id: Ù…Ø¹Ø±Ù Ø¬Ù‡Ø§Ø² GPU
        
        Returns:
            Ø°Ø§ÙƒØ±Ø© Ù…Ø³ØªØ®Ø¯Ù…Ø©ØŒ Ø°Ø§ÙƒØ±Ø© ÙƒÙ„ÙŠØ© (GB)
        """
        if not torch.cuda.is_available():
            return 0.0, 0.0
        
        torch.cuda.synchronize(device_id)
        used = torch.cuda.memory_allocated(device_id) / (1024**3)
        total = torch.cuda.get_device_properties(device_id).total_memory / (1024**3)
        
        return used, total
    
    def clear_memory(self) -> None:
        """Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
    
    def monitor_memory(self) -> Dict[str, float]:
        """
        Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        """
        memory_info = {}
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        sys_memory = psutil.virtual_memory()
        memory_info["system_used"] = sys_memory.used / (1024**3)
        memory_info["system_total"] = sys_memory.total / (1024**3)
        memory_info["system_percent"] = sys_memory.percent
        
        # Ø°Ø§ÙƒØ±Ø© GPU Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
        if torch.cuda.is_available():
            used, total = self.get_gpu_memory()
            memory_info["gpu_used"] = used
            memory_info["gpu_total"] = total
            memory_info["gpu_percent"] = (used / total) * 100 if total > 0 else 0
        
        return memory_info
    
    def print_device_info(self) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Ø²"""
        print("=" * 60)
        print("ğŸ–¥ï¸  Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Ø²:")
        print("=" * 60)
        
        print(f"Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ´ØºÙŠÙ„: {self.device_info['system']['os']} {self.device_info['system']['version']}")
        print(f"Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {self.device_info['cpu']['cores']} Ù†ÙˆØ§Ø©ØŒ {self.device_info['cpu']['threads']} Ø®ÙŠØ·")
        print(f"Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ù…: {self.device_info['cpu']['memory']:.2f} GB")
        
        if self.device_info["cuda"]["available"]:
            print(f"\nğŸ® CUDA Ù…ØªØ§Ø­: Ù†Ø¹Ù…")
            print(f"   Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø³Ø§Ø¦Ù‚: {self.device_info['cuda']['driver']}")
            
            for i, device in enumerate(self.device_info["cuda"]["devices"]):
                print(f"\n   GPU {i}: {device['name']}")
                print(f"     Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {device['memory']:.2f} GB")
                print(f"     Ø§Ù„Ø¥Ù…ÙƒØ§Ù†ÙŠØ©: {device['capability'][0]}.{device['capability'][1]}")
        else:
            print("\nğŸ® CUDA Ù…ØªØ§Ø­: Ù„Ø§")
        
        print("=" * 60)
    
    def optimize_for_inference(self, model: torch.nn.Module) -> torch.nn.Module:
        """
        ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        
        Args:
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ø³ÙŠÙ†Ù‡
        
        Returns:
            Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†
        """
        model.eval()
        
        if torch.cuda.is_available():
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            torch.cuda.empty_cache()
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
            with torch.no_grad():
                model = model.to(self.selected_device)
        
        return model
    
    def optimize_for_training(self, model: torch.nn.Module) -> torch.nn.Module:
        """
        ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        
        Args:
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ø³ÙŠÙ†Ù‡
        
        Returns:
            Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†
        """
        model.train()
        
        if torch.cuda.is_available():
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Automatic Mixed Precision (AMP)
            try:
                from torch.cuda.amp import autocast
                model.amp_enabled = True
            except ImportError:
                model.amp_enabled = False
        
        return model


def get_available_device(preference: str = "cuda") -> torch.device:
    """
    Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…ØªØ§Ø­
    
    Args:
        preference: ØªÙØ¶ÙŠÙ„ Ø§Ù„Ø¬Ù‡Ø§Ø²
    
    Returns:
        Ø¬Ù‡Ø§Ø² PyTorch
    """
    manager = DeviceManager(auto_select=False)
    return manager.get_device(preference)


if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¯ÙŠØ± Ø§Ù„Ø¬Ù‡Ø§Ø²
    manager = DeviceManager()
    manager.print_device_info()
    
    device = manager.select_best_device()
    print(f"\nâœ… Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø®ØªØ§Ø±: {device}")
    
    memory_info = manager.monitor_memory()
    print(f"\nğŸ“Š Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory_info}")