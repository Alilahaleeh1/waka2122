# -*- coding: utf-8 -*-
"""
Ø§Ù„Ù…Ø¯Ø±Ø¨ - Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
import os
import time
import json
from typing import Dict, Any, Optional, Tuple, List
from tqdm import tqdm
import wandb

from ..model.tiny_llm import TinyLLM
from .optimizer import create_optimizer
from .loss import LanguageModelingLoss
from ..utils.device_manager import DeviceManager


class Trainer:
    """Ù…Ø¯Ø±Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ"""
    
    def __init__(self,
                 model: nn.Module,
                 train_dataset: torch.utils.data.Dataset,
                 val_dataset: Optional[torch.utils.data.Dataset] = None,
                 config: Dict[str, Any] = None,
                 device: torch.device = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯Ø±Ø¨
        
        Args:
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ¯Ø±ÙŠØ¨Ù‡
            train_dataset: Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            val_dataset: Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            device: Ø¬Ù‡Ø§Ø² Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        """
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config or {}
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.batch_size = self.config.get('batch_size', 32)
        self.micro_batch_size = self.config.get('micro_batch_size', 4)
        self.gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 8)
        self.learning_rate = self.config.get('learning_rate', 3e-4)
        self.total_steps = self.config.get('total_steps', 100000)
        self.warmup_steps = self.config.get('warmup_steps', 2000)
        self.weight_decay = self.config.get('weight_decay', 0.01)
        self.grad_clip = self.config.get('grad_clip', 1.0)
        self.checkpoint_steps = self.config.get('checkpoint_steps', 5000)
        self.eval_steps = self.config.get('eval_steps', 1000)
        self.save_dir = self.config.get('save_dir', './checkpoints')
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        self.use_amp = self.config.get('use_amp', True)
        self.log_interval = self.config.get('log_interval', 10)
        self.eval_interval = self.config.get('eval_interval', 1000)
        self.save_interval = self.config.get('save_interval', 5000)
        
        # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø²
        self.model = self.model.to(self.device)
        
        # Ø¥Ù†Ø´Ø§Ø¡ DataLoaders
        self.train_loader = self._create_dataloader(train_dataset, shuffle=True)
        if val_dataset:
            self.val_loader = self._create_dataloader(val_dataset, shuffle=False)
        else:
            self.val_loader = None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù†
        self.optimizer = create_optimizer(
            model=self.model,
            learning_rate=self.learning_rate,
            weight_decay=self.weight_decay,
            optimizer_type='adamw'
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        self.scheduler = self._create_scheduler()
        
        # Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        self.criterion = LanguageModelingLoss(ignore_index=0)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ AMP (Automatic Mixed Precision)
        self.scaler = torch.cuda.amp.GradScaler() if self.use_amp and self.device.type == 'cuda' else None
        
        # Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.global_step = 0
        self.epoch = 0
        self.best_val_loss = float('inf')
        
        # Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø­ÙØ¸ Ø§Ù„Ù†Ù‚Ø§Ø·
        os.makedirs(self.save_dir, exist_ok=True)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ WandB (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        self.use_wandb = self.config.get('use_wandb', False)
        if self.use_wandb:
            self._init_wandb()
        
        print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯Ø±Ø¨")
        print(f"   Ø§Ù„Ø¬Ù‡Ø§Ø²: {self.device}")
        print(f"   Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©: {self.batch_size}")
        print(f"   Ø®Ø·ÙˆØ§Øª ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬: {self.gradient_accumulation_steps}")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø®Ø·ÙˆØ§Øª: {self.total_steps}")
    
    def _create_dataloader(self, dataset, shuffle: bool) -> DataLoader:
        """Ø¥Ù†Ø´Ø§Ø¡ DataLoader"""
        return DataLoader(
            dataset,
            batch_size=self.micro_batch_size,
            shuffle=shuffle,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True,
            collate_fn=getattr(dataset, 'collate_fn', None)
        )
    
    def _create_scheduler(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
        from torch.optim.lr_scheduler import LambdaLR
        
        def lr_lambda(current_step: int):
            # Warmup Ø«Ù… ØªÙ†Ø§Ù‚Øµ Ø®Ø·ÙŠ
            if current_step < self.warmup_steps:
                return float(current_step) / float(max(1, self.warmup_steps))
            
            # ØªÙ†Ø§Ù‚Øµ Ø®Ø·ÙŠ Ø¨Ø¹Ø¯ Warmup
            progress = float(current_step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))
            return max(0.0, 1.0 - progress)
        
        return LambdaLR(self.optimizer, lr_lambda)
    
    def _init_wandb(self):
        """ØªÙ‡ÙŠØ¦Ø© WandB"""
        try:
            wandb.init(
                project=self.config.get('wandb_project', 'deepseek-mini'),
                name=self.config.get('wandb_name', 'training-run'),
                config=self.config
            )
            print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© WandB")
        except Exception as e:
            print(f"âš ï¸  ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© WandB: {e}")
            self.use_wandb = False
    
    def train_step(self, batch: Dict[str, torch.Tensor]) -> float:
        """
        Ø®Ø·ÙˆØ© ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø­Ø¯Ø©
        
        Args:
            batch: Ø¯ÙØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        """
        # Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø²
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device) if 'attention_mask' in batch else None
        labels = batch['labels'].to(self.device) if 'labels' in batch else None
        
        # AMP forward pass
        with torch.cuda.amp.autocast(enabled=self.use_amp and self.device.type == 'cuda'):
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs['loss']
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ù„ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬
            loss = loss / self.gradient_accumulation_steps
        
        # AMP backward pass
        if self.scaler is not None:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()
        
        return loss.item() * self.gradient_accumulation_steps  # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‚ÙŠØ§Ø³
    
    def train_epoch(self) -> float:
        """
        Ø¯ÙˆØ±Ø© ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø­Ø¯Ø©
        
        Returns:
            Ù…ØªÙˆØ³Ø· Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø¯ÙˆØ±Ø©
        """
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {self.epoch}")
        
        for batch_idx, batch in enumerate(progress_bar):
            # Ø®Ø·ÙˆØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            loss = self.train_step(batch)
            total_loss += loss
            num_batches += 1
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ¯Ø±Ø¬ Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ø¥Ù„Ù‰ Ø®Ø·ÙˆØ§Øª ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Ù‚Øµ Ø§Ù„ØªØ¯Ø±Ø¬
                if self.grad_clip > 0:
                    if self.scaler is not None:
                        self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                if self.scaler is not None:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                # ØªØ­Ø¯ÙŠØ« Ù…Ø¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
                self.scheduler.step()
                
                # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
                self.optimizer.zero_grad()
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©
                self.global_step += 1
                
                # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
                current_lr = self.scheduler.get_last_lr()[0]
                self.learning_rates.append(current_lr)
                
                avg_loss = total_loss / num_batches
                self.train_losses.append(avg_loss)
                
                # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
                progress_bar.set_postfix({
                    'loss': avg_loss,
                    'lr': f'{current_lr:.2e}',
                    'step': self.global_step
                })
                
                # ØªØ³Ø¬ÙŠÙ„ Ø¥Ù„Ù‰ WandB
                if self.use_wandb and self.global_step % self.log_interval == 0:
                    wandb.log({
                        'train/loss': avg_loss,
                        'train/lr': current_lr,
                        'train/step': self.global_step
                    })
                
                # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯ÙˆØ±ÙŠ
                if self.val_loader and self.global_step % self.eval_interval == 0:
                    val_loss = self.evaluate()
                    self.val_losses.append(val_loss)
                    
                    if self.use_wandb:
                        wandb.log({
                            'val/loss': val_loss,
                            'val/step': self.global_step
                        })
                    
                    # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
                    if val_loss < self.best_val_loss:
                        self.best_val_loss = val_loss
                        self.save_checkpoint(is_best=True)
                
                # Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ø¯ÙˆØ±ÙŠØ©
                if self.global_step % self.save_interval == 0:
                    self.save_checkpoint()
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                if self.global_step >= self.total_steps:
                    break
        
        return total_loss / num_batches if num_batches > 0 else 0
    
    def evaluate(self) -> float:
        """
        ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ­Ù‚Ù‚
        
        Returns:
            Ù…ØªÙˆØ³Ø· Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ­Ù‚Ù‚
        """
        if not self.val_loader:
            return 0.0
        
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Evaluating"):
                # Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø²
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device) if 'attention_mask' in batch else None
                labels = batch['labels'].to(self.device) if 'labels' in batch else None
                
                # Forward pass
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs['loss']
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        
        print(f"\nğŸ“Š ØªÙ‚ÙŠÙŠÙ… - Ø®Ø·ÙˆØ© {self.global_step}: Ø®Ø³Ø§Ø±Ø© = {avg_loss:.4f}")
        
        self.model.train()
        return avg_loss
    
    def train(self) -> None:
        """Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        start_time = time.time()
        
        try:
            while self.global_step < self.total_steps:
                # ØªØ¯Ø±ÙŠØ¨ Ø¯ÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©
                train_loss = self.train_epoch()
                
                print(f"\nğŸ“ˆ Ø§Ù„Ø¯ÙˆØ±Ø© {self.epoch} - Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {train_loss:.4f}")
                
                # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯
                self.epoch += 1
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                if self.global_step >= self.total_steps:
                    break
        
        except KeyboardInterrupt:
            print("\nâš ï¸  Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ØªÙˆÙ‚Ù Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        
        finally:
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„
            end_time = time.time()
            training_time = end_time - start_time
            
            print(f"\nâœ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ÙƒØªÙ…Ù„!")
            print(f"   Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {training_time:.2f} Ø«Ø§Ù†ÙŠØ©")
            print(f"   Ø§Ù„Ø®Ø·ÙˆØ§Øª: {self.global_step}")
            print(f"   Ø§Ù„Ø¯ÙˆØ±Ø©: {self.epoch}")
            print(f"   Ø£ÙØ¶Ù„ Ø®Ø³Ø§Ø±Ø© ØªØ­Ù‚Ù‚: {self.best_val_loss:.4f}")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            self.save_checkpoint(is_final=True)
            
            # Ø¥ØºÙ„Ø§Ù‚ WandB
            if self.use_wandb:
                wandb.finish()
    
    def save_checkpoint(self, 
                       is_best: bool = False, 
                       is_final: bool = False) -> None:
        """
        Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´
        
        Args:
            is_best: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø£ÙØ¶Ù„ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´
            is_final: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        """
        checkpoint_name = 'checkpoint'
        if is_best:
            checkpoint_name = 'best_model'
        elif is_final:
            checkpoint_name = 'final_model'
        else:
            checkpoint_name = f'checkpoint_step_{self.global_step}'
        
        checkpoint_path = os.path.join(self.save_dir, f'{checkpoint_name}.pt')
        
        checkpoint = {
            'global_step': self.global_step,
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'config': self.config
        }
        
        # Ø¥Ø¶Ø§ÙØ© Scaler Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³ØªØ®Ø¯Ù…Ù‹Ø§
        if self.scaler is not None:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        torch.save(checkpoint, checkpoint_path)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„
        config_path = os.path.join(self.save_dir, f'{checkpoint_name}_config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {checkpoint_name} Ø¥Ù„Ù‰ {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path: str) -> None:
        """
        ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´
        
        Args:
            checkpoint_path: Ù…Ø³Ø§Ø± Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
        self.global_step = checkpoint['global_step']
        self.epoch = checkpoint['epoch']
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        self.train_losses = checkpoint.get('train_losses', [])
        self.val_losses = checkpoint.get('val_losses', [])
        self.learning_rates = checkpoint.get('learning_rates', [])
        
        # ØªØ­Ù…ÙŠÙ„ Scaler Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
        if 'scaler_state_dict' in checkpoint and self.scaler is not None:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ù…Ù† {checkpoint_path}")
        print(f"   Ø§Ù„Ø®Ø·ÙˆØ©: {self.global_step}, Ø§Ù„Ø¯ÙˆØ±Ø©: {self.epoch}")
        print(f"   Ø£ÙØ¶Ù„ Ø®Ø³Ø§Ø±Ø©: {self.best_val_loss:.4f}")
    
    def get_training_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        return {
            'global_step': self.global_step,
            'epoch': self.epoch,
            'best_val_loss': self.best_val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates
        }
    
    def plot_training_history(self, save_path: Optional[str] = None) -> None:
        """Ø±Ø³Ù… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(2, 2, figsize=(12, 8))
            
            # Ø±Ø³Ù… Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            axes[0, 0].plot(self.train_losses)
            axes[0, 0].set_title('Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨')
            axes[0, 0].set_xlabel('Ø§Ù„Ø®Ø·ÙˆØ©')
            axes[0, 0].set_ylabel('Ø§Ù„Ø®Ø³Ø§Ø±Ø©')
            axes[0, 0].grid(True, alpha=0.3)
            
            # Ø±Ø³Ù… Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ­Ù‚Ù‚
            if self.val_losses:
                axes[0, 1].plot(self.val_losses)
                axes[0, 1].set_title('Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ­Ù‚Ù‚')
                axes[0, 1].set_xlabel('Ø§Ù„Ø®Ø·ÙˆØ©')
                axes[0, 1].set_ylabel('Ø§Ù„Ø®Ø³Ø§Ø±Ø©')
                axes[0, 1].grid(True, alpha=0.3)
            
            # Ø±Ø³Ù… Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            if self.learning_rates:
                axes[1, 0].plot(self.learning_rates)
                axes[1, 0].set_title('Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…')
                axes[1, 0].set_xlabel('Ø§Ù„Ø®Ø·ÙˆØ©')
                axes[1, 0].set_ylabel('Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…')
                axes[1, 0].grid(True, alpha=0.3)
                axes[1, 0].set_yscale('log')
            
            # Ø±Ø³Ù… Perplexity (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§)
            if self.train_losses:
                perplexities = [np.exp(loss) for loss in self.train_losses]
                axes[1, 1].plot(perplexities)
                axes[1, 1].set_title('Perplexity Ø§Ù„ØªØ¯Ø±ÙŠØ¨')
                axes[1, 1].set_xlabel('Ø§Ù„Ø®Ø·ÙˆØ©')
                axes[1, 1].set_ylabel('Perplexity')
                axes[1, 1].grid(True, alpha=0.3)
                axes[1, 1].set_yscale('log')
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø±Ø³Ù… Ø¥Ù„Ù‰ {save_path}")
            else:
                plt.show()
                
        except ImportError:
            print("âš ï¸  Matplotlib ØºÙŠØ± Ù…Ø«Ø¨ØªØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø±Ø³Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©")


class DistributedTrainer(Trainer):
    """Ù…Ø¯Ø±Ø¨ Ù…ÙˆØ²Ø¹ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø£Ø¬Ù‡Ø²Ø© Ù…ØªØ¹Ø¯Ø¯Ø©"""
    
    def __init__(self, *args, **kwargs):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø§Ù„Ù…ÙˆØ²Ø¹"""
        super().__init__(*args, **kwargs)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹
        self.local_rank = kwargs.get('local_rank', 0)
        self.world_size = kwargs.get('world_size', 1)
        
        if self.world_size > 1:
            self._setup_distributed()
    
    def _setup_distributed(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹"""
        try:
            import torch.distributed as dist
            dist.init_process_group(backend='nccl')
            
            # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ DDP
            self.model = torch.nn.parallel.DistributedDataParallel(
                self.model,
                device_ids=[self.local_rank],
                output_device=self.local_rank
            )
            
            print(f"âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ (Ø§Ù„Ø¹Ø§Ù„Ù…: {self.world_size})")
            
        except Exception as e:
            print(f"âš ï¸  ÙØ´Ù„ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹: {e}")
            self.world_size = 1


def create_trainer(model_config: Dict[str, Any], 
                  data_config: Dict[str, Any],
                  training_config: Dict[str, Any]) -> Trainer:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯Ø±Ø¨ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    
    Args:
        model_config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        data_config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        training_config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    
    Returns:
        Ù…Ø¯Ø±Ø¨ Ø¬Ø§Ù‡Ø²
    """
    from ..model.tiny_llm import create_model_from_config
    from ..data.dataset import TextDataset
    from ..data.tokenizer import Tokenizer
    
    # Ø¥Ù†Ø´Ø§Ø¡ Tokenizer
    tokenizer_config = data_config.get('tokenizer', {})
    tokenizer = Tokenizer(tokenizer_config)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_dataset = TextDataset(
        data_path=data_config.get('train_path', './data/processed/train.pt'),
        tokenizer=tokenizer,
        max_length=data_config.get('max_length', 2048)
    )
    
    val_dataset = None
    if data_config.get('val_path'):
        val_dataset = TextDataset(
            data_path=data_config.get('val_path'),
            tokenizer=tokenizer,
            max_length=data_config.get('max_length', 2048)
        )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = create_model_from_config({'model': model_config})
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
    trainer = Trainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        config=training_config
    )
    
    return trainer


if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¯Ø±Ø¨
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¯Ø±Ø¨...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    from ..data.dataset import create_sample_dataset
    from ..model.tiny_llm import TinyLLM
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹ÙŠÙ†Ø©
    create_sample_dataset(num_samples=100)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ±
    model = TinyLLM(
        vocab_size=5000,
        d_model=128,
        n_heads=4,
        n_layers=2,
        max_seq_len=256
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Tokenizer
    from ..data.tokenizer import Tokenizer
    tokenizer_config = {
        "vocab_size": 5000,
        "special_tokens": {
            "bos": "<bos>",
            "eos": "<eos>",
            "pad": "<pad>",
            "unk": "<unk>"
        }
    }
    tokenizer = Tokenizer(tokenizer_config)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª
    from ..data.dataset import TextDataset
    dataset = TextDataset(
        data_path="./data/processed/sample.pt",
        tokenizer=tokenizer,
        max_length=128
    )
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_dataset, val_dataset, _ = dataset.split(train_ratio=0.8, val_ratio=0.1)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
    trainer = Trainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        config={
            'batch_size': 8,
            'micro_batch_size': 2,
            'gradient_accumulation_steps': 4,
            'total_steps': 100,
            'eval_steps': 50,
            'save_interval': 100
        }
    )
    
    # ØªØ¯Ø±ÙŠØ¨ Ù‚ØµÙŠØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    print("\nğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ø®ØªØ¨Ø§Ø±ÙŠ...")
    try:
        trainer.train()
        print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¯Ø±Ø¨: {e}")