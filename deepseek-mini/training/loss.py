# -*- coding: utf-8 -*-
"""
Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…Ø®ØµØµØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, List, Dict, Any


class LanguageModelingLoss(nn.Module):
    """Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ù„Ø­Ø´Ùˆ"""
    
    def __init__(self, ignore_index: int = 0, label_smoothing: float = 0.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©
        
        Args:
            ignore_index: ÙÙ‡Ø±Ø³ Ù„ØªØ¬Ø§Ù‡Ù„Ù‡ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© (Ø¹Ø§Ø¯Ø©Ù‹ Ø§Ù„Ø­Ø´Ùˆ)
            label_smoothing: ØªÙ†Ø¹ÙŠÙ… Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        """
        super().__init__()
        self.ignore_index = ignore_index
        self.label_smoothing = label_smoothing
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=ignore_index,
            label_smoothing=label_smoothing,
            reduction='mean'
        )
    
    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        
        Args:
            logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ [batch_size, seq_len, vocab_size]
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª [batch_size, seq_len]
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        """
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ logits Ùˆlabels
        batch_size, seq_len, vocab_size = logits.shape
        logits = logits.view(-1, vocab_size)
        labels = labels.view(-1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        loss = self.criterion(logits, labels)
        
        return loss
    
    def compute_perplexity(self, logits: torch.Tensor, labels: torch.Tensor) -> float:
        """
        Ø­Ø³Ø§Ø¨ Perplexity
        
        Args:
            logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        
        Returns:
            Ù‚ÙŠÙ…Ø© Perplexity
        """
        loss = self.forward(logits, labels)
        perplexity = torch.exp(loss).item()
        return perplexity


class FocalLoss(nn.Module):
    """Ø®Ø³Ø§Ø±Ø© Focal Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ Ø§Ù„ÙØ¦Ø§Øª"""
    
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, 
                 ignore_index: int = 0, reduction: str = 'mean'):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø®Ø³Ø§Ø±Ø© Focal
        
        Args:
            alpha: Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªÙˆØ§Ø²Ù†
            gamma: Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ±ÙƒÙŠØ²
            ignore_index: ÙÙ‡Ø±Ø³ Ù„ØªØ¬Ø§Ù‡Ù„Ù‡
            reduction: Ù†ÙˆØ¹ Ø§Ù„ØªØ®ÙÙŠØ¶ ('mean', 'sum', 'none')
        """
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ignore_index = ignore_index
        self.reduction = reduction
    
    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© Focal
        
        Args:
            logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        """
        # ØªØ­ÙˆÙŠÙ„ logits Ø¥Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        probs = F.softmax(logits, dim=-1)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
        batch_size, seq_len, vocab_size = probs.shape
        probs = probs.view(-1, vocab_size)
        labels = labels.view(-1)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ù„Ù„ÙØ¦Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ù„Ø©
        mask = (labels != self.ignore_index)
        probs = probs[mask]
        labels = labels[mask]
        
        if probs.numel() == 0:
            return torch.tensor(0.0, device=logits.device)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
        pt = probs.gather(1, labels.unsqueeze(1)).squeeze()
        
        # Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© Focal
        loss = -self.alpha * (1 - pt).pow(self.gamma) * torch.log(pt + 1e-8)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ®ÙÙŠØ¶
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss


class LabelSmoothingLoss(nn.Module):
    """Ø®Ø³Ø§Ø±Ø© Ù…Ø¹ ØªÙ†Ø¹ÙŠÙ… Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØ³Ù…ÙŠØ§Øª"""
    
    def __init__(self, vocab_size: int, smoothing: float = 0.1, 
                 ignore_index: int = 0, reduction: str = 'mean'):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø®Ø³Ø§Ø±Ø© ØªÙ†Ø¹ÙŠÙ… Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        
        Args:
            vocab_size: Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            smoothing: ÙƒÙ…ÙŠØ© Ø§Ù„ØªÙ†Ø¹ÙŠÙ…
            ignore_index: ÙÙ‡Ø±Ø³ Ù„ØªØ¬Ø§Ù‡Ù„Ù‡
            reduction: Ù†ÙˆØ¹ Ø§Ù„ØªØ®ÙÙŠØ¶
        """
        super().__init__()
        self.vocab_size = vocab_size
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.reduction = reduction
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¹ÙŠÙ…
        self.confidence = 1.0 - smoothing
        self.smoothing_value = smoothing / (vocab_size - 1)  # -1 Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©
    
    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© ØªÙ†Ø¹ÙŠÙ… Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        
        Args:
            logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        """
        batch_size, seq_len, vocab_size = logits.shape
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        logits = logits.view(-1, vocab_size)
        labels = labels.view(-1)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªØ³Ù…ÙŠØ§Øª Ù…Ù†Ø¹Ù…Ø©
        smoothed_labels = torch.full_like(logits, self.smoothing_value, 
                                         device=logits.device)
        
        # ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø«Ù‚Ø© Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
        mask = (labels != self.ignore_index).unsqueeze(1)
        smoothed_labels.scatter_(1, labels.unsqueeze(1), self.confidence)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ù†Ø§Ø¹
        smoothed_labels = smoothed_labels * mask.float()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø³Ø§Ù„Ø¨Ø© Ù„Ù„Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù„Ù„ÙˆØºØ§Ø±ÙŠØªÙ…ÙŠ
        log_probs = F.log_softmax(logits, dim=-1)
        loss = -torch.sum(smoothed_labels * log_probs, dim=-1)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ®ÙÙŠØ¶
        if self.reduction == 'mean':
            # Ù…ØªÙˆØ³Ø· ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ù„Ø©
            non_pad_elements = mask.sum().item()
            if non_pad_elements > 0:
                loss = loss.sum() / non_pad_elements
            else:
                loss = torch.tensor(0.0, device=logits.device)
        elif self.reduction == 'sum':
            loss = loss.sum()
        
        return loss


class KnowledgeDistillationLoss(nn.Module):
    """Ø®Ø³Ø§Ø±Ø© ØªÙ‚Ù„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ© (Knowledge Distillation)"""
    
    def __init__(self, temperature: float = 2.0, alpha: float = 0.5):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø®Ø³Ø§Ø±Ø© ØªÙ‚Ù„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ©
        
        Args:
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ù„Ù„ØªÙ†Ø¹ÙŠÙ…
            alpha: ÙˆØ²Ù† Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
        """
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_logits: torch.Tensor, 
                teacher_logits: torch.Tensor,
                labels: torch.Tensor) -> torch.Tensor:
        """
        Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© ØªÙ‚Ù„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ©
        
        Args:
            student_logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨
            teacher_logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù…
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„ÙƒÙ„ÙŠØ©
        """
        # Ø®Ø³Ø§Ø±Ø© KL Ø¨ÙŠÙ† ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆØ§Ù„Ø·Ø§Ù„Ø¨
        student_probs = F.log_softmax(student_logits / self.temperature, dim=-1)
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        
        kd_loss = self.kl_div(student_probs, teacher_probs) * (self.temperature ** 2)
        
        # Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
        ce_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), 
                                 labels.view(-1), ignore_index=0)
        
        # Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø®Ø³Ø§Ø±ØªÙŠÙ†
        total_loss = self.alpha * kd_loss + (1 - self.alpha) * ce_loss
        
        return total_loss


class ContrastiveLoss(nn.Module):
    """Ø®Ø³Ø§Ø±Ø© ØªØ¨Ø§ÙŠÙ†ÙŠØ© Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©"""
    
    def __init__(self, temperature: float = 0.07, margin: float = 0.5):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ†ÙŠØ©
        
        Args:
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
            margin: Ø§Ù„Ù‡Ø§Ù…Ø´ Ù„Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø«Ù„Ø§Ø«ÙŠØ©
        """
        super().__init__()
        self.temperature = temperature
        self.margin = margin
    
    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ†ÙŠØ©
        
        Args:
            embeddings: Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª [batch_size, seq_len, hidden_size]
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª [batch_size, seq_len]
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        """
        batch_size, seq_len, hidden_size = embeddings.shape
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        embeddings = embeddings.view(-1, hidden_size)  # [batch_size*seq_len, hidden_size]
        labels = labels.view(-1)  # [batch_size*seq_len]
        
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ø´Ùˆ
        mask = (labels != 0)
        embeddings = embeddings[mask]
        labels = labels[mask]
        
        if embeddings.size(0) < 2:
            return torch.tensor(0.0, device=embeddings.device)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity = torch.matmul(embeddings, embeddings.T) / self.temperature
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ù„Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
        labels_expanded = labels.unsqueeze(0)
        positive_mask = (labels_expanded == labels_expanded.T).float()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø§Ù„Ø°Ø§Øª
        self_mask = torch.eye(positive_mask.size(0), device=positive_mask.device)
        positive_mask = positive_mask - self_mask
        
        # Ø®Ø³Ø§Ø±Ø© InfoNCE
        exp_sim = torch.exp(similarity)
        
        # Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù„Ù„Ø³Ø§Ù„Ø¨
        sum_exp_sim = torch.sum(exp_sim * (1 - positive_mask), dim=1, keepdim=True)
        
        # Ø®Ø³Ø§Ø±Ø© Ù„Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ¬Ø¨Ø©
        positive_loss = -torch.log(exp_sim * positive_mask / (exp_sim * positive_mask + sum_exp_sim + 1e-8))
        positive_loss = torch.sum(positive_loss) / torch.sum(positive_mask)
        
        return positive_loss


class MixtureOfExpertsLoss(nn.Module):
    """Ø®Ø³Ø§Ø±Ø© Ù…ØªØ®ØµØµØ© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø°Ø§Øª Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ÙŠÙ† (MoE)"""
    
    def __init__(self, aux_loss_weight: float = 0.01, 
                 load_balance_weight: float = 0.01):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø®Ø³Ø§Ø±Ø© MoE
        
        Args:
            aux_loss_weight: ÙˆØ²Ù† Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
            load_balance_weight: ÙˆØ²Ù† Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø­Ù…ÙˆÙ„Ø©
        """
        super().__init__()
        self.aux_loss_weight = aux_loss_weight
        self.load_balance_weight = load_balance_weight
    
    def forward(self, logits: torch.Tensor, labels: torch.Tensor,
                gate_logits: torch.Tensor, expert_indices: torch.Tensor) -> torch.Tensor:
        """
        Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© MoE
        
        Args:
            logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
            gate_logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©
            expert_indices: ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„ÙƒÙ„ÙŠØ©
        """
        # Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        base_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                                   labels.view(-1), ignore_index=0)
        
        # Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡
        aux_loss = self._compute_auxiliary_loss(gate_logits, expert_indices)
        
        # Ø®Ø³Ø§Ø±Ø© Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø­Ù…ÙˆÙ„Ø©
        load_balance_loss = self._compute_load_balance_loss(gate_logits, expert_indices)
        
        # Ø§Ù„Ø¬Ù…Ø¹
        total_loss = base_loss + self.aux_loss_weight * aux_loss + \
                    self.load_balance_weight * load_balance_loss
        
        return total_loss
    
    def _compute_auxiliary_loss(self, gate_logits: torch.Tensor, 
                               expert_indices: torch.Tensor) -> torch.Tensor:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©"""
        # ØªØ´Ø¬ÙŠØ¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø¨Ø±Ø§Ø¡ Ù…Ø®ØªÙ„ÙÙŠÙ†
        expert_usage = torch.zeros(gate_logits.size(-1), device=gate_logits.device)
        
        for indices in expert_indices:
            expert_usage.scatter_add_(0, indices.flatten(), 
                                     torch.ones_like(indices.flatten()))
        
        # Ø§Ù„Ø®Ø³Ø§Ø±Ø©: ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ† ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        usage_mean = expert_usage.mean()
        usage_var = ((expert_usage - usage_mean) ** 2).mean()
        
        return usage_var
    
    def _compute_load_balance_loss(self, gate_logits: torch.Tensor,
                                 expert_indices: torch.Tensor) -> torch.Tensor:
        """Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø­Ù…ÙˆÙ„Ø©"""
        # Ø­Ø³Ø§Ø¨ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©
        gate_probs = F.softmax(gate_logits, dim=-1)
        
        # Ø­Ø³Ø§Ø¨ ÙƒÙ…ÙŠØ© Ø§Ù„Ø¹Ù…Ù„ Ù„ÙƒÙ„ Ø®Ø¨ÙŠØ±
        expert_load = torch.zeros(gate_probs.size(-1), device=gate_probs.device)
        
        for i in range(gate_probs.size(0)):
            for j in range(gate_probs.size(1)):
                expert_load += gate_probs[i, j, :]
        
        # ØªØ´Ø¬ÙŠØ¹ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ØªØ³Ø§ÙˆÙŠ
        load_mean = expert_load.mean()
        load_balance = ((expert_load - load_mean) ** 2).mean()
        
        return load_balance


class LossFunctionFactory:
    """Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©"""
    
    @staticmethod
    def create_loss(loss_type: str, **kwargs) -> nn.Module:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø©
        
        Args:
            loss_type: Ù†ÙˆØ¹ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        
        Returns:
            Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø©
        """
        if loss_type == 'cross_entropy':
            return LanguageModelingLoss(**kwargs)
        elif loss_type == 'focal':
            return FocalLoss(**kwargs)
        elif loss_type == 'label_smoothing':
            return LabelSmoothingLoss(**kwargs)
        elif loss_type == 'knowledge_distillation':
            return KnowledgeDistillationLoss(**kwargs)
        elif loss_type == 'contrastive':
            return ContrastiveLoss(**kwargs)
        elif loss_type == 'moe':
            return MixtureOfExpertsLoss(**kwargs)
        else:
            raise ValueError(f"Ù†ÙˆØ¹ Ø®Ø³Ø§Ø±Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {loss_type}")


class LossMonitor:
    """Ù…Ø±Ø§Ù‚Ø¨ Ù„Ù„Ø®Ø³Ø§Ø¦Ø± ÙˆØ¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø®Ø³Ø§Ø¦Ø±"""
        self.losses = []
        self.perplexities = []
        self.grad_norms = []
        self.learning_rates = []
    
    def update(self, loss: float, grad_norm: float = None, 
              lr: float = None, logits: torch.Tensor = None, 
              labels: torch.Tensor = None):
        """
        ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        
        Args:
            loss: Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
            grad_norm: Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„ØªØ¯Ø±Ø¬
            lr: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            logits: Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            labels: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        """
        self.losses.append(loss)
        
        if grad_norm is not None:
            self.grad_norms.append(grad_norm)
        
        if lr is not None:
            self.learning_rates.append(lr)
        
        if logits is not None and labels is not None:
            perplexity = self._compute_perplexity(logits, labels)
            self.perplexities.append(perplexity)
    
    def _compute_perplexity(self, logits: torch.Tensor, 
                           labels: torch.Tensor) -> float:
        """Ø­Ø³Ø§Ø¨ Perplexity"""
        loss_fn = LanguageModelingLoss()
        loss = loss_fn(logits, labels)
        return torch.exp(loss).item()
    
    def get_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        stats = {}
        
        if self.losses:
            stats['loss_mean'] = sum(self.losses) / len(self.losses)
            stats['loss_min'] = min(self.losses)
            stats['loss_max'] = max(self.losses)
            stats['loss_std'] = torch.std(torch.tensor(self.losses)).item()
        
        if self.perplexities:
            stats['ppl_mean'] = sum(self.perplexities) / len(self.perplexities)
            stats['ppl_min'] = min(self.perplexities)
            stats['ppl_max'] = max(self.perplexities)
        
        if self.grad_norms:
            stats['grad_norm_mean'] = sum(self.grad_norms) / len(self.grad_norms)
            stats['grad_norm_max'] = max(self.grad_norms)
        
        if self.learning_rates:
            stats['lr_mean'] = sum(self.learning_rates) / len(self.learning_rates)
        
        return stats
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨"""
        self.losses = []
        self.perplexities = []
        self.grad_norms = []
        self.learning_rates = []
    
    def print_summary(self):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        stats = self.get_stats()
        
        print("=" * 60)
        print("ðŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ø®Ø³Ø§Ø¦Ø±:")
        print("=" * 60)
        
        for key, value in stats.items():
            print(f"{key}: {value:.4f}")
        
        print("=" * 60)


def test_loss_functions():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©"""
    print("ðŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    batch_size = 4
    seq_len = 10
    vocab_size = 100
    
    logits = torch.randn(batch_size, seq_len, vocab_size)
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # ØªØ¹ÙŠÙŠÙ† Ø¨Ø¹Ø¶ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª ÙƒØ­Ø´Ùˆ
    labels[:, -2:] = 0
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± LanguageModelingLoss:")
    loss_fn = LanguageModelingLoss(ignore_index=0)
    loss = loss_fn(logits, labels)
    perplexity = loss_fn.compute_perplexity(logits, labels)
    
    print(f"   Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {loss:.4f}")
    print(f"   Perplexity: {perplexity:.4f}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø®Ø³Ø§Ø±Ø© Focal
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± FocalLoss:")
    focal_loss_fn = FocalLoss(ignore_index=0)
    focal_loss = focal_loss_fn(logits, labels)
    print(f"   Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {focal_loss:.4f}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø®Ø³Ø§Ø±Ø© ØªÙ†Ø¹ÙŠÙ… Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± LabelSmoothingLoss:")
    smoothing_loss_fn = LabelSmoothingLoss(vocab_size=vocab_size, smoothing=0.1)
    smoothing_loss = smoothing_loss_fn(logits, labels)
    print(f"   Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {smoothing_loss:.4f}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± LossFunctionFactory
    print("\n4. Ø§Ø®ØªØ¨Ø§Ø± LossFunctionFactory:")
    factory = LossFunctionFactory()
    
    loss_functions = ['cross_entropy', 'focal', 'label_smoothing']
    for loss_type in loss_functions:
        try:
            loss_fn = factory.create_loss(
                loss_type, 
                vocab_size=vocab_size,
                ignore_index=0
            )
            loss_value = loss_fn(logits, labels)
            print(f"   {loss_type}: {loss_value:.4f}")
        except Exception as e:
            print(f"   {loss_type}: Ø®Ø·Ø£ - {e}")
    
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± LossMonitor
    print("\n5. Ø§Ø®ØªØ¨Ø§Ø± LossMonitor:")
    monitor = LossMonitor()
    
    for i in range(5):
        monitor.update(
            loss=i * 0.1,
            grad_norm=i * 0.05,
            lr=1e-3,
            logits=logits,
            labels=labels
        )
    
    stats = monitor.get_stats()
    print(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {stats.get('loss_mean', 0):.4f}")
    print(f"   Ù…ØªÙˆØ³Ø· Perplexity: {stats.get('ppl_mean', 0):.4f}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_loss_functions()