# -*- coding: utf-8 -*-
"""
Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
"""

import torch
import torch.optim as optim
import math
from typing import Dict, Any, Optional, List, Tuple
from collections import defaultdict


class AdamW(optim.AdamW):
    """AdamW Ù…Ø­Ø³Ù† Ù…Ø¹ ØªØ³Ø®ÙŠÙ† Ù…Ø®ØµØµ ÙˆØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
    
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                 weight_decay=0.01, amsgrad=False, warmup_steps=0,
                 total_steps=100000, min_lr=1e-6):
        """
        ØªÙ‡ÙŠØ¦Ø© AdamW Ù…Ø¹ ØªØ³Ø®ÙŠÙ†
        
        Args:
            params: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            lr: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            betas: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨ÙŠØªØ§ Ù„Ù€ Adam
            eps: Ù‚ÙŠÙ…Ø© epsilon Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø¯Ø¯ÙŠ
            weight_decay: ØªØ³Ù„Ù„ Ø§Ù„ÙˆØ²Ù†
            amsgrad: Ø§Ø³ØªØ®Ø¯Ø§Ù… AMSGrad
            warmup_steps: Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ³Ø®ÙŠÙ†
            total_steps: Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            min_lr: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        """
        super().__init__(params, lr=lr, betas=betas, eps=eps,
                        weight_decay=weight_decay, amsgrad=amsgrad)
        
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.min_lr = min_lr
        self.current_step = 0
        
        # Ø­ÙØ¸ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        self.base_lr = lr
        
        # ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø£ØµÙ„ÙŠØ©
        for group in self.param_groups:
            group['initial_lr'] = lr
    
    def step(self, closure=None):
        """Ø®Ø·ÙˆØ© ØªØ­Ø¯ÙŠØ« Ù…Ø¹ Ø¶Ø¨Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…"""
        # Ø¶Ø¨Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ù‚Ø¨Ù„ Ø§Ù„Ø®Ø·ÙˆØ©
        self._adjust_learning_rate()
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        loss = super().step(closure)
        
        # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø·ÙˆØ©
        self.current_step += 1
        
        return loss
    
    def _adjust_learning_rate(self):
        """Ø¶Ø¨Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        if self.current_step < self.warmup_steps:
            # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ³Ø®ÙŠÙ†: Ø²ÙŠØ§Ø¯Ø© Ø®Ø·ÙŠØ©
            lr_mult = float(self.current_step) / float(max(1, self.warmup_steps))
        else:
            # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ¨Ø±ÙŠØ¯: ØªÙ†Ø§Ù‚Øµ Ø¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù…
            progress = float(self.current_step - self.warmup_steps) / float(
                max(1, self.total_steps - self.warmup_steps))
            lr_mult = max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯
        new_lr = self.min_lr + (self.base_lr - self.min_lr) * lr_mult
        
        # ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ù„Ø¬Ù…ÙŠØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        for group in self.param_groups:
            group['lr'] = new_lr
    
    def get_lr(self) -> float:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        return self.param_groups[0]['lr']
    
    def state_dict(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†"""
        state = super().state_dict()
        state['current_step'] = self.current_step
        state['warmup_steps'] = self.warmup_steps
        state['total_steps'] = self.total_steps
        state['min_lr'] = self.min_lr
        state['base_lr'] = self.base_lr
        return state
    
    def load_state_dict(self, state_dict):
        """ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†"""
        self.current_step = state_dict.pop('current_step', 0)
        self.warmup_steps = state_dict.pop('warmup_steps', 0)
        self.total_steps = state_dict.pop('total_steps', 100000)
        self.min_lr = state_dict.pop('min_lr', 1e-6)
        self.base_lr = state_dict.pop('base_lr', self.param_groups[0]['lr'])
        super().load_state_dict(state_dict)


class Lion(optim.Optimizer):
    """Ù…Ø­Ø³Ù† Lion (Evolved Sign Momentum)"""
    
    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø³Ù† Lion
        
        Args:
            params: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            lr: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            betas: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨ÙŠØªØ§ Ù„Ù„Ø²Ø®Ù…
            weight_decay: ØªØ³Ù„Ù„ Ø§Ù„ÙˆØ²Ù†
        """
        if not 0.0 <= lr:
            raise ValueError(f"Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ØºÙŠØ± ØµØ§Ù„Ø­: {lr}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Ø¨ÙŠØªØ§ 1 ØºÙŠØ± ØµØ§Ù„Ø­: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Ø¨ÙŠØªØ§ 2 ØºÙŠØ± ØµØ§Ù„Ø­: {betas[1]}")
        
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super().__init__(params, defaults)
    
    @torch.no_grad()
    def step(self, closure=None):
        """Ø®Ø·ÙˆØ© ØªØ­Ø¯ÙŠØ«"""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()
        
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            lr = group['lr']
            weight_decay = group['weight_decay']
            
            for p in group['params']:
                if p.grad is None:
                    continue
                
                grad = p.grad
                
                # Ø§Ù„Ø­Ø§Ù„Ø©
                state = self.state[p]
                
                # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø§Ù„Ø©
                if len(state) == 0:
                    state['exp_avg'] = torch.zeros_like(p)
                
                exp_avg = state['exp_avg']
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø²Ø®Ù…
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ­Ø¯ÙŠØ«
                update = exp_avg.sign().add_(p, alpha=weight_decay)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                p.add_(update, alpha=-lr)
        
        return loss


class Adafactor(optim.Optimizer):
    """Ù…Ø­Ø³Ù† Adafactor (Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©)"""
    
    def __init__(self, params, lr=None, beta1=0.9, beta2=0.999, eps=1e-8,
                 weight_decay=0.0, scale_parameter=True, relative_step=True,
                 warmup_init=False):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø³Ù† Adafactor
        
        Args:
            params: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            lr: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (Ø§Ø³ØªØ®Ø¯Ù… None Ù„Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©)
            beta1: Ù…Ø¹Ø§Ù…Ù„ Ø¨ÙŠØªØ§ Ù„Ù„Ø²Ø®Ù…
            beta2: Ù…Ø¹Ø§Ù…Ù„ Ø¨ÙŠØªØ§ Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠØ©
            eps: Ù‚ÙŠÙ…Ø© epsilon Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
            weight_decay: ØªØ³Ù„Ù„ Ø§Ù„ÙˆØ²Ù†
            scale_parameter: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            relative_step: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
            warmup_init: ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ³Ø®ÙŠÙ†
        """
        if lr is not None and lr < 0.0:
            raise ValueError(f"Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ØºÙŠØ± ØµØ§Ù„Ø­: {lr}")
        if not 0.0 <= beta1 < 1.0:
            raise ValueError(f"Ø¨ÙŠØªØ§ 1 ØºÙŠØ± ØµØ§Ù„Ø­: {beta1}")
        if not 0.0 <= beta2 < 1.0:
            raise ValueError(f"Ø¨ÙŠØªØ§ 2 ØºÙŠØ± ØµØ§Ù„Ø­: {beta2}")
        
        defaults = dict(
            lr=lr, beta1=beta1, beta2=beta2, eps=eps,
            weight_decay=weight_decay, scale_parameter=scale_parameter,
            relative_step=relative_step, warmup_init=warmup_init
        )
        super().__init__(params, defaults)
        
        # Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø·ÙˆØ©
        self.state['step'] = 0
    
    @torch.no_grad()
    def step(self, closure=None):
        """Ø®Ø·ÙˆØ© ØªØ­Ø¯ÙŠØ«"""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()
        
        self.state['step'] += 1
        step = self.state['step']
        
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                
                grad = p.grad
                if grad.is_sparse:
                    raise RuntimeError('Adafactor Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…ØªÙ†Ø§Ø«Ø±Ø©')
                
                # Ø§Ù„Ø­Ø§Ù„Ø©
                state = self.state[p]
                
                # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø§Ù„Ø©
                if len(state) == 0:
                    state['exp_avg'] = torch.zeros_like(p)
                    state['exp_avg_sq'] = torch.zeros_like(p)
                    state['step'] = 0
                
                state['step'] += 1
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                beta1, beta2 = group['beta1'], group['beta2']
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠØ©
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                # Ø­Ø³Ø§Ø¨ RMS
                rms = exp_avg_sq.sqrt().add_(group['eps'])
                
                # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
                if group['relative_step']:
                    # Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
                    step_size = 1.0 / max(1, state['step'])
                else:
                    step_size = group['lr']
                
                # Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ø°Ø§ Ø·Ù„Ø¨
                if group['scale_parameter']:
                    param_rms = p.data.pow(2).mean().sqrt().clamp(min=group['eps'])
                    step_size = step_size * param_rms
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø²Ø®Ù…
                exp_avg.mul_(beta1).add_(grad.div(rms), alpha=1 - beta1)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                p.data.add_(exp_avg, alpha=-step_size)
                
                # ØªØ³Ù„Ù„ Ø§Ù„ÙˆØ²Ù†
                if group['weight_decay'] > 0:
                    p.data.add_(p.data, alpha=-group['lr'] * group['weight_decay'])
        
        return loss


class Sophia(optim.Optimizer):
    """Ù…Ø­Ø³Ù† Sophia (Ø«Ø§Ù†ÙˆÙŠ Ù…Ù† Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©)"""
    
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                 weight_decay=0.0, hessian_update_interval=10, 
                 hessian_approx='diagonal'):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø³Ù† Sophia
        
        Args:
            params: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            lr: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            betas: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨ÙŠØªØ§
            eps: Ù‚ÙŠÙ…Ø© epsilon
            weight_decay: ØªØ³Ù„Ù„ Ø§Ù„ÙˆØ²Ù†
            hessian_update_interval: ÙØ§ØµÙ„ ØªØ­Ø¯ÙŠØ« Hessian
            hessian_approx: ØªÙ‚Ø±ÙŠØ¨ Hessian ('diagonal' Ø£Ùˆ 'kfac')
        """
        defaults = dict(
            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,
            hessian_update_interval=hessian_update_interval,
            hessian_approx=hessian_approx
        )
        super().__init__(params, defaults)
        
        # Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø·ÙˆØ©
        self.state['step'] = 0
    
    @torch.no_grad()
    def step(self, closure=None):
        """Ø®Ø·ÙˆØ© ØªØ­Ø¯ÙŠØ«"""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()
        
        self.state['step'] += 1
        step = self.state['step']
        
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                
                grad = p.grad
                
                # Ø§Ù„Ø­Ø§Ù„Ø©
                state = self.state[p]
                
                # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø§Ù„Ø©
                if len(state) == 0:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p)
                    state['exp_avg_sq'] = torch.zeros_like(p)
                    state['hessian'] = torch.zeros_like(p)
                
                state['step'] += 1
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                hessian = state['hessian']
                beta1, beta2 = group['betas']
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø²Ø®Ù…
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                
                # ØªØ­Ø¯ÙŠØ« Hessian Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
                if step % group['hessian_update_interval'] == 0:
                    if group['hessian_approx'] == 'diagonal':
                        # ØªÙ‚Ø±ÙŠØ¨ Ù‚Ø·Ø±ÙŠ Ù„Ù€ Hessian
                        hessian.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·ÙˆØ©
                hessian_clamped = hessian.clamp(min=group['eps'])
                update = exp_avg / hessian_clamped
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                p.data.add_(update, alpha=-group['lr'])
                
                # ØªØ³Ù„Ù„ Ø§Ù„ÙˆØ²Ù†
                if group['weight_decay'] > 0:
                    p.data.add_(p.data, alpha=-group['lr'] * group['weight_decay'])
        
        return loss


class GradientClipper:
    """Ø£Ø¯Ø§Ø© Ù‚Øµ Ø§Ù„ØªØ¯Ø±Ø¬"""
    
    @staticmethod
    def clip_grad_norm(parameters, max_norm: float, norm_type: float = 2.0):
        """
        Ù‚Øµ ØªØ¯Ø±Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        
        Args:
            parameters: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            max_norm: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù‚Ø§Ø¹Ø¯Ø©
            norm_type: Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© (2 Ù„Ù€ L2)
        
        Returns:
            Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        """
        return torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type)
    
    @staticmethod
    def clip_grad_value(parameters, clip_value: float):
        """
        Ù‚Øµ Ù‚ÙŠÙ… Ø§Ù„ØªØ¯Ø±Ø¬
        
        Args:
            parameters: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            clip_value: Ù‚ÙŠÙ…Ø© Ø§Ù„Ù‚Øµ
        """
        torch.nn.utils.clip_grad_value_(parameters, clip_value)
    
    @staticmethod
    def adaptive_clip(parameters, percentile: float = 90.0):
        """
        Ù‚Øµ ØªØ¯Ø±Ø¬ ØªÙƒÙŠÙÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©
        
        Args:
            parameters: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            percentile: Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ù„Ù„Ù‚Øµ
        
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ù‚Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
        """
        all_grads = []
        for p in parameters:
            if p.grad is not None:
                all_grads.append(p.grad.abs().flatten())
        
        if not all_grads:
            return 0.0
        
        all_grads = torch.cat(all_grads)
        clip_value = torch.quantile(all_grads, percentile / 100.0).item()
        
        torch.nn.utils.clip_grad_value_(parameters, clip_value)
        return clip_value


class OptimizerFactory:
    """Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª"""
    
    @staticmethod
    def create_optimizer(optimizer_type: str, model: torch.nn.Module, **kwargs):
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø³Ù†
        
        Args:
            optimizer_type: Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­Ø³Ù†
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†
        
        Returns:
            Ù…Ø­Ø³Ù† Ù…ÙƒÙˆÙ†
        """
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        params = OptimizerFactory._get_parameter_groups(model, kwargs)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        if optimizer_type.lower() == 'adamw':
            return AdamW(params, **kwargs)
        elif optimizer_type.lower() == 'adam':
            return optim.Adam(params, **kwargs)
        elif optimizer_type.lower() == 'sgd':
            return optim.SGD(params, **kwargs)
        elif optimizer_type.lower() == 'lion':
            return Lion(params, **kwargs)
        elif optimizer_type.lower() == 'adafactor':
            return Adafactor(params, **kwargs)
        elif optimizer_type.lower() == 'sophia':
            return Sophia(params, **kwargs)
        elif optimizer_type.lower() == 'rmsprop':
            return optim.RMSprop(params, **kwargs)
        else:
            raise ValueError(f"Ù†ÙˆØ¹ Ù…Ø­Ø³Ù† ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {optimizer_type}")
    
    @staticmethod
    def _get_parameter_groups(model: torch.nn.Module, config: Dict[str, Any]) -> List[Dict]:
        """
        ØªØ¬Ù…ÙŠØ¹ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ø¹Ø¯Ù„Ø§Øª ØªØ¹Ù„Ù… Ù…Ø®ØªÙ„ÙØ©
        
        Args:
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            config: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        """
        # Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ø·Ø¨Ù‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©
        no_decay = ['bias', 'LayerNorm.weight', 'RMSNorm.weight']
        
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in model.named_parameters() 
                          if not any(nd in n for nd in no_decay) and p.requires_grad],
                'weight_decay': config.get('weight_decay', 0.01),
                'lr': config.get('learning_rate', 3e-4)
            },
            {
                'params': [p for n, p in model.named_parameters() 
                          if any(nd in n for nd in no_decay) and p.requires_grad],
                'weight_decay': 0.0,
                'lr': config.get('learning_rate', 3e-4)
            }
        ]
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ø°Ø§ Ø·Ù„Ø¨
        if config.get('layerwise_lr', False):
            # Ù…Ø¹Ø¯Ù„Ø§Øª ØªØ¹Ù„Ù… Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ Ø·Ø¨Ù‚Ø©
            optimizer_grouped_parameters = OptimizerFactory._create_layerwise_groups(model, config)
        
        return optimizer_grouped_parameters
    
    @staticmethod
    def _create_layerwise_groups(model: torch.nn.Module, config: Dict[str, Any]) -> List[Dict]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø·Ø¨Ù‚Ø© Ø¨Ø·Ø¨Ù‚Ø©"""
        layers = []
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø©
            if 'embedding' in name:
                lr = config.get('embedding_lr', config.get('learning_rate', 3e-4) * 0.5)
                weight_decay = config.get('embedding_weight_decay', config.get('weight_decay', 0.01))
            elif 'attention' in name:
                lr = config.get('attention_lr', config.get('learning_rate', 3e-4))
                weight_decay = config.get('attention_weight_decay', config.get('weight_decay', 0.01))
            elif 'norm' in name or 'ln' in name:
                lr = config.get('norm_lr', config.get('learning_rate', 3e-4) * 2.0)
                weight_decay = 0.0  # Ù„Ø§ ØªØ³Ù„Ù„ Ù„Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©
            elif 'bias' in name:
                lr = config.get('bias_lr', config.get('learning_rate', 3e-4))
                weight_decay = 0.0
            else:
                lr = config.get('learning_rate', 3e-4)
                weight_decay = config.get('weight_decay', 0.01)
            
            layers.append({
                'params': [param],
                'lr': lr,
                'weight_decay': weight_decay
            })
        
        return layers


def create_optimizer(model: torch.nn.Module, 
                    learning_rate: float = 3e-4,
                    weight_decay: float = 0.01,
                    optimizer_type: str = 'adamw',
                    **kwargs) -> torch.optim.Optimizer:
    """
    Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø³Ù†
    
    Args:
        model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        learning_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        weight_decay: ØªØ³Ù„Ù„ Ø§Ù„ÙˆØ²Ù†
        optimizer_type: Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­Ø³Ù†
        **kwargs: Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    
    Returns:
        Ù…Ø­Ø³Ù† Ù…ÙƒÙˆÙ†
    """
    config = {
        'learning_rate': learning_rate,
        'weight_decay': weight_decay,
        **kwargs
    }
    
    return OptimizerFactory.create_optimizer(optimizer_type, model, **config)


def test_optimizers():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ø®ØªØ¨Ø§Ø± ØµØºÙŠØ±
    model = torch.nn.Sequential(
        torch.nn.Linear(10, 20),
        torch.nn.ReLU(),
        torch.nn.Linear(20, 10)
    )
    
    # Ø§Ø®ØªØ¨Ø§Ø± AdamW
    print("\n1. Ø§Ø®ØªØ¨Ø§Ø± AdamW:")
    optimizer = AdamW(model.parameters(), lr=1e-3, warmup_steps=10, total_steps=100)
    
    # Ø®Ø·ÙˆØ§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆÙ‡Ù…ÙŠØ©
    for step in range(5):
        # ØªØ¯Ø±Ø¬ ÙˆÙ‡Ù…ÙŠ
        for param in model.parameters():
            if param.grad is None:
                param.grad = torch.randn_like(param)
        
        optimizer.step()
        print(f"   Ø§Ù„Ø®Ø·ÙˆØ© {step}: lr = {optimizer.get_lr():.2e}")
    
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Lion
    print("\n2. Ø§Ø®ØªØ¨Ø§Ø± Lion:")
    optimizer = Lion(model.parameters(), lr=1e-3)
    
    for step in range(3):
        for param in model.parameters():
            if param.grad is None:
                param.grad = torch.randn_like(param)
        
        optimizer.step()
    
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± OptimizerFactory
    print("\n3. Ø§Ø®ØªØ¨Ø§Ø± OptimizerFactory:")
    optimizer = create_optimizer(
        model=model,
        learning_rate=1e-3,
        weight_decay=0.01,
        optimizer_type='adamw',
        warmup_steps=10,
        total_steps=100
    )
    
    print(f"   Ø§Ù„Ù†ÙˆØ¹: {type(optimizer).__name__}")
    print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù‚Øµ Ø§Ù„ØªØ¯Ø±Ø¬
    print("\n4. Ø§Ø®ØªØ¨Ø§Ø± Ù‚Øµ Ø§Ù„ØªØ¯Ø±Ø¬:")
    clipper = GradientClipper()
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªØ¯Ø±Ø¬Ø§Øª ÙƒØ¨ÙŠØ±Ø©
    for param in model.parameters():
        param.grad = torch.ones_like(param) * 10.0
    
    original_norm = clipper.clip_grad_norm(model.parameters(), max_norm=1.0)
    print(f"   Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {original_norm:.2f}")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚Øµ
    total_norm = 0
    for param in model.parameters():
        if param.grad is not None:
            total_norm += param.grad.norm().item() ** 2
    
    total_norm = total_norm ** 0.5
    print(f"   Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Øµ: {total_norm:.2f}")
    
    if total_norm <= 1.0 + 1e-6:
        print(f"   âœ“ ØªÙ… Ø¨Ù†Ø¬Ø§Ø­")
    else:
        print(f"   âœ— ÙØ´Ù„ Ø§Ù„Ù‚Øµ")
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")


if __name__ == "__main__":
    test_optimizers()